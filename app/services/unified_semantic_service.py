import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import json
import sqlite3
from dotenv import load_dotenv
from .time_parser import parse_time_context

# Load environment variables
load_dotenv()

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_community.utilities import SQLDatabase
from langchain_community.agent_toolkits import SQLDatabaseToolkit
from langchain_community.agent_toolkits.sql.base import create_sql_agent
from langchain.agents import AgentType
from langchain.tools import BaseTool
from langchain.schema import AgentAction, AgentFinish
from langchain.callbacks.manager import CallbackManagerForToolRun

# Import the new QueryBuilder
from app.services.query_builder import QueryBuilder

logger = logging.getLogger(__name__)

# MockLLM removed - using real LLM only

class LLMTranslator:
    """LLM-based semantic translation for Persian to English queries"""
    
    def __init__(self, llm):
        self.llm = llm
        self.few_shot_examples = [
            {
                "persian": "Ø¢Ø®Ø±ÛŒÙ† Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ú©ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ØŸ",
                "english": "When was the last irrigation performed?"
            },
            {
                "persian": "Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø¯Ø§Ø´Øª Ù…Ø­ØµÙˆÙ„ Ú¯ÙˆØ¬Ù‡ Ú†Ù†Ø¯ Ú©ÛŒÙ„ÙˆÚ¯Ø±Ù… Ø§Ø³ØªØŸ",
                "english": "What is the predicted tomato yield in kilograms?"
            },
            {
                "persian": "ÙˆØ¶Ø¹ÛŒØª Ø¢ÙØ§Øª Ø¯Ø± Ú¯Ù„Ø®Ø§Ù†Ù‡ Ø´Ù…Ø§Ø±Ù‡ Û² Ú†Ø·ÙˆØ±Ù‡ØŸ",
                "english": "What is the pest situation in greenhouse 2?"
            },
            {
                "persian": "ÙˆØ¶Ø¹ÛŒØª Ø¯Ù…Ø§ÛŒ Ø§Ù…Ø±ÙˆØ² Ú†Ø·ÙˆØ±Ù‡ØŸ",
                "english": "How is today's temperature status?"
            },
            {
                "persian": "Ø§Ø¨ÛŒØ§Ø±ÛŒ Ø§Ù…Ø±ÙˆØ² Ú†Ø·ÙˆØ±Ù‡ØŸ",
                "english": "How is today's irrigation status?"
            },
            {
                "persian": "Ø§Ù…Ø±ÙˆØ² Ø³Ù… Ù¾Ø§Ø´ÛŒ Ú©Ù†Ù…ØŸ",
                "english": "Should I spray pesticides today?"
            },
            {
                "persian": "Ù„Ø§Ù† Ù…Ø§ ØªÙˆ Ú†Ù‡ Ø¨Ø®Ø´ÛŒ Ø§Ø² Ú¯Ù„Ø®ÙˆÙ†Ù‡ Ù‡Ø³ØªÛŒÙ…ØŸ",
                "english": "What section of the greenhouse are we currently in?"
            }
        ]
    
    def detect_language(self, text: str) -> str:
        """Detect if text is Persian or English with hybrid detection for mixed content"""
        if not text or not text.strip():
            return 'en'
        
        # Count Persian characters (Arabic script range)
        persian_chars = sum(1 for char in text if '\u0600' <= char <= '\u06FF')
        # Count Arabic characters (extended range)
        arabic_chars = sum(1 for char in text if '\u0600' <= char <= '\u06FF' or '\u0750' <= char <= '\u077F')
        # Count English characters
        english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
        # Count all alphabetic characters
        total_alpha = sum(1 for char in text if char.isalpha())
        
        if total_alpha == 0:
            return 'en'
        
        # Calculate ratios
        persian_ratio = (persian_chars + arabic_chars) / total_alpha
        english_ratio = english_chars / total_alpha
        
        # Hybrid detection logic
        if persian_ratio > 0.4:
            return 'fa'  # Primarily Persian
        elif english_ratio > 0.6:
            return 'en'  # Primarily English
        elif persian_ratio > 0.2 and english_ratio > 0.2:
            # Mixed content - check for Persian keywords
            persian_keywords = ['Ø¢Ø¨', 'Ø¯Ù…Ø§', 'Ø±Ø·ÙˆØ¨Øª', 'Ø®Ø§Ú©', 'Ú¯ÛŒØ§Ù‡', 'Ø¢ÙØ§Øª', 'Ù…ØµØ±Ù', 'Ø§Ù…Ø±ÙˆØ²', 'Ø¯ÛŒØ±ÙˆØ²', 'Ù‡ÙØªÙ‡', 'Ù…Ø§Ù‡']
            if any(keyword in text for keyword in persian_keywords):
                return 'fa'
            else:
                return 'en'
        else:
            return 'en'  # Default to English
    
    def translate_query_to_english(self, persian_query: str) -> str:
        """Translate Persian query to natural English using LLM"""
        try:
            # Build few-shot examples string
            examples_str = "\n".join([
                f"Persian: {ex['persian']}\nEnglish: {ex['english']}\n"
                for ex in self.few_shot_examples
            ])
            
            prompt = f"""You are an expert translator specializing in agriculture and greenhouse management queries.

Your task is to translate Persian queries into natural, fluent English that captures the full semantic meaning and context.

Few-shot examples:
{examples_str}

Now translate this Persian query:
Persian: {persian_query}

English:"""

            # Log LLM call with truncated prompt
            truncated_prompt = prompt[:200] + "..." if len(prompt) > 200 else prompt
            logger.info(f"ğŸ¤– LLM Call - Translation (Prompt): {truncated_prompt}")

            response = self.llm.invoke(prompt)
            # Safe accessor for response content (LangChain sometimes returns .text instead of .content)
            translated_query = getattr(response, 'content', None) or getattr(response, 'text', '')
            if not translated_query:
                raise ValueError("Empty response from LLM")
            translated_query = translated_query.strip()
            
            # Log LLM response (truncated)
            truncated_response = translated_query[:100] + "..." if len(translated_query) > 100 else translated_query
            logger.info(f"ğŸ¤– LLM Call - Translation (Response): {truncated_response}")
            
            logger.info(f" LLM Translation: '{persian_query}' -> '{translated_query}'")
            return translated_query
            
        except Exception as e:
            logger.error(f" Translation Error: {str(e)}")
            # Fallback to basic translation
            return self._fallback_translation(persian_query)
    
    def translate_response_to_persian(self, english_response: str) -> str:
        """Translate English response back to Persian using LLM"""
        try:
            prompt = f"""You are an expert translator. Translate this English response about agriculture/greenhouse management into natural Persian:

English: {english_response}

Persian:"""

            # Log LLM call with truncated prompt
            truncated_prompt = prompt[:200] + "..." if len(prompt) > 200 else prompt
            logger.info(f"ğŸ¤– LLM Call - Response Translation (Prompt): {truncated_prompt}")

            response = self.llm.invoke(prompt)
            # Safe accessor for response content (LangChain sometimes returns .text instead of .content)
            persian_response = getattr(response, 'content', None) or getattr(response, 'text', '')
            if not persian_response:
                raise ValueError("Empty response from LLM")
            persian_response = persian_response.strip()
            
            # Fix encoding issues by ensuring UTF-8 encoding
            if isinstance(persian_response, str):
                # Ensure the string is properly encoded
                persian_response = persian_response.encode('utf-8', errors='ignore').decode('utf-8')
            
            # Log LLM response (truncated) with safe encoding
            try:
                truncated_response = persian_response[:100] + "..." if len(persian_response) > 100 else persian_response
                logger.info(f"ğŸ¤– LLM Call - Response Translation (Response): {truncated_response}")
            except UnicodeEncodeError:
                logger.info(f"ğŸ¤– LLM Call - Response Translation (Response): [Persian text - encoding safe]")
            
            # Safe logging with encoding handling
            try:
                logger.info(f" Response Translation: '{english_response[:50]}...' -> '{persian_response[:50]}...'")
            except UnicodeEncodeError:
                logger.info(f" Response Translation: English -> Persian (encoding safe)")
            return persian_response
            
        except Exception as e:
            logger.error(f" Response Translation Error: {str(e)}")
            # Return original response with proper encoding
            try:
                return english_response.encode('utf-8', errors='ignore').decode('utf-8')
            except:
                return "Translation error occurred"
    
    def _fallback_translation(self, persian_text: str) -> str:
        """Fallback word-by-word translation if LLM fails with expanded ontology mappings"""
        translations = {
            # Basic terms
            "Ø¢Ø¨ÛŒØ§Ø±ÛŒ": "irrigation", "Ø§Ø¨ÛŒØ§Ø±ÛŒ": "irrigation", "Ø¢Ø¨": "water", "Ù…ØµØ±Ù Ø¢Ø¨": "water usage",
            "Ø¯Ù…Ø§": "temperature", "Ø¯Ù…Ø§ÛŒ": "temperature", "Ø±Ø·ÙˆØ¨Øª": "humidity", "ÙØ´Ø§Ø±": "pressure",
            "Ø§Ù…Ø±ÙˆØ²": "today", "Ø¯ÛŒØ±ÙˆØ²": "yesterday", "ÙˆØ¶Ø¹ÛŒØª": "status", "Ú†Ø·ÙˆØ±Ù‡": "how is",
            "Ú¯Ù„Ø®Ø§Ù†Ù‡": "greenhouse", "Ú¯Ù„Ø®ÙˆÙ†Ù‡": "greenhouse", "Ø¢ÙØ§Øª": "pests", "Ø¢ÙØª": "pest",
            
            # Time expressions
            "Ù‡ÙØªÙ‡": "week", "Ù…Ø§Ù‡": "month", "Ø³Ø§Ù„": "year", "Ø±ÙˆØ²": "day", "Ø³Ø§Ø¹Øª": "hour",
            "Ú¯Ø°Ø´ØªÙ‡": "past", "Ø§Ø®ÛŒØ±": "recent", "Ù¾ÛŒØ´": "ago", "Ù‚Ø¨Ù„": "before",
            "Ø§ÛŒÙ†": "this", "Ø¢Ù†": "that", "Ø¢Ø®Ø±ÛŒÙ†": "last", "Ø§ÙˆÙ„ÛŒÙ†": "first",
            
            # Sensor types
            "Ø®Ø§Ú©": "soil", "Ú¯ÛŒØ§Ù‡": "plant", "Ø¨Ø±Ú¯": "leaf", "Ù…ÛŒÙˆÙ‡": "fruit",
            "Ù†ÙˆØ±": "light", "Ø¨Ø§Ø¯": "wind", "Ø¨Ø§Ø±Ø§Ù†": "rain", "Ø¨Ø±Ù": "snow",
            "Ú©ÙˆØ¯": "fertilizer", "Ø³Ù…": "pesticide", "Ø¨ÛŒÙ…Ø§Ø±ÛŒ": "disease",
            
            # Actions and comparisons
            "Ù…Ù‚Ø§ÛŒØ³Ù‡": "compare", "ØªÙØ§ÙˆØª": "difference", "Ù†Ø³Ø¨Øª": "ratio", "Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„": "versus",
            "Ø¨ÛŒØ´ØªØ±": "more", "Ú©Ù…ØªØ±": "less", "Ø¨Ø§Ù„Ø§": "high", "Ù¾Ø§ÛŒÛŒÙ†": "low",
            "Ø§ÙØ²Ø§ÛŒØ´": "increase", "Ú©Ø§Ù‡Ø´": "decrease", "ØªØºÛŒÛŒØ±": "change",
            
            # Agricultural terms
            "Ù…Ø­ØµÙˆÙ„": "crop", "Ø¨Ø±Ø¯Ø§Ø´Øª": "harvest", "Ú©Ø§Ø´Øª": "planting", "Ø¢Ø¨ÛŒØ§Ø±ÛŒ": "irrigation",
            "Ú©ÙˆØ¯Ø¯Ù‡ÛŒ": "fertilization", "Ø³Ù…Ù¾Ø§Ø´ÛŒ": "spraying", "Ù‡Ø±Ø³": "pruning",
            "Ø¨Ø°Ø±": "seed", "Ù†Ù‡Ø§Ù„": "sapling", "Ø¯Ø±Ø®Øª": "tree", "Ø¨ÙˆØªÙ‡": "bush",
            
            # Technical terms
            "Ø³Ù†Ø³ÙˆØ±": "sensor", "Ø¯Ø§Ø¯Ù‡": "data", "Ø§Ø·Ù„Ø§Ø¹Ø§Øª": "information", "Ú¯Ø²Ø§Ø±Ø´": "report",
            "ØªØ­Ù„ÛŒÙ„": "analysis", "Ù†ØªÛŒØ¬Ù‡": "result", "Ù…Ù‚Ø¯Ø§Ø±": "value", "Ø¹Ø¯Ø¯": "number"
        }
        
        # Handle long structured JSON responses
        if len(persian_text) > 1000 or ('{' in persian_text and '}' in persian_text):
            # For long or JSON responses, try to translate key phrases only
            key_phrases = []
            for persian_term, english_term in translations.items():
                if persian_term in persian_text:
                    key_phrases.append(f"{persian_term} -> {english_term}")
            
            if key_phrases:
                return f"[Translation failed - key terms: {', '.join(key_phrases[:5])}]"
            else:
                return "[Translation failed - no known terms found]"
        
        # Regular word-by-word translation for short texts
        words = persian_text.split()
        translated_words = []
        
        for word in words:
            if word in translations:
                translated_words.append(translations[word])
            else:
                translated_words.append(word)
        
        return " ".join(translated_words)

    async def stream_response(self, prompt: str):
        """Stream LLM response token by token"""
        try:
            logger.info(f"ğŸš€ Starting streaming response...")
            token_count = 0
            async for chunk in self.llm.astream(prompt):
                if hasattr(chunk, 'content') and chunk.content:
                    token_count += 1
                    # Fix encoding issues by ensuring UTF-8 encoding
                    content = chunk.content
                    if isinstance(content, str):
                        content = content.encode('utf-8', errors='ignore').decode('utf-8')
                    yield content
                else:
                    logger.warning(f" Chunk without content: {chunk}")
            
            logger.info(f"âœ… Streaming completed. Total tokens: {token_count}")
        except Exception as e:
            logger.error(f"âŒ Streaming Error: {str(e)}")
            yield f"Error: {str(e)}"

class SQLValidator:
    """Validates and sanitizes SQL queries"""
    
    def __init__(self, sql_db):
        self.sql_db = sql_db
    
    def validate_query(self, query: str) -> Dict[str, Any]:
        """Validate SQL query for safety and correctness with column whitelist"""
        try:
            # Check for dangerous operations
            dangerous_keywords = ['DROP TABLE', 'DELETE FROM', 'UPDATE SET', 'INSERT INTO', 'ALTER TABLE', 'CREATE TABLE', 'TRUNCATE TABLE']
            query_upper = query.upper()
            
            for keyword in dangerous_keywords:
                if keyword in query_upper:
                    return {
                        "valid": False,
                        "message": f"Dangerous operation detected: {keyword}",
                        "suggestions": ["Use SELECT queries only"]
                    }
            
            # Check if query uses allowed tables (allow SELECT * for exploration)
            if 'sensor_data' not in query_upper and 'SELECT *' not in query_upper:
                return {
                    "valid": False,
                    "message": "Query must use sensor_data table only",
                    "suggestions": ["Use SELECT * FROM sensor_data WHERE ..."]
                }
            
            # Basic syntax check
            if not query_upper.strip().startswith('SELECT'):
                return {
                    "valid": False,
                    "message": "Only SELECT queries are allowed",
                    "suggestions": ["Start your query with SELECT"]
                }
            
            # Column whitelist validation (skip for SELECT * queries)
            allowed_columns = [
                'timestamp', 'sensor_type', 'value', 'location', 'unit',
                'time_period', 'avg_value', 'min_value', 'max_value', 'data_points'
            ]
            
            # Extract column names from SELECT clause
            import re
            select_match = re.search(r'SELECT\s+(.*?)\s+FROM', query_upper, re.IGNORECASE)
            if select_match:
                select_clause = select_match.group(1)
                # Handle * and specific columns
                if select_clause.strip() != '*':
                    # Extract column names (handle aliases)
                    columns = re.findall(r'(\w+)(?:\s+AS\s+\w+)?', select_clause)
                    for column in columns:
                        if column.upper() not in [col.upper() for col in allowed_columns]:
                            return {
                                "valid": False,
                                "message": f"Column '{column}' is not allowed",
                                "suggestions": [f"Use only allowed columns: {', '.join(allowed_columns)}"]
                }
            
            return {
                "valid": True,
                "message": "Query is valid",
                "suggestions": []
            }
            
        except Exception as e:
            return {
                "valid": False,
                "message": f"Validation error: {str(e)}",
                "suggestions": ["Check query syntax"]
            }

class UnifiedSemanticQueryService:
    """Unified service combining semantic layer and dynamic query generation"""
    
    def __init__(self):
        # Initialize LLM
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        self.model_name = os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini")
        
        # Log configuration for debugging
        logger.info(f" LLM Configuration:")
        logger.info(f"   API Key: {'SET' if self.api_key and len(self.api_key) > 10 else 'NOT SET'}")
        logger.info(f"   Base URL: {self.base_url}")
        logger.info(f"   Model: {self.model_name}")
        
        if self.api_key and self.api_key != "your-openai-api-key-here" and len(self.api_key) > 10:
            self.llm = ChatOpenAI(
                openai_api_key=self.api_key,
                openai_api_base=self.base_url,
                model_name=self.model_name,
                temperature=0.1,
                streaming=True  # Enable streaming
            )
            logger.info(f" Unified Service: Real LLM initialized: {self.model_name}")
            logger.info(f" Unified Service: API Base URL: {self.base_url}")
        else:
            logger.error(f" Unified Service: Invalid API key - length: {len(self.api_key) if self.api_key else 0}")
            logger.error(f" Unified Service: API key value: {self.api_key}")
            raise ValueError(" Unified Service: OpenAI API key is required. Please set OPENAI_API_KEY environment variable.")
        
        # Initialize components
        self.translator = LLMTranslator(self.llm)
        
        # Initialize SQL Database connection for dynamic query generation
        self.db_engine = self._initialize_db_engine()
        self.sql_db = SQLDatabase(self.db_engine)
        self.sql_toolkit = SQLDatabaseToolkit(db=self.sql_db, llm=self.llm)
        
        # Create SQL agent for dynamic query generation
        self.sql_agent = create_sql_agent(
            llm=self.llm,
            toolkit=self.sql_toolkit,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=15,
            agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            system_message=self._get_sql_system_message()
        )
        
        self.validator = SQLValidator(self.sql_db)
        
        # Build comprehensive ontology
        self.ontology = self._build_comprehensive_ontology()
        
        # Initialize QueryBuilder for semantic JSON to SQL conversion
        self.query_builder = QueryBuilder(self.ontology)
        
        # Initialize conversation memory (session-based)
        self.conversation_memories = {}  # session_id -> ConversationBufferMemory
        
        logger.info(" UnifiedSemanticQueryService initialized successfully with conversation history and QueryBuilder")
    
    def _get_or_create_memory(self, session_id: str) -> ConversationBufferMemory:
        """Get or create conversation memory for a session"""
        if session_id not in self.conversation_memories:
            self.conversation_memories[session_id] = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="output",
                k=10  # Keep last 10 messages
            )
            logger.info(f" Created new conversation memory for session: {session_id}")
        return self.conversation_memories[session_id]
    
    def _get_conversation_context(self, session_id: str) -> str:
        """Get conversation history as context string"""
        memory = self._get_or_create_memory(session_id)
        chat_history = memory.chat_memory.messages
        
        if not chat_history:
            return "No previous conversation history."
        
        # Format conversation history
        context_lines = []
        for i, message in enumerate(chat_history[-10:]):  # Last 10 messages
            role = "User" if message.type == "human" else "Assistant"
            content = message.content[:200] + "..." if len(message.content) > 200 else message.content
            context_lines.append(f"{role}: {content}")
        
        return "\n".join(context_lines)
    
    def _save_conversation_history(self, session_id: str, user_query: str, assistant_response: str):
        """Save conversation history for a session"""
        try:
            memory = self._get_or_create_memory(session_id)
            
            # Add user message
            memory.chat_memory.add_user_message(user_query)
            
            # Add assistant response
            memory.chat_memory.add_ai_message(assistant_response)
            
            logger.info(f" Saved conversation history for session: {session_id}")
            
        except Exception as e:
            logger.error(f" Error saving conversation history: {str(e)}")
    
    def _initialize_db_engine(self):
        """Initialize database engine for SQLite"""
        try:
            from sqlalchemy import create_engine
            # For SQLite, create a proper SQLAlchemy engine with Liara path
            if os.getenv("LIARA_APP_ID"):
                db_dir = "/var/lib/data"
                os.makedirs(db_dir, exist_ok=True)
                db_path = os.path.join(db_dir, "smart_dashboard.db")
                engine = create_engine(f"sqlite:///{db_path}")
            else:
                engine = create_engine("sqlite:///smart_dashboard.db")
            return engine
        except Exception as e:
            logger.error(f"Database initialization error: {e}")
            return None
    
    def _get_sql_system_message(self):
        """Get system message for SQL agent"""
        return """You are an AI assistant specialized in generating SQL queries for an agriculture sensor database.

DATABASE SCHEMA:
- Table: sensor_data
  - Columns: id (INTEGER), timestamp (TEXT), sensor_type (TEXT), value (REAL)
  - Sensor types: temperature, humidity, pressure, light, co2_level, wind_speed, rainfall, soil_temperature, soil_moisture, water_usage, pest_count, disease_risk

RULES:
1. Only use SELECT queries
2. Always use the sensor_data table
3. Be specific with WHERE clauses
4. Use appropriate date/time functions for filtering
5. Return structured results

EXAMPLES:
- "last irrigation" -> SELECT * FROM sensor_data WHERE sensor_type = 'water_usage' ORDER BY timestamp DESC LIMIT 1;
- "temperature today" -> SELECT * FROM sensor_data WHERE sensor_type = 'temperature' AND DATE(timestamp) = DATE('now') ORDER BY timestamp DESC;
- "pest trend last 7 days" -> SELECT sensor_type, COUNT(*) as count, AVG(value) as avg_value FROM sensor_data WHERE sensor_type = 'pest_count' AND timestamp >= datetime('now', '-7 days') GROUP BY sensor_type;
"""
    
    def _build_comprehensive_ontology(self) -> Dict[str, Any]:
        """Build comprehensive ontology for agriculture management"""
        return {
            "sensor_mappings": {
                # Environmental Sensors
                "temperature": {
                    "persian_terms": ["Ø¯Ù…Ø§", "Ú¯Ø±Ù…Ø§", "Ø­Ø±Ø§Ø±Øª", "Ø¯Ø±Ø¬Ù‡ Ø­Ø±Ø§Ø±Øª", "Ø¯Ù…Ø§ÛŒ Ù‡ÙˆØ§", "Ø¯Ø±Ø¬Ù‡", "Ú¯Ø±Ù…Ø§ÛŒ Ù‡ÙˆØ§", "Ø­Ø±Ø§Ø±Øª Ù‡ÙˆØ§", "Ø¯Ù…Ø§ÛŒ Ù…Ø­ÛŒØ·", "Ú¯Ø±Ù…Ø§ÛŒ Ù…Ø­ÛŒØ·", "Ø­Ø±Ø§Ø±Øª Ù…Ø­ÛŒØ·", "Ø¯Ù…Ø§ÛŒ Ú¯Ù„Ø®Ø§Ù†Ù‡", "Ú¯Ø±Ù…Ø§ÛŒ Ú¯Ù„Ø®Ø§Ù†Ù‡", "Ø­Ø±Ø§Ø±Øª Ú¯Ù„Ø®Ø§Ù†Ù‡"],
                    "english_terms": ["temperature", "temp", "heat", "thermal", "air temperature", "degree", "air heat", "air thermal", "ambient temperature", "ambient heat", "ambient thermal", "greenhouse temperature", "greenhouse heat", "greenhouse thermal"],
                    "unit": "Â°C",
                    "range": {"min": 15.02, "max": 34.99, "avg": 21.51},
                    "description": "Air temperature readings"
                },
                "humidity": {
                    "persian_terms": ["Ø±Ø·ÙˆØ¨Øª", "Ù†Ù…", "Ø´Ø±Ø¬ÛŒ", "Ø±Ø·ÙˆØ¨Øª Ù‡ÙˆØ§", "Ø±Ø·ÙˆØ¨Øª Ù…Ø­ÛŒØ·", "Ù†Ù… Ù‡ÙˆØ§", "Ø´Ø±Ø¬ÛŒ Ù‡ÙˆØ§", "Ø±Ø·ÙˆØ¨Øª Ú¯Ù„Ø®Ø§Ù†Ù‡", "Ù†Ù… Ú¯Ù„Ø®Ø§Ù†Ù‡", "Ø´Ø±Ø¬ÛŒ Ú¯Ù„Ø®Ø§Ù†Ù‡", "Ø±Ø·ÙˆØ¨Øª Ù†Ø³Ø¨ÛŒ", "Ù†Ù… Ù†Ø³Ø¨ÛŒ"],
                    "english_terms": ["humidity", "moisture", "dampness", "air humidity", "ambient humidity", "air moisture", "air dampness", "greenhouse humidity", "greenhouse moisture", "greenhouse dampness", "relative humidity", "relative moisture"],
                    "unit": "%",
                    "range": {"min": 22.50, "max": 89.99, "avg": 72.68},
                    "description": "Air humidity percentage"
                },
                "pressure": {
                    "persian_terms": ["ÙØ´Ø§Ø±", "Ù‡ÙˆØ§", "Ø¨Ø§Ø±ÙˆÙ…ØªØ±", "ÙØ´Ø§Ø± Ù‡ÙˆØ§"],
                    "english_terms": ["pressure", "atmospheric", "barometric"],
                    "unit": "hPa",
                    "range": {"min": 25.00, "max": 1030.00, "avg": 1004.81},
                    "description": "Atmospheric pressure readings"
                },
                "light": {
                    "persian_terms": ["Ù†ÙˆØ±", "Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ", "Ù„Ø§Ù…Ù¾", "Ù†ÙˆØ± Ø®ÙˆØ±Ø´ÛŒØ¯"],
                    "english_terms": ["light", "brightness", "illumination", "lux"],
                    "unit": "lux",
                    "range": {"min": 0.00, "max": 999.60, "avg": 277.57},
                    "description": "Light intensity measurements"
                },
                "co2_level": {
                    "persian_terms": ["Ø¯ÛŒ Ø§Ú©Ø³ÛŒØ¯ Ú©Ø±Ø¨Ù†", "co2", "Ú©Ø±Ø¨Ù† Ø¯ÛŒ Ø§Ú©Ø³ÛŒØ¯"],
                    "english_terms": ["co2", "carbon dioxide", "co2 level"],
                    "unit": "ppm",
                    "range": {"min": 300.00, "max": 600.00, "avg": 425.54},
                    "description": "Carbon dioxide concentration"
                },
                "wind_speed": {
                    "persian_terms": ["Ø³Ø±Ø¹Øª Ø¨Ø§Ø¯", "Ø¨Ø§Ø¯", "Ø³Ø±Ø¹Øª ÙˆØ²Ø´ Ø¨Ø§Ø¯"],
                    "english_terms": ["wind", "wind speed", "air velocity"],
                    "unit": "m/s",
                    "range": {"min": 0.00, "max": 20.00, "avg": 10.26},
                    "description": "Wind speed measurements"
                },
                "rainfall": {
                    "persian_terms": ["Ø¨Ø§Ø±Ø§Ù†", "Ø¨Ø§Ø±Ù†Ø¯Ú¯ÛŒ", "Ù…ÛŒØ²Ø§Ù† Ø¨Ø§Ø±Ø§Ù†"],
                    "english_terms": ["rain", "rainfall", "precipitation"],
                    "unit": "mm",
                    "range": {"min": 0.00, "max": 9.97, "avg": 0.55},
                    "description": "Rainfall measurements"
                },
                
                # Soil Sensors
                "soil_moisture": {
                    "persian_terms": ["Ø±Ø·ÙˆØ¨Øª Ø®Ø§Ú©", "Ù†Ù… Ø®Ø§Ú©", "Ø¢Ø¨ Ø®Ø§Ú©", "Ø®Ø§Ú©", "Ø²Ù…ÛŒÙ†", "Ø¨Ø³ØªØ±", "Ø±Ø·ÙˆØ¨Øª Ø²Ù…ÛŒÙ†", "Ø¢Ø¨ Ø²Ù…ÛŒÙ†", "Ù†Ù… Ø²Ù…ÛŒÙ†", "Ø®Ø§Ú© Ù…Ø±Ø·ÙˆØ¨", "Ø²Ù…ÛŒÙ† Ù…Ø±Ø·ÙˆØ¨", "Ø±Ø·ÙˆØ¨Øª Ø¨Ø³ØªØ±", "Ø¢Ø¨ Ø¨Ø³ØªØ±", "Ù†Ù… Ø¨Ø³ØªØ±"],
                    "english_terms": ["soil moisture", "soil water", "ground moisture", "soil", "ground", "substrate", "ground water", "soil wetness", "ground wetness", "substrate moisture", "substrate water", "substrate wetness"],
                    "unit": "%",
                    "range": {"min": 20.00, "max": 79.86, "avg": 51.18},
                    "description": "Soil moisture percentage"
                },
                "soil_ph": {
                    "persian_terms": ["Ù¾ÛŒ Ø§Ú† Ø®Ø§Ú©", "Ø§Ø³ÛŒØ¯ÛŒØªÙ‡ Ø®Ø§Ú©", "ph Ø®Ø§Ú©"],
                    "english_terms": ["soil ph", "soil acidity", "ph level"],
                    "unit": "pH",
                    "range": {"min": 5.50, "max": 7.50, "avg": 6.67},
                    "description": "Soil pH level"
                },
                "soil_temperature": {
                    "persian_terms": ["Ø¯Ù…Ø§ÛŒ Ø®Ø§Ú©", "Ø­Ø±Ø§Ø±Øª Ø®Ø§Ú©", "Ú¯Ø±Ù…Ø§ÛŒ Ø®Ø§Ú©"],
                    "english_terms": ["soil temperature", "ground temperature"],
                    "unit": "Â°C",
                    "range": {"min": 15.00, "max": 28.00, "avg": 22.30},
                    "description": "Soil temperature readings"
                },
                
                # Plant Growth Sensors
                "plant_height": {
                    "persian_terms": ["Ù‚Ø¯ Ú¯ÛŒØ§Ù‡", "Ø§Ø±ØªÙØ§Ø¹ Ú¯ÛŒØ§Ù‡", "Ø¨Ù„Ù†Ø¯ÛŒ Ú¯ÛŒØ§Ù‡"],
                    "english_terms": ["plant height", "plant growth", "height"],
                    "unit": "cm",
                    "range": {"min": 10.00, "max": 188.54, "avg": 19.99},
                    "description": "Plant height measurements"
                },
                "fruit_count": {
                    "persian_terms": ["ØªØ¹Ø¯Ø§Ø¯ Ù…ÛŒÙˆÙ‡", "Ø´Ù…Ø§Ø± Ù…ÛŒÙˆÙ‡", "Ù…ÛŒÙˆÙ‡ Ù‡Ø§"],
                    "english_terms": ["fruit count", "fruit number", "fruits"],
                    "unit": "count",
                    "range": {"min": 0.00, "max": 19.90, "avg": 1.72},
                    "description": "Number of fruits per plant"
                },
                "fruit_size": {
                    "persian_terms": ["Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…ÛŒÙˆÙ‡", "Ø³Ø§ÛŒØ² Ù…ÛŒÙˆÙ‡", "Ø¨Ø²Ø±Ú¯ÛŒ Ù…ÛŒÙˆÙ‡"],
                    "english_terms": ["fruit size", "fruit diameter", "size"],
                    "unit": "cm",
                    "range": {"min": 1.00, "max": 1.90, "avg": 1.21},
                    "description": "Fruit size measurements"
                },
                
                # Nutrient Sensors
                "nitrogen_level": {
                    "persian_terms": ["Ù†ÛŒØªØ±ÙˆÚ˜Ù†", "Ø§Ø²Øª", "Ø³Ø·Ø­ Ù†ÛŒØªØ±ÙˆÚ˜Ù†"],
                    "english_terms": ["nitrogen", "n level", "nitrogen content"],
                    "unit": "ppm",
                    "range": {"min": 20.00, "max": 96.57, "avg": 62.96},
                    "description": "Nitrogen level in soil"
                },
                "phosphorus_level": {
                    "persian_terms": ["ÙØ³ÙØ±", "Ø³Ø·Ø­ ÙØ³ÙØ±", "Ù…Ù‚Ø¯Ø§Ø± ÙØ³ÙØ±"],
                    "english_terms": ["phosphorus", "p level", "phosphorus content"],
                    "unit": "ppm",
                    "range": {"min": 10.00, "max": 50.00, "avg": 34.97},
                    "description": "Phosphorus level in soil"
                },
                "potassium_level": {
                    "persian_terms": ["Ù¾ØªØ§Ø³ÛŒÙ…", "Ø³Ø·Ø­ Ù¾ØªØ§Ø³ÛŒÙ…", "Ù…Ù‚Ø¯Ø§Ø± Ù¾ØªØ§Ø³ÛŒÙ…"],
                    "english_terms": ["potassium", "k level", "potassium content"],
                    "unit": "ppm",
                    "range": {"min": 30.00, "max": 145.65, "avg": 95.59},
                    "description": "Potassium level in soil"
                },
                
                # Pest & Disease Sensors
                "pest_count": {
                    "persian_terms": ["ØªØ¹Ø¯Ø§Ø¯ Ø¢ÙØª", "Ø´Ù…Ø§Ø± Ø¢ÙØª", "Ø¢ÙØ§Øª", "Ø¢ÙØª", "Ø­Ø´Ø±Ù‡", "Ø­Ø´Ø±Ø§Øª", "Ù…Ú¯Ø³", "Ù…Ú¯Ø³â€ŒÙ‡Ø§", "Ù¾Ø´Ù‡", "Ù¾Ø´Ù‡â€ŒÙ‡Ø§", "Ú©Ø±Ù…", "Ú©Ø±Ù…â€ŒÙ‡Ø§", "Ù„Ø§Ø±Ùˆ", "Ù„Ø§Ø±ÙˆÙ‡Ø§", "ØªØ®Ù…", "ØªØ®Ù…â€ŒÙ‡Ø§", "Ø¢ÙØªâ€ŒÙ‡Ø§", "Ø¢ÙØ§Øª Ù…Ø¶Ø±", "Ø­Ø´Ø±Ø§Øª Ù…Ø¶Ø±", "Ø±Ø´Ø¯ Ø¢ÙØ§Øª", "Ø±Ø´Ø¯ Ø¢ÙØª", "Ø§ÙØ²Ø§ÛŒØ´ Ø¢ÙØ§Øª", "Ø§ÙØ²Ø§ÛŒØ´ Ø¢ÙØª"],
                    "english_terms": ["pest count", "pest number", "pests", "pest", "insect", "insects", "fly", "flies", "mosquito", "mosquitoes", "worm", "worms", "larva", "larvae", "egg", "eggs", "harmful pests", "harmful insects"],
                    "unit": "count",
                    "range": {"min": 0.00, "max": 48.32, "avg": 1.43},
                    "description": "Number of pests detected"
                },
                "pest_detection": {
                    "persian_terms": ["ØªØ´Ø®ÛŒØµ Ø¢ÙØª", "Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¢ÙØª", "Ø¢ÙØª ÛŒØ§Ø¨ÛŒ"],
                    "english_terms": ["pest detection", "pest identified", "detection"],
                    "unit": "binary",
                    "range": {"min": 0.00, "max": 1.00, "avg": 0.02},
                    "description": "Pest detection status"
                },
                "disease_risk": {
                    "persian_terms": ["Ø®Ø·Ø± Ø¨ÛŒÙ…Ø§Ø±ÛŒ", "Ø±ÛŒØ³Ú© Ø¨ÛŒÙ…Ø§Ø±ÛŒ", "Ø§Ø­ØªÙ…Ø§Ù„ Ø¨ÛŒÙ…Ø§Ø±ÛŒ"],
                    "english_terms": ["disease risk", "risk level", "disease probability"],
                    "unit": "%",
                    "range": {"min": 0.00, "max": 98.44, "avg": 47.28},
                    "description": "Disease risk percentage"
                },
                
                # Water Management
                "water_usage": {
                    "persian_terms": ["Ù…ØµØ±Ù Ø¢Ø¨", "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¢Ø¨", "Ø¢Ø¨ Ù…ØµØ±ÙÛŒ", "Ø¢Ø¨ÛŒØ§Ø±ÛŒ", "Ø§Ø¨ÛŒØ§Ø±ÛŒ", "Ø¢Ø¨", "Ø¢Ø¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡", "Ù…ØµØ±Ù Ø¢Ø¨", "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ø¨", "Ø¢Ø¨ Ù…ØµØ±Ù Ø´Ø¯Ù‡", "Ù…Ù‚Ø¯Ø§Ø± Ø¢Ø¨", "Ø­Ø¬Ù… Ø¢Ø¨", "Ù„ÛŒØªØ± Ø¢Ø¨", "Ø¢Ø¨ Ù„ÛŒØªØ±ÛŒ"],
                    "english_terms": ["water usage", "water consumption", "water used", "irrigation", "watering", "water", "water consumed", "water usage", "water use", "water consumed", "water amount", "water volume", "water liters", "liters of water"],
                    "unit": "L",
                    "range": {"min": 0.00, "max": 49.74, "avg": 4.33},
                    "description": "Water usage in liters"
                },
                "water_efficiency": {
                    "persian_terms": ["Ø¨Ø§Ø²Ø¯Ù‡ÛŒ Ø¢Ø¨", "Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¢Ø¨", "Ù…ØµØ±Ù Ø¨Ù‡ÛŒÙ†Ù‡ Ø¢Ø¨"],
                    "english_terms": ["water efficiency", "water optimization", "efficiency"],
                    "unit": "%",
                    "range": {"min": 61.52, "max": 94.86, "avg": 81.15},
                    "description": "Water usage efficiency"
                },
                
                # Yield & Production
                "yield_prediction": {
                    "persian_terms": ["Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ù…Ø­ØµÙˆÙ„", "ØªØ®Ù…ÛŒÙ† Ù…Ø­ØµÙˆÙ„", "Ù…Ø­ØµÙˆÙ„ Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ"],
                    "english_terms": ["yield prediction", "crop yield", "predicted yield"],
                    "unit": "kg",
                    "range": {"min": 100.00, "max": 174.51, "avg": 105.50},
                    "description": "Predicted crop yield"
                },
                "yield_efficiency": {
                    "persian_terms": ["Ø¨Ø§Ø²Ø¯Ù‡ÛŒ Ù…Ø­ØµÙˆÙ„", "Ú©Ø§Ø±Ø§ÛŒÛŒ Ù…Ø­ØµÙˆÙ„", "Ø¨Ù‡Ø±Ù‡ ÙˆØ±ÛŒ"],
                    "english_terms": ["yield efficiency", "production efficiency", "efficiency"],
                    "unit": "%",
                    "range": {"min": 62.57, "max": 89.86, "avg": 86.40},
                    "description": "Crop yield efficiency"
                },
                
                # Market & Economics
                "tomato_price": {
                    "persian_terms": ["Ù‚ÛŒÙ…Øª Ú¯ÙˆØ¬Ù‡", "Ø¨Ù‡Ø§ÛŒ Ú¯ÙˆØ¬Ù‡", "Ú¯ÙˆØ¬Ù‡ Ù‚ÛŒÙ…Øª"],
                    "english_terms": ["tomato price", "price per kg", "market price"],
                    "unit": "price_per_kg",  # Normalized: removed $ unit not in DB schema
                    "range": {"min": 1.50, "max": 4.46, "avg": 2.71},
                    "description": "Tomato market price"
                },
                "lettuce_price": {
                    "persian_terms": ["Ù‚ÛŒÙ…Øª Ú©Ø§Ù‡Ùˆ", "Ø¨Ù‡Ø§ÛŒ Ú©Ø§Ù‡Ùˆ", "Ú©Ø§Ù‡Ùˆ Ù‚ÛŒÙ…Øª"],
                    "english_terms": ["lettuce price", "price per head", "market price"],
                    "unit": "price_per_head",  # Normalized: removed $ unit not in DB schema
                    "range": {"min": 0.80, "max": 2.98, "avg": 1.59},
                    "description": "Lettuce market price"
                },
                "pepper_price": {
                    "persian_terms": ["Ù‚ÛŒÙ…Øª ÙÙ„ÙÙ„", "Ø¨Ù‡Ø§ÛŒ ÙÙ„ÙÙ„", "ÙÙ„ÙÙ„ Ù‚ÛŒÙ…Øª"],
                    "english_terms": ["pepper price", "price per kg", "market price"],
                    "unit": "price_per_kg",  # Normalized: removed $ unit not in DB schema
                    "range": {"min": 2.01, "max": 5.00, "avg": 3.40},
                    "description": "Pepper market price"
                },
                
                # Additional Sensors
                "motion": {
                    "persian_terms": ["Ø­Ø±Ú©Øª", "Ø¬Ù†Ø¨Ø´", "ÙØ¹Ø§Ù„ÛŒØª"],
                    "english_terms": ["motion", "movement", "activity"],
                    "unit": "count",
                    "range": {"min": 0.00, "max": 30.00, "avg": 0.52},
                    "description": "Motion detection"
                },
                "fertilizer_usage": {
                    "persian_terms": ["Ù…ØµØ±Ù Ú©ÙˆØ¯", "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©ÙˆØ¯", "Ú©ÙˆØ¯ Ù…ØµØ±ÙÛŒ"],
                    "english_terms": ["fertilizer usage", "fertilizer consumption", "fertilizer used"],
                    "unit": "kg",
                    "range": {"min": 0.00, "max": 5.00, "avg": 0.38},
                    "description": "Fertilizer usage in kilograms"
                },
                "energy_usage": {
                    "persian_terms": ["Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ", "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø±Ù‚", "Ø§Ù†Ø±Ú˜ÛŒ Ù…ØµØ±ÙÛŒ"],
                    "english_terms": ["energy usage", "power consumption", "energy used"],
                    "unit": "kWh",
                    "range": {"min": 10.00, "max": 195.80, "avg": 26.05},
                    "description": "Energy consumption in kilowatt-hours"
                },
                
                # Normalized: co2 -> co2_level (removed duplicate)
                # Normalized: cost_per_kg -> tomato_price (removed duplicate)
                "demand_level": {
                    "persian_terms": ["Ø³Ø·Ø­ ØªÙ‚Ø§Ø¶Ø§", "Ù…ÛŒØ²Ø§Ù† ØªÙ‚Ø§Ø¶Ø§", "ØªÙ‚Ø§Ø¶Ø§"],
                    "english_terms": ["demand level", "demand", "market demand"],
                    "unit": "level",  # Normalized: removed % unit not in DB schema
                    "range": {"min": 0.00, "max": 100.00, "avg": 75.00},
                    "description": "Market demand level"
                },
                "leaf_count": {
                    "persian_terms": ["ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø±Ú¯", "Ø´Ù…Ø§Ø± Ø¨Ø±Ú¯", "Ø¨Ø±Ú¯ Ù‡Ø§"],
                    "english_terms": ["leaf count", "leaf number", "leaves"],
                    "unit": "count",
                    "range": {"min": 5.00, "max": 50.00, "avg": 25.00},
                    "description": "Number of leaves per plant"
                },
                "leaf_wetness": {
                    "persian_terms": ["Ø±Ø·ÙˆØ¨Øª Ø¨Ø±Ú¯", "Ù†Ù… Ø¨Ø±Ú¯", "Ø¢Ø¨ Ø¨Ø±Ú¯"],
                    "english_terms": ["leaf wetness", "leaf moisture", "foliar wetness"],
                    "unit": "%",
                    "range": {"min": 0.00, "max": 100.00, "avg": 45.00},
                    "description": "Leaf wetness percentage"
                },
                "nutrient_uptake": {
                    "persian_terms": ["Ø¬Ø°Ø¨ Ù…ÙˆØ§Ø¯ Ù…ØºØ°ÛŒ", "Ù…ØµØ±Ù Ù…ÙˆØ§Ø¯ Ù…ØºØ°ÛŒ", "Ù…ÙˆØ§Ø¯ Ù…ØºØ°ÛŒ"],
                    "english_terms": ["nutrient uptake", "nutrient absorption", "nutrient consumption"],
                    "unit": "mg/L",
                    "range": {"min": 0.00, "max": 100.00, "avg": 50.00},
                    "description": "Nutrient uptake rate"
                },
                "profit_margin": {
                    "persian_terms": ["Ø­Ø§Ø´ÛŒÙ‡ Ø³ÙˆØ¯", "Ø³ÙˆØ¯", "Ø¯Ø±Ø¢Ù…Ø¯"],
                    "english_terms": ["profit margin", "profit", "revenue"],
                    "unit": "%",
                    "range": {"min": 15.00, "max": 35.00, "avg": 25.00},
                    "description": "Profit margin percentage"
                },
                "supply_level": {
                    "persian_terms": ["Ø³Ø·Ø­ Ø¹Ø±Ø¶Ù‡", "Ù…ÛŒØ²Ø§Ù† Ø¹Ø±Ø¶Ù‡", "Ø¹Ø±Ø¶Ù‡"],
                    "english_terms": ["supply level", "supply", "market supply"],
                    "unit": "%",
                    "range": {"min": 0.00, "max": 100.00, "avg": 80.00},
                    "description": "Market supply level percentage"
                },
                "test_temperature": {
                    "persian_terms": ["Ø¯Ù…Ø§ ØªØ³Øª", "Ø¯Ù…Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´", "ØªØ³Øª Ø¯Ù…Ø§"],
                    "english_terms": ["test temperature", "testing temperature", "experimental temperature"],
                    "unit": "Â°C",
                    "range": {"min": 15.00, "max": 35.00, "avg": 25.00},
                    "description": "Test temperature readings"
                }
            },
            "query_patterns": {
                "current_value": {
                    "persian": ["Ø§Ù„Ø§Ù†", "ÙØ¹Ù„Ø§", "Ø­Ø§Ù„Ø§", "Ø§Ú©Ù†ÙˆÙ†", "Ø§Ù…Ø±ÙˆØ²"],
                    "english": ["current", "now", "today", "latest", "recent"],
                    "sql_pattern": "SELECT * FROM sensor_data WHERE sensor_type = '{sensor_type}' ORDER BY timestamp DESC LIMIT 1"
                },
                "average_value": {
                    "persian": ["Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†", "Ù…ØªÙˆØ³Ø·", "Ù…ÛŒØ§Ù†Ù‡"],
                    "english": ["average", "mean", "avg"],
                    "sql_pattern": "SELECT AVG(value) as avg_value FROM sensor_data WHERE sensor_type = '{sensor_type}'"
                },
                "trend_analysis": {
                    "persian": ["Ø±ÙˆÙ†Ø¯", "ØªØºÛŒÛŒØ±Ø§Øª", "Ù†ÙˆØ³Ø§Ù†Ø§Øª"],
                    "english": ["trend", "changes", "fluctuations"],
                    "sql_pattern": "SELECT timestamp, value FROM sensor_data WHERE sensor_type = '{sensor_type}' ORDER BY timestamp DESC LIMIT 10"
                },
                "comparison": {
                    "persian": ["Ù…Ù‚Ø§ÛŒØ³Ù‡", "ØªÙØ§ÙˆØª", "ÙØ±Ù‚"],
                    "english": ["compare", "difference", "vs"],
                    "sql_pattern": "SELECT sensor_type, AVG(value) as avg_value FROM sensor_data WHERE sensor_type IN ('{sensor1}', '{sensor2}') GROUP BY sensor_type"
                },
                "time_based": {
                    "persian": ["Ø³Ù‡ Ø±ÙˆØ²", "Ú†Ù‡Ø§Ø± Ø±ÙˆØ²", "Ù¾Ù†Ø¬ Ø±ÙˆØ²", "Ø´Ø´ Ø±ÙˆØ²", "Ù‡ÙØª Ø±ÙˆØ²", "ÛŒÚ© Ù‡ÙØªÙ‡", "Ø¯Ùˆ Ù‡ÙØªÙ‡", "Ø³Ù‡ Ù‡ÙØªÙ‡", "ÛŒÚ© Ù…Ø§Ù‡", "Ø¯Ùˆ Ù…Ø§Ù‡", "Ø³Ù‡ Ù…Ø§Ù‡", "Ø§Ø®ÛŒØ±", "Ú¯Ø°Ø´ØªÙ‡", "Ù‚Ø¨Ù„ÛŒ"],
                    "english": ["three days", "four days", "five days", "six days", "seven days", "one week", "two weeks", "three weeks", "one month", "two months", "three months", "last", "past", "previous"],
                    "sql_pattern": "SELECT timestamp, value FROM sensor_data WHERE sensor_type = '{sensor_type}' AND timestamp >= datetime('now', '-{days} days') ORDER BY timestamp DESC"
                }
            },
            "chart_patterns": {
                "trend_chart": {
                    "persian": [
                        "Ø±ÙˆÙ†Ø¯", "Ù†Ù…ÙˆØ¯Ø§Ø±", "Ú¯Ø±Ø§Ù", "ØªØºÛŒÛŒØ±Ø§Øª", "Ù†ÙˆØ³Ø§Ù†Ø§Øª", "Ù…Ù‚ÛŒØ§Ø³ Ø²Ù…Ø§Ù†ÛŒ", 
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯", "Ú¯Ø±Ø§Ù ØªØºÛŒÛŒØ±Ø§Øª", "Ø±Ø´Ø¯", "Ø§ÙØ²Ø§ÛŒØ´", "Ú©Ø§Ù‡Ø´", 
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±Ø´Ø¯", "Ú¯Ø±Ø§Ù Ø±Ø´Ø¯", "Ø±ÙˆÙ†Ø¯ Ø±Ø´Ø¯", "ØªØºÛŒÛŒØ±Ø§Øª Ø±Ø´Ø¯",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø²Ù…Ø§Ù†ÛŒ", "Ú¯Ø±Ø§Ù Ø²Ù…Ø§Ù†ÛŒ", "Ø±ÙˆÙ†Ø¯ Ø²Ù…Ø§Ù†ÛŒ", "ØªØºÛŒÛŒØ±Ø§Øª Ø²Ù…Ø§Ù†ÛŒ",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ù‡ Ø±ÙˆØ²", "Ù†Ù…ÙˆØ¯Ø§Ø± Ù‡ÙØªÙ‡", "Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ø§Ù‡", "Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø§Ù„",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ø®ÛŒØ±", "Ù†Ù…ÙˆØ¯Ø§Ø± Ú¯Ø°Ø´ØªÙ‡", "Ù†Ù…ÙˆØ¯Ø§Ø± ÙØ¹Ù„ÛŒ"
                    ],
                    "english": [
                        "trend", "chart", "graph", "changes", "fluctuations", "over time", 
                        "timeline", "trend chart", "line chart", "growth", "increase", 
                        "decrease", "growth chart", "growth graph", "growth trend",
                        "time chart", "time graph", "time trend", "time changes",
                        "last 3 days", "last week", "last month", "last year",
                        "recent", "past", "current", "over time"
                    ],
                    "chart_type": "line"
                },
                "comparison_chart": {
                    "persian": [
                        "Ù…Ù‚Ø§ÛŒØ³Ù‡", "ØªÙØ§ÙˆØª", "ÙØ±Ù‚", "Ù†Ø³Ø¨Øª", "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨ÛŒÙ†", "Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡", 
                        "Ú¯Ø±Ø§Ù Ù…Ù‚Ø§ÛŒØ³Ù‡", "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§", "Ù†Ø³Ø¨Øª Ø¨Ù‡", "Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„", "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ùˆ",
                        "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú†Ù†Ø¯", "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‡Ù…Ù‡", "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù„", "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¬Ø²Ø¦ÛŒ",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø³ØªÙˆÙ†ÛŒ", "Ú¯Ø±Ø§Ù Ø³ØªÙˆÙ†ÛŒ", "Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ", "Ú¯Ø±Ø§Ù Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ"
                    ],
                    "english": [
                        "compare", "difference", "vs", "between", "comparison", "ratio", 
                        "comparison chart", "bar chart", "compared to", "relative to", 
                        "versus", "compare two", "compare multiple", "compare all",
                        "column chart", "bar graph", "side by side"
                    ],
                    "chart_type": "bar"
                },
                "distribution_chart": {
                    "persian": [
                        "ØªÙˆØ²ÛŒØ¹", "Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒ", "Ù…Ø­Ø¯ÙˆØ¯Ù‡", "Ø¨Ø§Ø²Ù‡", "Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹", 
                        "Ú¯Ø±Ø§Ù Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒ", "ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡", "Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒ Ø¯Ø§Ø¯Ù‡", "Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¯Ø§Ø¯Ù‡",
                        "Ø¨Ø§Ø²Ù‡ Ø¯Ø§Ø¯Ù‡", "Ù†Ù…ÙˆØ¯Ø§Ø± Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù…", "Ú¯Ø±Ø§Ù Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù…", "ØªÙˆØ²ÛŒØ¹ Ø¢Ù…Ø§Ø±ÛŒ",
                        "Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒ Ø¢Ù…Ø§Ø±ÛŒ", "Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¢Ù…Ø§Ø±ÛŒ", "Ø¨Ø§Ø²Ù‡ Ø¢Ù…Ø§Ø±ÛŒ"
                    ],
                    "english": [
                        "distribution", "spread", "range", "variation", "distribution chart", 
                        "histogram", "data distribution", "data spread", "data range",
                        "statistical distribution", "statistical spread", "statistical range"
                    ],
                    "chart_type": "histogram"
                },
                "pie_chart": {
                    "persian": [
                        "Ø¯Ø±ØµØ¯", "Ù†Ø³Ø¨Øª", "Ø³Ù‡Ù…", "Ø¨Ø®Ø´", "Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ", 
                        "Ú¯Ø±Ø§Ù Ø¯Ø±ØµØ¯ÛŒ", "Ù†Ù…ÙˆØ¯Ø§Ø± Ú©ÛŒÚ©ÛŒ", "Ú¯Ø±Ø§Ù Ú©ÛŒÚ©ÛŒ", "Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§ÛŒØ±Ù‡",
                        "Ú¯Ø±Ø§Ù Ø¯Ø§ÛŒØ±Ù‡", "Ø¯Ø±ØµØ¯ Ú©Ù„", "Ù†Ø³Ø¨Øª Ú©Ù„", "Ø³Ù‡Ù… Ú©Ù„", "Ø¨Ø®Ø´ Ú©Ù„",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø±ØµØ¯ÛŒ", "Ú¯Ø±Ø§Ù Ø¯Ø±ØµØ¯ÛŒ", "Ù†Ù…ÙˆØ¯Ø§Ø± Ù†Ø³Ø¨ÛŒ", "Ú¯Ø±Ø§Ù Ù†Ø³Ø¨ÛŒ"
                    ],
                    "english": [
                        "percentage", "proportion", "share", "part", "pie chart", 
                        "percentage chart", "circle chart", "donut chart", "percentage of total",
                        "proportion of total", "share of total", "part of total"
                    ],
                    "chart_type": "pie"
                },
                "agricultural_specific": {
                    "persian": [
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ", "Ú¯Ø±Ø§Ù Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ", "Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ø²Ø±Ø¹Ù‡", "Ú¯Ø±Ø§Ù Ù…Ø²Ø±Ø¹Ù‡",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ø­ØµÙˆÙ„", "Ú¯Ø±Ø§Ù Ù…Ø­ØµÙˆÙ„", "Ù†Ù…ÙˆØ¯Ø§Ø± Ø¢ÙØ§Øª", "Ú¯Ø±Ø§Ù Ø¢ÙØ§Øª",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø¢Ø¨ÛŒØ§Ø±ÛŒ", "Ú¯Ø±Ø§Ù Ø¢Ø¨ÛŒØ§Ø±ÛŒ", "Ù†Ù…ÙˆØ¯Ø§Ø± Ú©ÙˆØ¯", "Ú¯Ø±Ø§Ù Ú©ÙˆØ¯",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø¨Ø±Ø¯Ø§Ø´Øª", "Ú¯Ø±Ø§Ù Ø¨Ø±Ø¯Ø§Ø´Øª", "Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ø­ÛŒØ·", "Ú¯Ø±Ø§Ù Ù…Ø­ÛŒØ·",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø§Ú©", "Ú¯Ø±Ø§Ù Ø®Ø§Ú©", "Ù†Ù…ÙˆØ¯Ø§Ø± Ø¢Ø¨", "Ú¯Ø±Ø§Ù Ø¢Ø¨",
                        "Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ù…Ø§", "Ú¯Ø±Ø§Ù Ø¯Ù…Ø§", "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±Ø·ÙˆØ¨Øª", "Ú¯Ø±Ø§Ù Ø±Ø·ÙˆØ¨Øª"
                    ],
                    "english": [
                        "agricultural chart", "farm chart", "crop chart", "pest chart",
                        "irrigation chart", "fertilizer chart", "harvest chart", "environment chart",
                        "soil chart", "water chart", "temperature chart", "humidity chart",
                        "agricultural graph", "farm graph", "crop graph", "pest graph"
                    ],
                    "chart_type": "line"
                }
            },
            "sample_queries": {
                "irrigation": [
                    "Ø±Ø·ÙˆØ¨Øª Ø®Ø§Ú© Ø§Ù„Ø§Ù† Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
                    "Ø¢Ø¨ Ù…ØµØ±ÙÛŒ Ø§Ù…Ø±ÙˆØ² Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
                    "Ø¨Ø§Ø²Ø¯Ù‡ÛŒ Ø¢Ø¨ Ú†Ø·ÙˆØ±Ù‡ØŸ",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯ Ø¢Ø¨ÛŒØ§Ø±ÛŒ",
                    "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ØµØ±Ù Ø¢Ø¨",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±Ø´Ø¯ Ù…ØµØ±Ù Ø¢Ø¨ Ø¯Ø± Ø³Ù‡ Ø±ÙˆØ² Ø§Ø®ÛŒØ±",
                    "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ø¨ÛŒÙ† Ù…Ù†Ø§Ø·Ù‚ Ù…Ø®ØªÙ„Ù",
                    "ØªÙˆØ²ÛŒØ¹ Ù…ØµØ±Ù Ø¢Ø¨ Ø¯Ø± Ù…Ø²Ø±Ø¹Ù‡",
                    "Ø¯Ø±ØµØ¯ Ù…ØµØ±Ù Ø¢Ø¨ Ù‡Ø± Ø¨Ø®Ø´",
                    "What is the current soil moisture?",
                    "How much water was used today?",
                    "Show me water efficiency trends",
                    "Water usage trend chart",
                    "Compare irrigation between different zones",
                    "Show water distribution histogram"
                ],
                "environment": [
                    "Ø¯Ù…Ø§ Ø§Ù„Ø§Ù† Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
                    "Ø±Ø·ÙˆØ¨Øª Ù‡ÙˆØ§ Ú†Ø·ÙˆØ±Ù‡ØŸ",
                    "ÙØ´Ø§Ø± Ù‡ÙˆØ§ Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ù…Ø§",
                    "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ù…Ø§ Ùˆ Ø±Ø·ÙˆØ¨Øª",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±Ø´Ø¯ Ø¢ÙØ§Øª Ø¯Ø± Ø³Ù‡ Ø±ÙˆØ² Ø§Ø®ÛŒØ±",
                    "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø´Ø±Ø§ÛŒØ· Ù…Ø­ÛŒØ·ÛŒ Ø¨ÛŒÙ† Ù…Ù†Ø§Ø·Ù‚",
                    "ØªÙˆØ²ÛŒØ¹ Ø¯Ù…Ø§ Ø¯Ø± Ù…Ø²Ø±Ø¹Ù‡",
                    "Ø¯Ø±ØµØ¯ Ø±Ø·ÙˆØ¨Øª Ù†Ø³Ø¨ÛŒ Ù‡ÙˆØ§",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯ ØªØºÛŒÛŒØ±Ø§Øª Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§",
                    "What is the current temperature?",
                    "Show me humidity levels",
                    "Are the environmental conditions optimal?",
                    "Temperature trend over time",
                    "Show pest growth chart for last 3 days",
                    "Environmental distribution histogram"
                ],
                "pest": [
                    "Ø¢ÙØ§Øª Ø§Ù…Ø±ÙˆØ² Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
                    "Ø®Ø·Ø± Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ú†Ø·ÙˆØ±Ù‡ØŸ",
                    "ØªØ´Ø®ÛŒØµ Ø¢ÙØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ØŸ",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹ Ø¢ÙØ§Øª",
                    "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¢ÙØ§Øª Ùˆ Ø¨ÛŒÙ…Ø§Ø±ÛŒ",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±Ø´Ø¯ Ø¢ÙØ§Øª Ø¯Ø± Ø³Ù‡ Ø±ÙˆØ² Ø§Ø®ÛŒØ±",
                    "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¢ÙØ§Øª Ø¨ÛŒÙ† Ù…Ù†Ø§Ø·Ù‚ Ù…Ø®ØªÙ„Ù",
                    "ØªÙˆØ²ÛŒØ¹ Ø¢ÙØ§Øª Ø¯Ø± Ù…Ø²Ø±Ø¹Ù‡",
                    "Ø¯Ø±ØµØ¯ Ø¢ÙØ§Øª Ù‡Ø± Ù†ÙˆØ¹",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯ Ø§ÙØ²Ø§ÛŒØ´ Ø¢ÙØ§Øª",
                    "What pests have been detected today?",
                    "Show me disease risk levels",
                    "Is pest detection active?",
                    "Pest distribution chart",
                    "Show pest growth trend for last 3 days",
                    "Compare pest levels between zones"
                ],
                "growth": [
                    "Ù‚Ø¯ Ú¯ÛŒØ§Ù‡ Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
                    "ØªØ¹Ø¯Ø§Ø¯ Ù…ÛŒÙˆÙ‡ Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
                    "Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ù…Ø­ØµÙˆÙ„ Ú†Ø·ÙˆØ±Ù‡ØŸ",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± Ø±Ø´Ø¯ Ú¯ÛŒØ§Ù‡",
                    "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…ÛŒÙˆÙ‡",
                    "What is the plant height?",
                    "How many fruits are there?",
                    "Show me yield predictions",
                    "Plant growth trend chart"
                ],
                "market": [
                    "Ù‚ÛŒÙ…Øª Ú¯ÙˆØ¬Ù‡ Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
                    "Ø¨Ù‡Ø§ÛŒ Ú©Ø§Ù‡Ùˆ Ú†Ø·ÙˆØ±Ù‡ØŸ",
                    "Ù‚ÛŒÙ…Øª ÙÙ„ÙÙ„ Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
                    "Ù†Ù…ÙˆØ¯Ø§Ø± Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§",
                    "Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‚ÛŒÙ…Øª Ù…Ø­ØµÙˆÙ„Ø§Øª",
                    "What is the tomato price?",
                    "Show me lettuce prices",
                    "How are pepper prices trending?",
                    "Price comparison chart"
                ]
            }
        }
    
    def _get_live_sensor_data(self, sensor_types: List[str] = None) -> List[Dict[str, Any]]:
        """Get live sensor data from SQLite database"""
        try:
            # Use proper database path for Liara
            if os.getenv("LIARA_APP_ID"):
                db_dir = "/var/lib/data"
                os.makedirs(db_dir, exist_ok=True)
                db_path = os.path.join(db_dir, "smart_dashboard.db")
            else:
                db_path = "smart_dashboard.db"
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            if sensor_types:
                # Get specific sensor types
                placeholders = ','.join(['?' for _ in sensor_types])
                query = f"""
                    SELECT timestamp, sensor_type, value 
                    FROM sensor_data 
                    WHERE sensor_type IN ({placeholders})
                    ORDER BY timestamp DESC 
                    LIMIT 20
                """
                cursor.execute(query, sensor_types)
            else:
                # Get all recent data
                cursor.execute("""
                    SELECT timestamp, sensor_type, value 
                    FROM sensor_data 
                    ORDER BY timestamp DESC 
                    LIMIT 20
                """)
            
            results = cursor.fetchall()
            live_data = []
            for row in results:
                live_data.append({
                    "timestamp": row[0],
                    "sensor_type": row[1],
                    "value": row[2]
                })
            
            conn.close()
            return live_data
            
        except Exception as e:
            logger.error(f"Error fetching live sensor data: {str(e)}")
            return []
    
    def process_query(self, query: str, feature_context: str = "dashboard", session_id: str = "default", intent: str = "data_query", is_comparison: bool = False) -> Dict[str, Any]:
        """Main processing function with conversation history"""
        try:
            logger.info(f" Processing query: '{query}' for session: {session_id} with intent: {intent}")
            
            # Get conversation history
            conversation_context = self._get_conversation_context(session_id)
            logger.info(f" Conversation context: {len(conversation_context)} characters")
            
            # Step 1: Detect language
            detected_lang = self.translator.detect_language(query)
            translated_query = query
            
            # Step 2: If Persian, translate to English using LLM
            if detected_lang == 'fa':
                translated_query = self.translator.translate_query_to_english(query)
                logger.info(f" LLM Translated Persian query to English: {translated_query}")
            
            # Step 2.1: Parse time context using the new time_parser
            time_context = parse_time_context(translated_query)
            logger.info(f" Time context parsed: {time_context}")
            
            # Step 2.5: Detect comparison intent if not already set
            if not is_comparison:
                is_comparison = self._detect_comparison_intent(translated_query)
                logger.info(f" Comparison detection result: {is_comparison}")
            
            # NEW: Handle alert management intent
            if intent == "alert_management":
                return self._process_alert_query(translated_query, session_id, detected_lang)
            
            # Step 3: Generate and execute SQL using LangChain SQLDatabaseChain
            sql_result = self._generate_and_execute_sql(translated_query, feature_context, conversation_context, is_comparison, time_context)
            
            # Step 3.1: Log SQL for debugging time filtering
            if sql_result.get("sql"):
                sql_query = sql_result["sql"]
                logger.info(f"ğŸ” GENERATED SQL: {sql_query}")
                
                # Check if SQL contains proper time filtering
                if "timestamp" in sql_query.lower() and ("between" in sql_query.lower() or ">=" in sql_query.lower()):
                    logger.info(f"âœ… SQL contains time filtering")
                else:
                    logger.warning(f"âš ï¸ SQL may be missing time filtering")
            else:
                logger.warning(f"âš ï¸ No SQL generated")
            
            # Step 3.5: Check if SQL execution failed
            if not sql_result.get("success", False):
                logger.error(f" SQL Execution Failed: {sql_result.get('error', 'Unknown error')}")
                error_response = {
                    "success": False,
                    "error": sql_result.get("error", "SQL execution failed"),
                    "query": query,
                    "translated_query": translated_query,
                    "sql": sql_result.get("sql", ""),
                    "validation": sql_result.get("validation", {}),
                    "language": detected_lang,
                    "feature_context": feature_context,
                    "timestamp": datetime.utcnow().isoformat()
                }
                
                # If original was Persian, translate error to Persian
                if detected_lang == 'fa':
                    error_response["error"] = f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª: {sql_result.get('error', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ')}"
                
                logger.error(f" BLOCKING RESPONSE: SQL execution failed, returning error instead of generating response")
                return error_response
            
            # Step 3.6: Check if no data was retrieved
            data_points = len(sql_result.get("raw_data", []))
            if data_points == 0:
                logger.warning(f" NO DATA RETRIEVED: SQL executed successfully but returned 0 data points")
                no_data_response = {
                    "success": True,
                    "response": "No data available for the requested time range and sensor type.",
                    "data": [],
                    "sql": sql_result.get("sql", ""),
                    "validation": sql_result.get("validation", {}),
                    "language": detected_lang,
                    "feature_context": feature_context,
                    "timestamp": datetime.utcnow().isoformat(),
                    "data_points": 0
                }
                
                # If original was Persian, translate to Persian
                if detected_lang == 'fa':
                    no_data_response["response"] = "Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ùˆ Ù†ÙˆØ¹ Ø­Ø³Ú¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª."
                
                logger.warning(f" BLOCKING RESPONSE: No data retrieved, returning no-data response instead of generating fake data")
                return no_data_response
            
            # Step 4: Format result into structured JSON (only if SQL execution succeeded)
            structured_result = self._format_structured_response(sql_result, translated_query, feature_context, intent, query, detected_lang, time_context)
            
            # Step 5: If original was Persian, translate response back to Persian using LLM
            if detected_lang == 'fa':
                structured_result = self._translate_response_to_persian(structured_result)
            
            # Step 6: Save conversation history
            self._save_conversation_history(session_id, query, structured_result.get('summary', 'No response'))
            
            logger.info(f" Query processed successfully with conversation history")
            return structured_result
            
        except Exception as e:
            logger.error(f" Dynamic SQL Service Error: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "query": query
            }
    
    def _detect_comparison_intent(self, query: str) -> bool:
        """Detect comparison intent from query - English only (after LLM translation)"""
        query_lower = query.lower()
        
        # STRICT comparison detection - only detect when user explicitly wants comparison
        
        # 1. EXPLICIT comparison keywords (high confidence)
        explicit_comparison_keywords = [
            "compare", "comparison", "vs", "versus", "against", "between", "difference", "compared to",
            "contrast", "versus", "against", "relative to", "in relation to"
        ]
        
        # 2. EXPLICIT time comparison patterns (high confidence)
        explicit_time_comparison_patterns = [
            "today vs yesterday", "today and yesterday", "this week vs last week", "this month vs last month",
            "this year vs last year", "current vs previous", "now vs then", "recent vs past",
            "last week vs this week", "last month vs this month", "yesterday vs today",
            "previous vs current", "old vs new", "past vs present", "past vs future"
        ]
        
        # 3. EXPLICIT trend analysis patterns (medium confidence - only if they mention comparison)
        trend_comparison_patterns = [
            "trend comparison", "trend difference", "trend vs", "trend versus",
            "growth trend vs", "growth trend versus", "growth trend comparison",
            "trend analysis vs", "trend analysis versus", "trend analysis comparison"
        ]
        
        # 4. EXPLICIT statistical comparison terms (medium confidence)
        statistical_comparison_patterns = [
            "correlation between", "relationship between", "connection between", "association between",
            "variation between", "difference between", "comparison between"
        ]
        
        # Check for explicit comparison keywords
        has_explicit_comparison = any(word in query_lower for word in explicit_comparison_keywords)
        
        # Check for explicit time comparison patterns
        has_explicit_time_comparison = any(pattern in query_lower for pattern in explicit_time_comparison_patterns)
        
        # Check for trend comparison patterns (only if they explicitly mention comparison)
        has_trend_comparison = any(pattern in query_lower for pattern in trend_comparison_patterns)
        
        # Check for statistical comparison patterns
        has_statistical_comparison = any(pattern in query_lower for pattern in statistical_comparison_patterns)
        
        # Additional checks for explicit comparison intent
        has_explicit_between = "between" in query_lower and any(word in query_lower for word in ["and", "vs", "versus"])
        has_explicit_vs = "vs" in query_lower or "versus" in query_lower
        has_explicit_compare = "compare" in query_lower or "comparison" in query_lower
        
        # Only return True if we have STRONG evidence of comparison intent
        is_comparison = (
            has_explicit_comparison or 
            has_explicit_time_comparison or 
            has_trend_comparison or 
            has_statistical_comparison or
            has_explicit_between or
            has_explicit_vs or
            has_explicit_compare
        )
        
        return is_comparison
    
    def _expand_time_ranges(self, detected_range: str, time_config: Dict[str, Any], is_comparison: bool = False) -> List[str]:
        """Expand user phrases into explicit time ranges for comparison using canonical format"""
        try:
            import re
            
            # Only expand for actual comparison queries
            if not is_comparison:
                return [detected_range] if detected_range else []
            
            # Handle specific time expressions with canonical format
            if "hours_ago" in detected_range:
                # Extract number of hours
                hour_match = re.search(r'(\d+)_hours_ago', detected_range)
                if hour_match:
                    hours = int(hour_match.group(1))
                    # For comparison queries, create multiple time periods
                    # For 4 hours, create: 1_hours_ago, 2_hours_ago, 3_hours_ago, 4_hours_ago
                    ranges = []
                    for i in range(1, hours + 1):
                        ranges.append(f"{i}_hours_ago")
                    return ranges
            
            # Handle time_config based expansion (when detected_range is not specific)
            elif time_config and "hours" in time_config:
                hours = time_config["hours"]
                # For comparison queries, create multiple time periods
                ranges = []
                for i in range(1, hours + 1):
                    ranges.append(f"{i}_hours_ago")
                return ranges
            
            elif "days_ago" in detected_range:
                # Extract number of days
                day_match = re.search(r'(\d+)_days_ago', detected_range)
                if day_match:
                    days = int(day_match.group(1))
                    # For comparison queries, create multiple time periods
                    ranges = []
                    for i in range(1, days + 1):
                        ranges.append(f"{i}_days_ago")
                    return ranges
            
            elif "weeks_ago" in detected_range:
                # Extract number of weeks
                week_match = re.search(r'(\d+)_weeks_ago', detected_range)
                if week_match:
                    weeks = int(week_match.group(1))
                    # For comparison queries, create multiple time periods
                    ranges = []
                    for i in range(1, weeks + 1):
                        ranges.append(f"{i}_weeks_ago")
                    return ranges
            
            # Handle granular time expressions - use canonical format consistently
            if time_config.get("granularity") == "hour":
                hours = time_config.get("hours", 1)
                # For comparison queries, create multiple time periods
                ranges = []
                for i in range(1, hours + 1):
                    ranges.append(f"{i}_hours_ago")
                return ranges
            elif time_config.get("granularity") == "day":
                days = time_config.get("days", 1)
                # For comparison queries, create multiple time periods
                ranges = []
                for i in range(1, days + 1):
                    ranges.append(f"{i}_days_ago")
                return ranges
            elif time_config.get("granularity") == "week":
                weeks = time_config.get("weeks", 1)
                # For comparison queries, create multiple time periods
                ranges = []
                for i in range(1, weeks + 1):
                    ranges.append(f"{i}_weeks_ago")
                return ranges
            
            # Default fallback
            return [detected_range]
                
        except Exception as e:
            logger.error(f"Error expanding time ranges: {e}")
            return [detected_range]

    def _compute_previous_time_range(self, time_range: str) -> str:
        """Compute the previous time range for comparison queries"""
        # Handle days_ago_X format specifically
        if time_range.startswith("days_ago_"):
            try:
                days = int(time_range.split("_")[2])
                return f"days_ago_{days + 1}"
            except (IndexError, ValueError):
                return f"previous_{time_range}"
        
        range_mapping = {
            "today": "yesterday",
            "yesterday": "day_before_yesterday", 
            "last_week": "previous_week",
            "this_week": "last_week",
            "last_month": "previous_month",
            "this_month": "last_month",
            "last_year": "previous_year",
            "this_year": "last_year",
            "last_3_days": "previous_3_days",
            "last_7_days": "previous_7_days",
            "last_30_days": "previous_30_days"
        }
        
        return range_mapping.get(time_range, f"previous_{time_range}")
    
    def _time_range_to_sql_filter(self, time_range: str, reference: str = "now") -> tuple:
        """Convert time range to SQL filter with smart time boundaries"""
        import datetime
        import re
        
        # Get current date for calculations - use UTC for consistency
        now = datetime.datetime.now(datetime.timezone.utc)
        
        # Handle canonical format: 1_hours_ago, 2_hours_ago, etc.
        if time_range.endswith("_hours_ago"):
            m = re.search(r'(\d+)_hours_ago', time_range)
            hours = int(m.group(1)) if m else 1
            # Correct semantics: X_hours_ago = window [now - X hours, now - (X-1) hours)
            # 1_hours_ago â†’ [now - 1h, now)
            # 2_hours_ago â†’ [now - 2h, now - 1h)
            start_date = now - datetime.timedelta(hours=hours)
            end_date = now - datetime.timedelta(hours=hours-1)
            label = f"{hours}_hours_ago"
            
        # Handle canonical format: 1_days_ago, 2_days_ago, etc.
        elif time_range.endswith("_days_ago"):
            m = re.search(r'(\d+)_days_ago', time_range)
            days = int(m.group(1)) if m else 1
            # Correct semantics: X_days_ago = window [target_day 00:00:00, target_day +1 00:00:00)
            # 1_days_ago â†’ [yesterday 00:00, today 00:00)
            # 2_days_ago â†’ [day_before_yesterday 00:00, yesterday 00:00)
            target = now - datetime.timedelta(days=days)
            start_date = target.replace(hour=0, minute=0, second=0, microsecond=0)
            end_date = start_date + datetime.timedelta(days=1)
            label = f"{days}_days_ago"
            
        # Handle canonical format: 1_weeks_ago, 2_weeks_ago, etc.
        elif time_range.endswith("_weeks_ago"):
            m = re.search(r'(\d+)_weeks_ago', time_range)
            weeks = int(m.group(1)) if m else 1
            # Correct semantics: X_weeks_ago = window [target_week_start, target_week_start + 7 days)
            # 1_weeks_ago â†’ [last_week_monday 00:00, this_week_monday 00:00)
            # 2_weeks_ago â†’ [week_before_last_monday 00:00, last_week_monday 00:00)
            target = now - datetime.timedelta(weeks=weeks)
            # Get start of week (Monday)
            days_since_monday = target.weekday()
            week_start = target - datetime.timedelta(days=days_since_monday)
            start_date = week_start.replace(hour=0, minute=0, second=0, microsecond=0)
            end_date = start_date + datetime.timedelta(days=7)
            label = f"{weeks}_weeks_ago"
            
        # Handle legacy formats for backward compatibility
        elif time_range == "today":
            start_date = now.replace(hour=0, minute=0, second=0, microsecond=0)
            end_date = start_date + datetime.timedelta(days=1)
            label = "today"
        elif time_range == "yesterday":
            # Yesterday = yesterday 00:00 to today 00:00
            yesterday = now - datetime.timedelta(days=1)
            start_date = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
            end_date = now.replace(hour=0, minute=0, second=0, microsecond=0)
            label = "yesterday"
        elif time_range.startswith("last_") and time_range.endswith("_days"):
            # e.g., last_3_days -> rolling N days
            try:
                n = int(time_range.split("_")[1])
            except Exception:
                n = 3
            start_date = now - datetime.timedelta(days=n)
            end_date = now
            label = time_range
        elif time_range.startswith("previous_") and time_range.endswith("_days"):
            # previous_3_days -> prior N-day window before last_N_days
            try:
                n = int(time_range.split("_")[1])
            except Exception:
                n = 3
            # Smart logic: previous_3_days = 3 days before the last 3 days
            start_date = now - datetime.timedelta(days=2*n)
            end_date = now - datetime.timedelta(days=n)
            label = f"previous_{n}_days"
        elif time_range == "last_week":
            # Last 7 days (rolling)
            start_date = now - datetime.timedelta(days=7)
            end_date = now
            label = "last_week"
        elif time_range == "previous_week":
            # Previous week = 7 days before last week
            start_date = now - datetime.timedelta(days=14)
            end_date = now - datetime.timedelta(days=7)
            label = "previous_week"
        elif time_range == "last_month":
            # Last 30 days (rolling)
            start_date = now - datetime.timedelta(days=30)
            end_date = now
            label = "last_month"
        elif time_range == "previous_month":
            # Previous month = 30 days before last month
            start_date = now - datetime.timedelta(days=60)
            end_date = now - datetime.timedelta(days=30)
            label = "previous_month"
        elif time_range == "last_3_days":
            start_date = now - datetime.timedelta(days=3)
            end_date = now
            label = "last_3_days"
        elif time_range == "previous_3_days":
            # Previous 3 days = 3 days before the last 3 days
            start_date = now - datetime.timedelta(days=6)
            end_date = now - datetime.timedelta(days=3)
            label = "previous_3_days"
        else:
            # Default to last 24 hours
            start_date = now - datetime.timedelta(days=1)
            end_date = now
            label = time_range
        
        # Format for SQLite with exclusive end semantics - ensure UTC
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=datetime.timezone.utc)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=datetime.timezone.utc)
            
        start_iso = start_date.strftime("%Y-%m-%dT%H:%M:%S")
        end_iso = end_date.strftime("%Y-%m-%dT%H:%M:%S")
        
        # Use exclusive end semantics: >= start AND < end
        sqlite_condition = f"timestamp >= '{start_iso}' AND timestamp < '{end_iso}'"
        
        return (label, start_iso, end_iso, sqlite_condition)

    def _generate_semantic_json(self, english_query: str, language: str = "en", is_comparison: bool = False) -> Dict[str, Any]:
        """Generate semantic JSON from natural language query using LLM"""
        try:
            logger.info(f" Generating semantic JSON for: '{english_query}' (comparison: {is_comparison})")
            
            # Use the passed comparison flag instead of detecting it again
            print(f" COMPARISON DETECTION: query={len(english_query)} characters, is_comparison={is_comparison}")
            
            # Get mapping metadata
            mapping_result = self._map_query_to_sensor_type(english_query, language)
            sensor_type = mapping_result["sensor_type"]
            mapping_type = mapping_result.get("mapping_type", "unknown")
            
            # Override mapping_type if comparison detected
            if is_comparison:
                mapping_type = "comparison"
                print(f" OVERRIDE: Set mapping_type to 'comparison'")
            
            # Parse time expression
            time_config = self._parse_time_expression(english_query, language)
            print(f" TIME CONFIG: {time_config}")
            
            # Build semantic JSON based on mapping and time config
            semantic_json = self._build_semantic_json_from_mapping(sensor_type, time_config, english_query, mapping_type)
            
            # Validate semantic JSON
            validation_result = self.query_builder.validate_semantic_json(semantic_json)
            if not validation_result["valid"]:
                logger.error(f" Semantic JSON validation failed: {validation_result['error']}")
                # Return fallback semantic JSON with validation feedback
                return self._get_fallback_semantic_json(sensor_type, validation_result)
            
            print(f" SEMANTIC JSON BUILT: {semantic_json}")
            logger.info(f" Generated semantic JSON: {semantic_json}")
            return semantic_json
            
        except Exception as e:
            logger.error(f" Error generating semantic JSON: {e}")
            return self._get_fallback_semantic_json("temperature")
    
    def _build_semantic_json_from_mapping(self, sensor_type: str, time_config: Dict[str, Any], english_query: str, mapping_type: str = "unknown") -> Dict[str, Any]:
        """Build semantic JSON from sensor mapping and time configuration"""
        try:
            query_lower = english_query.lower()
            
            # Use mapping_type to determine if this is a comparison query
            is_comparison = mapping_type == "comparison"
            print(f" MAPPING TYPE: {mapping_type}, IS_COMPARISON: {is_comparison}")
            
            # Detect multiple time ranges for comparison
            time_ranges = self._detect_comparison_time_ranges(english_query, query_lower)
            print(f" TIME RANGES: {time_ranges}")
            
            # Handle comparison queries first
            if is_comparison:
                if len(time_ranges) > 1:
                    # Multiple time ranges detected
                    semantic_json = {
                        "entity": sensor_type,
                        "aggregation": "average",
                        "time_range": time_ranges,
                        "comparison": True,
                        "grouping": "by_day",
                        "format": "comparison"
                    }
                else:
                    # Single time range detected - expand to comparison pair
                    detected_range = time_ranges[0] if time_ranges else time_config.get("range", "last_week")
                    
                    # EXPAND TIME RANGES: Convert user phrases to explicit ranges
                    expanded_ranges = self._expand_time_ranges(detected_range, time_config, is_comparison)
                    print(f" EXPANDED TIME RANGES: {expanded_ranges}")
                    
                    # Set appropriate grouping based on time_config granularity (prioritize over detected_range)
                    if time_config and "granularity" in time_config:
                        granularity = time_config["granularity"]
                        if granularity == "day":
                            grouping = "by_day"
                        elif granularity == "hour":
                            grouping = "by_hour"
                        elif granularity == "minute":
                            grouping = "by_minute"
                        elif granularity == "week":
                            grouping = "by_week"
                        elif granularity == "month":
                            grouping = "by_month"
                        else:
                            grouping = "by_day"  # default
                    else:
                        # Fallback to detected_range pattern
                        if "week" in detected_range:
                            grouping = "by_week"
                        elif "month" in detected_range:
                            grouping = "by_month"
                        elif "day" in detected_range or detected_range in ["today", "yesterday"]:
                            grouping = "by_day"
                        elif "hour" in detected_range:
                            grouping = "by_hour"
                        else:
                            grouping = "by_day"  # default
                    
                    semantic_json = {
                        "entity": sensor_type,
                        "aggregation": "average",
                        "time_range": expanded_ranges,
                        "comparison": True,
                        "grouping": grouping,
                        "format": "comparison"
                    }
                    print(f" EXPANDED COMPARISON: {expanded_ranges}, grouping: {grouping}")
            # Handle compound queries (sensor_type is already a list)
            elif isinstance(sensor_type, list):
                semantic_json = {
                    "entity": sensor_type,
                    "aggregation": "average",
                    "time_range": "last_24_hours",
                    "comparison": False,
                    "grouping": "none",
                    "format": "comparison"
                }
            else:
                # Single entity (non-comparison)
                semantic_json = {
                    "entity": sensor_type,
                    "aggregation": "current",
                    "time_range": "last_24_hours",
                    "comparison": False,
                    "grouping": "none",
                    "format": "value"
                }
            
            # Update based on time configuration (but preserve expanded ranges for comparison queries)
            # CRITICAL: Do not override time_range for comparison queries that already have a list
            is_comparison_with_list = (semantic_json.get("comparison", False) and 
                                    isinstance(semantic_json.get("time_range"), list))
            
            logger.debug(f"Time config override check: is_comparison_with_list={is_comparison_with_list}, "
                        f"comparison={semantic_json.get('comparison')}, "
                        f"time_range_type={type(semantic_json.get('time_range'))}, "
                        f"time_range={semantic_json.get('time_range')}")
            
            if time_config and not is_comparison_with_list:
                if "days" in time_config:
                    days = time_config["days"]
                    semantic_json["time_range"] = self._map_days_to_time_range(days)
                
            if "hours" in time_config:
                hours = time_config["hours"]
                semantic_json["time_range"] = self._map_hours_to_time_range(hours)
                
                if "minutes" in time_config:
                    minutes = time_config["minutes"]
                    semantic_json["time_range"] = self._map_minutes_to_time_range(minutes)
                
                # Set grouping based on granularity (only for non-comparison queries)
                granularity = time_config.get("granularity", "day")
                print(f" GRANULARITY: {granularity}")
                if granularity == "day":
                    semantic_json["grouping"] = "by_day"
                elif granularity == "hour":
                    semantic_json["grouping"] = "by_hour"
                elif granularity == "minute":
                    semantic_json["grouping"] = "by_minute"
                elif granularity == "week":
                    semantic_json["grouping"] = "by_week"
                print(f" GROUPING SET TO: {semantic_json['grouping']}")
            elif is_comparison_with_list:
                print(f" PRESERVING COMPARISON TIME_RANGE: {semantic_json.get('time_range')}")
            
            # Update aggregation based on query patterns
            if any(word in query_lower for word in ["average", "mean", "avg", "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†", "Ù…ØªÙˆØ³Ø·"]):
                semantic_json["aggregation"] = "average"
            elif any(word in query_lower for word in ["current", "latest", "now", "today", "Ø§Ù„Ø§Ù†", "ÙØ¹Ù„Ø§", "Ø§Ù…Ø±ÙˆØ²"]):
                semantic_json["aggregation"] = "current"
            elif any(word in query_lower for word in ["trend", "changes", "over time", "Ø±ÙˆÙ†Ø¯", "ØªØºÛŒÛŒØ±Ø§Øª"]):
                semantic_json["format"] = "trend"
                semantic_json["aggregation"] = "average"
            elif any(word in query_lower for word in ["compare", "comparison", "Ù…Ù‚Ø§ÛŒØ³Ù‡", "ØªÙØ§ÙˆØª"]):
                semantic_json["format"] = "comparison"
                semantic_json["aggregation"] = "average"
            
            return semantic_json
            
        except Exception as e:
            logger.error(f" Error building semantic JSON: {e}")
            return self._get_fallback_semantic_json(sensor_type)
    
    def _detect_comparison_time_ranges(self, english_query: str, query_lower: str) -> List[str]:
        """ROBUST COMPARISON TIME RANGE DETECTOR: Handle ALL comparison scenarios intelligently"""
        time_ranges = []
        
        # Only detect comparison time ranges if the query explicitly mentions comparison
        has_explicit_comparison = any(word in query_lower for word in [
            "compare", "comparison", "vs", "versus", "against", "between", "difference", "compared to",
            "contrast", "relative to", "in relation to"
        ])
        
        if not has_explicit_comparison:
            return time_ranges  # Return empty list for non-comparison queries
        
        # ROBUST COMPARISON PATTERNS - Handle ALL comparison scenarios
        import re
        
        # 1. "Compare X vs Y" patterns
        compare_vs_pattern = re.search(r'compare\s+(.+?)\s+(?:vs|versus|against)\s+(.+?)', query_lower)
        if compare_vs_pattern:
            period1 = compare_vs_pattern.group(1).strip()
            period2 = compare_vs_pattern.group(2).strip()
            time_ranges = [self._map_time_period_to_range(period1), self._map_time_period_to_range(period2)]
            return time_ranges
        
        # 2. "Between X and Y" patterns
        between_pattern = re.search(r'between\s+(.+?)\s+and\s+(.+?)', query_lower)
        if between_pattern:
            period1 = between_pattern.group(1).strip()
            period2 = between_pattern.group(2).strip()
            time_ranges = [self._map_time_period_to_range(period1), self._map_time_period_to_range(period2)]
            return time_ranges
        
        # 3. "X vs Y" patterns
        vs_pattern = re.search(r'(.+?)\s+(?:vs|versus)\s+(.+?)', query_lower)
        if vs_pattern:
            period1 = vs_pattern.group(1).strip()
            period2 = vs_pattern.group(2).strip()
            time_ranges = [self._map_time_period_to_range(period1), self._map_time_period_to_range(period2)]
            return time_ranges
        
        # 4. "X hours ago" patterns (only for comparison)
        hours_ago_match = re.search(r'(\d+)\s*hours?\s*ago', query_lower)
        if hours_ago_match:
            hours = int(hours_ago_match.group(1))
            time_ranges = [f"hours_ago_{hours}", f"hours_ago_{hours + 1}"]
            return time_ranges

        # 5. "last/past N days" patterns (only for comparison)
        last_days = re.search(r'(?:last|past)\s*(\d+)\s*days?', query_lower)
        if last_days:
            n = int(last_days.group(1))
            time_ranges = [f"last_{n}_days", f"previous_{n}_days"]
            return time_ranges
        
        # 6. "last/past N hours" patterns (only for comparison)
        last_hours = re.search(r'(?:last|past)\s*(\d+)\s*hours?', query_lower)
        if last_hours:
            n = int(last_hours.group(1))
            time_ranges = [f"last_{n}_hours", f"previous_{n}_hours"]
            return time_ranges

        return time_ranges
    
    def _map_time_period_to_range(self, period: str) -> str:
        """Map a time period description to a time range"""
        period_lower = period.lower().strip()
        
        # Map common time period descriptions
        period_mapping = {
            "today": "today",
            "yesterday": "yesterday", 
            "last week": "last_week",
            "this week": "this_week",
            "last month": "last_month",
            "this month": "this_month",
            "last year": "last_year",
            "this year": "this_year",
            "last hour": "last_hour",
            "this hour": "this_hour",
            "last day": "last_day",
            "this day": "this_day"
        }
        
        # Check for exact matches first
        if period_lower in period_mapping:
            return period_mapping[period_lower]
        
        # Check for "last N days/hours" patterns
        import re
        last_days_match = re.search(r'last\s+(\d+)\s+days?', period_lower)
        if last_days_match:
            n = int(last_days_match.group(1))
            return f"last_{n}_days"
        
        last_hours_match = re.search(r'last\s+(\d+)\s+hours?', period_lower)
        if last_hours_match:
            n = int(last_hours_match.group(1))
            return f"last_{n}_hours"
        
        # Default fallback
        return period_lower

    def _process_comparison_data(self, raw_data: List[Dict[str, Any]], time_ranges: List[str]) -> Dict[str, Any]:
        """Process comparison data to compute deltas and percentage changes"""
        try:
            if not raw_data or len(time_ranges) < 2:
                return None
            
            # Group data by time period and sensor type
            grouped_data = {}
            for item in raw_data:
                time_period = item.get("time_period", "")
                sensor_type = item.get("sensor_type", "unknown")
                avg_value = item.get("avg_value", 0)
                
                if time_period not in grouped_data:
                    grouped_data[time_period] = {}
                grouped_data[time_period][sensor_type] = avg_value
            
            # Map time periods to time ranges
            time_period_mapping = {}
            for i, time_range in enumerate(time_ranges):
                # Find the closest time period for this range
                for time_period in grouped_data.keys():
                    if self._time_period_matches_range(time_period, time_range):
                        time_period_mapping[time_range] = time_period
                        break
            
            # Compute comparison metrics
            comparison_results = {}
            for sensor_type in set([item.get("sensor_type") for item in raw_data]):
                sensor_comparison = {}
                
                # Get values for each time range
                for time_range in time_ranges:
                    time_period = time_period_mapping.get(time_range)
                    if time_period and time_period in grouped_data:
                        value = grouped_data[time_period].get(sensor_type, 0)
                        sensor_comparison[time_range] = value
                
                # Compute deltas if we have at least 2 values
                if len(sensor_comparison) >= 2:
                    values = list(sensor_comparison.values())
                    if len(values) == 2:
                        # Two-period comparison
                        old_value, new_value = values
                        delta = new_value - old_value
                        percent_change = (delta / old_value * 100) if old_value != 0 else 0
                        
                        sensor_comparison["delta"] = delta
                        sensor_comparison["percent_change"] = percent_change
                        sensor_comparison["trend"] = "increasing" if delta > 0 else "decreasing" if delta < 0 else "stable"
                
                comparison_results[sensor_type] = sensor_comparison
            
            return {
                "time_ranges": time_ranges,
                "time_period_mapping": time_period_mapping,
                "sensor_comparisons": comparison_results,
                "overall_trend": self._compute_overall_trend(comparison_results)
            }
            
        except Exception as e:
            logger.error(f" Error processing comparison data: {e}")
            return None
    
    def _time_period_matches_range(self, time_period: str, time_range: str) -> bool:
        """Check if a time period matches a specific time range"""
        try:
            from datetime import datetime, timedelta
            
            # Parse time period (format: YYYY-MM-DD or YYYY-WW)
            if len(time_period) == 10:  # YYYY-MM-DD format
                period_date = datetime.strptime(time_period, "%Y-%m-%d").date()
                today = datetime.now().date()
                
                if time_range == "today":
                    return period_date == today
                elif time_range == "yesterday":
                    return period_date == today - timedelta(days=1)
                elif time_range == "this_week":
                    week_start = today - timedelta(days=today.weekday())
                    return week_start <= period_date <= today
                elif time_range == "last_week":
                    week_start = today - timedelta(days=today.weekday())
                    last_week_start = week_start - timedelta(days=7)
                    last_week_end = week_start - timedelta(days=1)
                    return last_week_start <= period_date <= last_week_end
            
            return False
            
        except Exception as e:
            logger.error(f" Error matching time period: {e}")
            return False
    
    def _compute_overall_trend(self, comparison_results: Dict[str, Any]) -> str:
        """Compute overall trend across all sensors"""
        try:
            trends = []
            for sensor_type, comparison in comparison_results.items():
                if "trend" in comparison:
                    trends.append(comparison["trend"])
            
            if not trends:
                return "unknown"
            
            # Count trends
            increasing = trends.count("increasing")
            decreasing = trends.count("decreasing")
            stable = trends.count("stable")
            
            if increasing > decreasing and increasing > stable:
                return "increasing"
            elif decreasing > increasing and decreasing > stable:
                return "decreasing"
            else:
                return "stable"
                
        except Exception as e:
            logger.error(f" Error computing overall trend: {e}")
            return "unknown"

    def _get_fallback_semantic_json(self, sensor_type: str, validation_result: Dict[str, Any] = None) -> Dict[str, Any]:
        """Get fallback semantic JSON with validation feedback"""
        # Check if entity is missing and provide feedback
        if validation_result and "missing_entity" in validation_result.get("error", ""):
            logger.warning(f"  Entity missing in semantic JSON, defaulting to temperature. Original sensor_type: {sensor_type}")
            sensor_type = "temperature"  # Default fallback
        
        if isinstance(sensor_type, list):
            entities = sensor_type
            return {
                "entity": entities,
                "aggregation": "current",
                "time_range": "last_24_hours",
                "comparison": False,
                "grouping": "none",
                "format": "value",
                "fallback_reason": validation_result.get("error", "unknown") if validation_result else "default"
            }
        else:
            return {
                "entity": sensor_type,
                "aggregation": "current",
                "time_range": "last_24_hours",
                "comparison": False,
                "grouping": "none",
                "format": "value",
                "fallback_reason": validation_result.get("error", "unknown") if validation_result else "default"
            }
    
    def _generate_and_execute_sql(self, english_query: str, feature_context: str, conversation_context: str = "", is_comparison: bool = False, time_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Generate and execute SQL query using semantic JSON flow with fallback mechanism"""
        try:
            logger.info(f" Generating SQL using semantic JSON flow for: '{english_query}' (comparison: {is_comparison})")
            
            # Step 1: Generate semantic JSON from natural language query
            semantic_json = self._generate_semantic_json(english_query, language="en", is_comparison=is_comparison)
            print(f" SEMANTIC JSON GENERATED: {semantic_json}")
            logger.info(f" Generated semantic JSON: {semantic_json}")
            
            # Step 2: Convert semantic JSON to SQL using QueryBuilder
            # Include time_context in semantic_json metadata
            if time_context:
                semantic_json["time_context"] = time_context
            sql_query = self.query_builder.build_sql_from_semantic_json(semantic_json)
            print(f" SQL FROM SEMANTIC JSON: {sql_query}")
            logger.info(f" Generated SQL from semantic JSON: {sql_query}")
            logger.debug(f"SQL generation details: semantic_json={semantic_json} -> sql={sql_query}")
            
            # Step 3: Execute SQL with validation
            execution_result = self._execute_direct_sql(sql_query)
            
            # Step 4: Check if execution was successful
            if not execution_result["success"]:
                logger.error(f" SQL Execution Failed: {execution_result['error']}")
                logger.error(f" ERROR: SQL execution failed for query: {sql_query}")
                logger.error(f" ERROR: Execution result: {execution_result}")
                return {
                    "sql": sql_query,
                    "raw_data": [],
                    "success": False,
                    "error": execution_result["error"],
                    "validation": execution_result.get("validation", {}),
                    "semantic_json": semantic_json,
                    "refined_by_llm": False
                }
            
            # Step 5: Check if validation failed
            validation_result = execution_result.get("validation", {})
            if not validation_result.get("valid", True):
                logger.error(f" SQL Validation Failed: {validation_result.get('message', 'Unknown validation error')}")
                logger.error(f" ERROR: SQL validation failed for query: {sql_query}")
                logger.error(f" ERROR: Validation result: {validation_result}")
                return {
                    "sql": sql_query,
                    "raw_data": [],
                    "success": False,
                    "error": validation_result.get("message", "Query validation failed"),
                    "validation": validation_result,
                    "semantic_json": semantic_json,
                    "refined_by_llm": False
                }
            
            # Step 6: Check if we got data
            data_points = len(execution_result["data"])
            logger.info(f" Data Points: {data_points}")
            
            # Step 7: If no data, try fallback with relaxed semantic JSON
            if data_points == 0:
                logger.warning(f" No data returned, trying fallback with relaxed semantic JSON")
                fallback_result = self._try_fallback_semantic_json(english_query, semantic_json)
                if fallback_result["success"] and len(fallback_result["raw_data"]) > 0:
                    logger.info(f" Fallback successful: {len(fallback_result['raw_data'])} data points")
                    return fallback_result
                else:
                    logger.warning(f" Fallback also returned no data")
                    # CRITICAL: Return failure when no data is found
                    return {
                        "sql": sql_query,
                        "raw_data": [],
                        "success": False,
                        "error": "No data available for the requested time range and sensor type",
                        "validation": execution_result.get("validation", {}),
                        "semantic_json": semantic_json,
                        "refined_by_llm": False
                    }
            
            # Step 8: Return successful result only if we have data
            return {
                "sql": sql_query,
                "raw_data": execution_result["data"],
                "success": True,
                "validation": execution_result.get("validation", {}),
                "semantic_json": semantic_json,
                "refined_by_llm": False
            }
                
        except Exception as e:
            logger.error(f" SQL Generation Error: {str(e)}")
            return {
                "sql": "SELECT * FROM sensor_data LIMIT 5",
                "raw_data": [],
                "success": False,
                "error": str(e),
                "semantic_json": {"entity": "temperature", "aggregation": "current", "time_range": "last_24_hours", "grouping": "none", "format": "value"},
                "refined_by_llm": False
            }
    
    def _try_fallback_semantic_json(self, english_query: str, original_semantic_json: Dict[str, Any]) -> Dict[str, Any]:
        """Try fallback with relaxed semantic JSON (entity fallback, no grouping)"""
        try:
            logger.info(f" Trying fallback with relaxed semantic JSON")
            logger.warning(f" FALLBACK TRIGGERED: No data returned from original query, attempting relaxed semantic JSON fallback")
            
            # Create relaxed semantic JSON
            fallback_semantic_json = original_semantic_json.copy()
            
            # Remove grouping for fallback
            fallback_semantic_json["grouping"] = "none"
            
            # If compound query, try with first entity only
            if isinstance(fallback_semantic_json["entity"], list) and len(fallback_semantic_json["entity"]) > 1:
                fallback_semantic_json["entity"] = fallback_semantic_json["entity"][0]
                logger.info(f" Fallback: Using single entity instead of compound")
            
            # Try with current aggregation instead of average
            if fallback_semantic_json["aggregation"] == "average":
                fallback_semantic_json["aggregation"] = "current"
                logger.info(f" Fallback: Using current instead of average")
            
            # Generate SQL from fallback semantic JSON
            fallback_sql = self.query_builder.build_sql_from_semantic_json(fallback_semantic_json)
            logger.info(f" Fallback SQL: {fallback_sql}")
            
            # Execute fallback SQL
            execution_result = self._execute_direct_sql(fallback_sql)
            
            if execution_result["success"] and len(execution_result["data"]) > 0:
                return {
                    "sql": fallback_sql,
                    "raw_data": execution_result["data"],
                    "success": True,
                    "validation": execution_result.get("validation", {}),
                    "semantic_json": fallback_semantic_json,
                    "refined_by_llm": True,
                    "fallback_used": True
                }
            else:
                # Second-level fallback: Use direct LLM SQL agent
                logger.warning(f"  First fallback failed, trying direct LLM SQL agent")
                return self._try_direct_llm_sql_agent(english_query, fallback_semantic_json)
                
        except Exception as e:
            logger.error(f" Fallback semantic JSON error: {str(e)}")
            return {
                "sql": "SELECT * FROM sensor_data LIMIT 5",
                "raw_data": [],
                "success": False,
                "error": str(e),
                "semantic_json": original_semantic_json,
                "refined_by_llm": True,
                "fallback_used": True
            }
    
    def _try_direct_llm_sql_agent(self, english_query: str, fallback_semantic_json: Dict[str, Any]) -> Dict[str, Any]:
        """Second-level fallback: Use direct LLM SQL agent when semantic JSON fails"""
        try:
            logger.info(f" Second-level fallback: Using direct LLM SQL agent for: '{english_query}'")
            logger.warning(f" FALLBACK TRIGGERED: First fallback failed, attempting direct LLM SQL agent fallback")
            
            # Check if SQL agent is available
            if not hasattr(self, 'sql_agent') or self.sql_agent is None:
                logger.warning(f"  SQL agent not available, using basic fallback")
                logger.warning(f" FALLBACK TRIGGERED: SQL agent unavailable, using basic fallback query")
                return self._get_basic_fallback_result(english_query, fallback_semantic_json)
            
            # Use direct LLM SQL agent
            try:
                agent_result = self.sql_agent.run(english_query)
                logger.info(f" Direct LLM SQL agent result: {str(agent_result)[:200]}...")
                
                # Extract SQL from agent result if possible
                if hasattr(agent_result, 'sql') and agent_result.sql:
                    sql_query = agent_result.sql
                elif isinstance(agent_result, str) and 'SELECT' in agent_result.upper():
                    # Try to extract SQL from string result
                    import re
                    sql_match = re.search(r'(SELECT.*?)(?:\n|$)', agent_result, re.IGNORECASE | re.DOTALL)
                    if sql_match:
                        sql_query = sql_match.group(1).strip()
                    else:
                        sql_query = "SELECT * FROM sensor_data LIMIT 5"
                else:
                    sql_query = "SELECT * FROM sensor_data LIMIT 5"
                
                # Execute the SQL
                execution_result = self._execute_direct_sql(sql_query)
                
                if execution_result["success"] and len(execution_result["data"]) > 0:
                    return {
                        "sql": sql_query,
                        "raw_data": execution_result["data"],
                        "success": True,
                        "validation": execution_result.get("validation", {}),
                        "semantic_json": fallback_semantic_json,
                        "refined_by_llm": True,
                        "fallback_used": True,
                        "direct_llm_agent_used": True
                    }
                else:
                    return self._get_basic_fallback_result(english_query, fallback_semantic_json)
                    
            except Exception as agent_error:
                logger.error(f" Direct LLM SQL agent error: {str(agent_error)}")
                return self._get_basic_fallback_result(english_query, fallback_semantic_json)
                
        except Exception as e:
            logger.error(f" Direct LLM SQL agent fallback error: {str(e)}")
            return self._get_basic_fallback_result(english_query, fallback_semantic_json)
    
    def _get_basic_fallback_result(self, english_query: str, fallback_semantic_json: Dict[str, Any]) -> Dict[str, Any]:
        """Basic fallback when all else fails"""
        basic_sql = "SELECT * FROM sensor_data ORDER BY timestamp DESC LIMIT 10"
        execution_result = self._execute_direct_sql(basic_sql)
        
        return {
            "sql": basic_sql,
            "raw_data": execution_result.get("data", []),
            "success": execution_result["success"],
            "error": "All fallbacks exhausted, using basic query",
            "validation": execution_result.get("validation", {}),
            "semantic_json": fallback_semantic_json,
            "refined_by_llm": True,
            "fallback_used": True,
            "basic_fallback_used": True
        }
    
    def _detect_chart_request(self, query: str, language: str = "en") -> Dict[str, Any]:
        """Detect if user wants a chart and what type"""
        query_lower = query.lower()
        chart_patterns = self.ontology.get("chart_patterns", {})
        
        for pattern_name, pattern_data in chart_patterns.items():
            # Check Persian terms
            if language == "fa":
                persian_terms = pattern_data.get("persian", [])
                for term in persian_terms:
                    if term in query_lower:
                        return {
                            "wants_chart": True,
                            "chart_type": pattern_data["chart_type"],
                            "pattern": pattern_name,
                            "matched_term": term
                        }
            
            # Check English terms
            english_terms = pattern_data.get("english", [])
            for term in english_terms:
                if term in query_lower:
                    return {
                        "wants_chart": True,
                        "chart_type": pattern_data["chart_type"],
                        "pattern": pattern_name,
                        "matched_term": term
                    }
        
        return {"wants_chart": False}

    def _process_aggregated_chart_data(self, aggregated_data: List[Dict[str, Any]], chart_type: str) -> Dict[str, Any]:
        """Process aggregated data for chart visualization"""
        try:
            # Check if this is time breakdown data (has 'time_period' field)
            is_time_breakdown = "time_period" in aggregated_data[0] if aggregated_data else False
            is_daily_breakdown = "day" in aggregated_data[0] if aggregated_data else False
            
            if chart_type == "line":
                # Process aggregated data for line charts
                labels = []
                values = []
                min_values = []
                max_values = []
                
                for item in aggregated_data:
                    if is_daily_breakdown:
                        # Daily breakdown data
                        day = item.get("day", "Unknown")
                        avg_value = float(item.get("avg_value", 0))
                        min_value = float(item.get("min_value", 0))
                        max_value = float(item.get("max_value", 0))
                        
                        # Format day for display
                        try:
                            from datetime import datetime
                            dt = datetime.strptime(day, "%Y-%m-%d")
                            labels.append(dt.strftime("%d %b"))  # "22 Sep", "23 Sep", etc.
                        except:
                            labels.append(day)
                        
                        values.append(avg_value)
                        min_values.append(min_value)
                        max_values.append(max_value)
                    elif is_time_breakdown:
                        # Time period breakdown data (hourly, weekly, monthly)
                        time_period = item.get("time_period", "Unknown")
                        avg_value = float(item.get("avg_value", 0))
                        min_value = float(item.get("min_value", 0))
                        max_value = float(item.get("max_value", 0))
                        
                        # Format time period for display based on period type
                        try:
                            from datetime import datetime
                            if ":" in time_period:  # Hourly data (YYYY-MM-DD HH:00)
                                dt = datetime.strptime(time_period, "%Y-%m-%d %H:%M")
                                labels.append(dt.strftime("%d %b %H:%M"))  # "22 Sep 14:00"
                            elif len(time_period) == 7:  # Weekly data (YYYY-WW)
                                year, week = time_period.split("-")
                                labels.append(f"Week {week}, {year}")  # "Week 39, 2025"
                            elif len(time_period) == 7 and time_period.count("-") == 1:  # Monthly data (YYYY-MM)
                                dt = datetime.strptime(time_period, "%Y-%m")
                                labels.append(dt.strftime("%b %Y"))  # "Sep 2025"
                            else:  # Daily data (YYYY-MM-DD)
                                dt = datetime.strptime(time_period, "%Y-%m-%d")
                                labels.append(dt.strftime("%d %b"))  # "22 Sep"
                        except:
                            labels.append(time_period)
                        
                        values.append(avg_value)
                        min_values.append(min_value)
                        max_values.append(max_value)
                    else:
                        # Regular time period data (legacy)
                        time_period = item.get("time_period", "Unknown")
                        avg_value = float(item.get("avg_value", 0))
                        
                        # Format time period for display
                        try:
                            from datetime import datetime
                            dt = datetime.strptime(time_period, "%Y-%m-%d")
                            labels.append(dt.strftime("%d %b"))  # "22 Sep", "23 Sep", etc.
                        except:
                            labels.append(time_period)
                        
                        values.append(avg_value)
                
                if (is_daily_breakdown or is_time_breakdown) and min_values and max_values:
                    # Enhanced chart data for daily breakdown with min/max ranges
                    chart_data = {
                        "labels": labels,
                        "datasets": [
                            {
                                "label": "Average Value",
                                "data": values,
                                "borderColor": "rgb(59, 130, 246)",
                                "backgroundColor": "rgba(59, 130, 246, 0.1)",
                                "tension": 0.4,
                                "fill": True,
                                "pointRadius": 6,
                                "pointHoverRadius": 8,
                                "pointBackgroundColor": "rgb(59, 130, 246)",
                                "pointBorderColor": "#ffffff",
                                "pointBorderWidth": 2
                            },
                            {
                                "label": "Min Value",
                                "data": min_values,
                                "borderColor": "rgb(239, 68, 68)",
                                "backgroundColor": "rgba(239, 68, 68, 0.1)",
                                "tension": 0.4,
                                "fill": False,
                                "pointRadius": 4,
                                "pointHoverRadius": 6,
                                "pointBackgroundColor": "rgb(239, 68, 68)",
                                "pointBorderColor": "#ffffff",
                                "pointBorderWidth": 2,
                                "borderDash": [5, 5]
                            },
                            {
                                "label": "Max Value",
                                "data": max_values,
                                "borderColor": "rgb(16, 185, 129)",
                                "backgroundColor": "rgba(16, 185, 129, 0.1)",
                                "tension": 0.4,
                                "fill": False,
                                "pointRadius": 4,
                                "pointHoverRadius": 6,
                                "pointBackgroundColor": "rgb(16, 185, 129)",
                                "pointBorderColor": "#ffffff",
                                "pointBorderWidth": 2,
                                "borderDash": [5, 5]
                            }
                        ]
                    }
                else:
                    # Regular chart data
                    chart_data = {
                        "labels": labels,
                        "datasets": [{
                            "label": "Average Value",
                            "data": values,
                            "borderColor": "rgb(59, 130, 246)",
                            "backgroundColor": "rgba(59, 130, 246, 0.1)",
                            "tension": 0.4,
                            "fill": True,
                            "pointRadius": 6,
                            "pointHoverRadius": 8,
                            "pointBackgroundColor": "rgb(59, 130, 246)",
                            "pointBorderColor": "#ffffff",
                            "pointBorderWidth": 2
                        }]
                    }
                
            elif chart_type == "bar":
                # Process aggregated data for bar charts
                labels = []
                avg_values = []
                min_values = []
                max_values = []
                
                for item in aggregated_data:
                    time_period = item.get("time_period", "Unknown")
                    avg_value = float(item.get("avg_value", 0))
                    min_value = float(item.get("min_value", 0))
                    max_value = float(item.get("max_value", 0))
                    
                    # Format time period for display
                    try:
                        from datetime import datetime
                        dt = datetime.strptime(time_period, "%Y-%m-%d")
                        labels.append(dt.strftime("%d %b"))
                    except:
                        labels.append(time_period)
                    
                    avg_values.append(avg_value)
                    min_values.append(min_value)
                    max_values.append(max_value)
                
                chart_data = {
                    "labels": labels,
                    "datasets": [
                        {
                            "label": " Average",
                            "data": avg_values,
                            "backgroundColor": "rgba(16, 185, 129, 0.8)",
                            "borderColor": "rgba(16, 185, 129, 1)",
                            "borderWidth": 2
                        },
                        {
                            "label": " Min",
                            "data": min_values,
                            "backgroundColor": "rgba(245, 101, 101, 0.8)",
                            "borderColor": "rgba(245, 101, 101, 1)",
                            "borderWidth": 2
                        },
                        {
                            "label": " Max",
                            "data": max_values,
                            "backgroundColor": "rgba(251, 191, 36, 0.8)",
                            "borderColor": "rgba(251, 191, 36, 1)",
                            "borderWidth": 2
                        }
                    ]
                }
                
            else:
                # Default fallback for other chart types
                labels = []
                values = []
                
                for item in aggregated_data:
                    time_period = item.get("time_period", "Unknown")
                    avg_value = float(item.get("avg_value", 0))
                    
                    try:
                        from datetime import datetime
                        dt = datetime.strptime(time_period, "%Y-%m-%d")
                        labels.append(dt.strftime("%d %b"))
                    except:
                        labels.append(time_period)
                    
                    values.append(avg_value)
                
                chart_data = {
                    "labels": labels,
                    "datasets": [{
                        "label": " Average Value",
                        "data": values,
                        "backgroundColor": "rgba(59, 130, 246, 0.8)",
                        "borderColor": "rgba(59, 130, 246, 1)",
                        "borderWidth": 2
                    }]
                }
            
            logger.info(f" Processed aggregated chart data: {len(labels)} labels, {len(values) if 'values' in locals() else 'N/A'} values")
            return chart_data
            
        except Exception as e:
            logger.error(f" Error processing aggregated chart data: {str(e)}")
            return None

    def _process_chart_data(self, raw_data: List[Dict[str, Any]], chart_type: str) -> Dict[str, Any]:
        """Process raw data for chart visualization with enhanced formatting (handles both aggregated and raw data)"""
        if not raw_data:
            return None
        
        # Check if data is in aggregated format (time-aware queries)
        if raw_data and "avg_value" in raw_data[0]:
            logger.info(f" Processing aggregated chart data with {len(raw_data)} time periods")
            return self._process_aggregated_chart_data(raw_data, chart_type)
        
        # Handle raw data format (legacy)
        logger.info(f" Processing raw chart data with {len(raw_data)} data points")
        if chart_type == "line":
            # Enhanced time series data for line charts
            labels = []
            values = []
            timestamps = []
            
            # First pass: collect all timestamps and values
            for item in raw_data:
                timestamp = item.get("timestamp", "")
                if timestamp:
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        timestamps.append(dt)
                        values.append(float(item.get("value", 0)))
                    except:
                        timestamps.append(None)
                        values.append(float(item.get("value", 0)))
                else:
                    timestamps.append(None)
                    values.append(float(item.get("value", 0)))
            
            # Determine time range and format accordingly
            if timestamps and any(timestamps):
                valid_timestamps = [t for t in timestamps if t is not None]
                if valid_timestamps:
                    min_time = min(valid_timestamps)
                    max_time = max(valid_timestamps)
                    time_diff = max_time - min_time
                    
                    # Format labels based on time range
                    for i, timestamp in enumerate(timestamps):
                        if timestamp is not None:
                            if time_diff.days >= 1:
                                # Multi-day range: show "23 Sep", "24 Sep", etc.
                                labels.append(timestamp.strftime("%d %b"))
                            elif time_diff.total_seconds() >= 3600:  # 1 hour or more
                                # Multi-hour range: show "23 Sep 14:00", "23 Sep 15:00", etc.
                                labels.append(timestamp.strftime("%d %b %H:%M"))
                            elif time_diff.total_seconds() >= 300:  # 5 minutes or more
                                # Multi-minute range: show "14:00", "14:15", etc.
                                labels.append(timestamp.strftime("%H:%M"))
                            else:
                                # Very short range: show "14:00:30", "14:00:45", etc.
                                labels.append(timestamp.strftime("%H:%M:%S"))
                        else:
                            labels.append(f"Point {i+1}")
                    
                    # Remove duplicate labels while preserving order
                    seen = set()
                    unique_labels = []
                    for label in labels:
                        if label not in seen:
                            unique_labels.append(label)
                            seen.add(label)
                        else:
                            # Add a small suffix to make it unique
                            counter = 1
                            while f"{label}_{counter}" in seen:
                                counter += 1
                            unique_labels.append(f"{label}_{counter}")
                            seen.add(f"{label}_{counter}")
                    
                    labels = unique_labels
                else:
                    # No valid timestamps, use generic labels
                    labels = [f"Point {i+1}" for i in range(len(values))]
            else:
                # No timestamps at all, use generic labels
                labels = [f"Point {i+1}" for i in range(len(values))]
            
            chart_data = {
                "labels": labels,
                "datasets": [{
                    "label": " Sensor Reading",
                    "data": values,
                    "borderColor": "rgb(59, 130, 246)",
                    "backgroundColor": "rgba(59, 130, 246, 0.1)",
                    "tension": 0.4,
                    "fill": True,
                    "pointRadius": 4,
                    "pointHoverRadius": 6,
                    "pointBackgroundColor": "rgb(59, 130, 246)",
                    "pointBorderColor": "#ffffff",
                    "pointBorderWidth": 2
                }]
            }
        
        elif chart_type == "bar":
            # Enhanced comparison data for bar charts
            sensor_types = []
            avg_values = []
            
            # Group data by sensor type and calculate averages
            sensor_data = {}
            for item in raw_data:
                sensor_type = item.get("sensor_type", "Unknown")
                value = float(item.get("value", 0))
                
                if sensor_type not in sensor_data:
                    sensor_data[sensor_type] = []
                sensor_data[sensor_type].append(value)
            
            for sensor_type, values in sensor_data.items():
                sensor_types.append(sensor_type.replace('_', ' ').title())
                avg_values.append(sum(values) / len(values))
            
            chart_data = {
                "labels": sensor_types,
                "datasets": [{
                    "label": " Average Value",
                    "data": avg_values,
                    "backgroundColor": "rgba(16, 185, 129, 0.8)",
                    "borderColor": "rgba(16, 185, 129, 1)",
                    "borderWidth": 2,
                    "borderRadius": 4,
                    "borderSkipped": False
                }]
            }
        
        elif chart_type == "histogram":
            # Enhanced distribution data for histograms
            values = [float(item.get("value", 0)) for item in raw_data]
            
            if not values:
                return None
                
            min_val, max_val = min(values), max(values)
            range_val = max_val - min_val
            
            # Dynamic bin creation based on data range
            num_bins = min(6, max(3, len(values) // 5))
            bin_size = range_val / num_bins if range_val > 0 else 1
            
            bins = [0] * num_bins
            bin_labels = []
            
            for i in range(num_bins):
                bin_start = min_val + (i * bin_size)
                bin_end = min_val + ((i + 1) * bin_size)
                bin_labels.append(f"{bin_start:.1f}-{bin_end:.1f}")
            
            for value in values:
                bin_index = min(int((value - min_val) / bin_size), num_bins - 1)
                bins[bin_index] += 1
            
            chart_data = {
                "labels": bin_labels,
                "datasets": [{
                    "label": " Frequency Distribution",
                    "data": bins,
                    "backgroundColor": "rgba(245, 101, 101, 0.8)",
                    "borderColor": "rgba(245, 101, 101, 1)",
                    "borderWidth": 2,
                    "borderRadius": 4,
                    "borderSkipped": False
                }]
            }
        
        elif chart_type == "pie":
            # Enhanced categorical data for pie charts
            sensor_counts = {}
            
            for item in raw_data:
                sensor_type = item.get("sensor_type", "Unknown")
                sensor_counts[sensor_type] = sensor_counts.get(sensor_type, 0) + 1
            
            labels = []
            data = []
            
            for sensor_type, count in sensor_counts.items():
                labels.append(sensor_type.replace('_', ' ').title())
                data.append(count)
            
            # Professional color palette
            colors = [
                "rgba(59, 130, 246, 0.8)",   # Blue
                "rgba(16, 185, 129, 0.8)",   # Green
                "rgba(245, 101, 101, 0.8)",  # Red
                "rgba(251, 191, 36, 0.8)",   # Yellow
                "rgba(139, 92, 246, 0.8)",   # Purple
                "rgba(236, 72, 153, 0.8)",   # Pink
                "rgba(34, 197, 94, 0.8)",    # Emerald
                "rgba(249, 115, 22, 0.8)"    # Orange
            ]
            
            chart_data = {
                "labels": labels,
                "datasets": [{
                    "data": data,
                    "backgroundColor": colors[:len(data)],
                    "borderColor": "#ffffff",
                    "borderWidth": 2,
                    "hoverBorderWidth": 3
                }]
            }
        
        else:
            # Default fallback
            chart_data = {
                "labels": [f"Data Point {i+1}" for i in range(len(raw_data))],
                "datasets": [{
                    "label": " Values",
                    "data": [float(item.get("value", 0)) for item in raw_data],
                    "backgroundColor": "rgba(59, 130, 246, 0.8)",
                    "borderColor": "rgba(59, 130, 246, 1)",
                    "borderWidth": 2
                }]
            }
        
        return chart_data

    def _create_histogram_bins(self, values: List[float]) -> List[int]:
        """Create histogram bins for distribution charts"""
        bins = [0, 0, 0, 0, 0, 0]  # 6 bins
        for value in values:
            if value < 10:
                bins[0] += 1
            elif value < 20:
                bins[1] += 1
            elif value < 30:
                bins[2] += 1
            elif value < 40:
                bins[3] += 1
            elif value < 50:
                bins[4] += 1
            else:
                bins[5] += 1
        return bins
    
    def _map_query_to_sensor_type(self, query: str, language: str = "en", feature: str = "dashboard") -> Dict[str, Any]:
        """Map natural language query to sensor type using ontology"""
        # Normalize query (remove ZWJ, normalize Arabic letters, strip diacritics)
        def _normalize_text(text: str) -> str:
            try:
                import re
                # Remove zero-width non-joiner and joiner
                text = text.replace('\u200c', '').replace('\u200d', '')
                # Normalize Arabic Yeh/Kaf variants
                text = text.replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©')
                # Remove diacritics
                text = re.sub(r"[\u064B-\u065F]", "", text)
                return text
            except Exception:
                return text
        query_norm = _normalize_text(query)
        query_lower = query_norm.lower()
        ontology = self.ontology
        
        # Check for dangerous queries first (more specific to avoid false positives)
        dangerous_keywords = ['drop table', 'delete from', 'update set', 'insert into', 'alter table', 'create table', 'truncate table', 'remove table', 'clear table']
        for keyword in dangerous_keywords:
            if keyword in query_lower:
                logger.warning(f"  Dangerous query detected in sensor mapping: {keyword}")
                return {
                    "sensor_type": query,
                    "matched_term": keyword,
                    "mapping_type": "dangerous",
                    "confidence": 0.0,
                    "candidates": []
                }
        
        # Check for comparison queries FIRST (highest priority)
        comparison_words = ["Ù…Ù‚Ø§ÛŒØ³Ù‡", "compare", "vs", "Ø¨Ø§", "versus"]
        is_comparison = any(word in query_lower for word in comparison_words)
        matched_comparison_words = [word for word in comparison_words if word in query_lower]
        print(f" DEBUG _map_query_to_sensor_type: query={len(query)} characters")
        print(f" DEBUG _map_query_to_sensor_type: query_lower={len(query_lower)} characters")
        print(f" DEBUG _map_query_to_sensor_type: matched_comparison_words={matched_comparison_words}")
        print(f" DEBUG _map_query_to_sensor_type: is_comparison={is_comparison}")
        if is_comparison:
            print(f" COMPARISON DETECTED - proceed with normal ontology mapping; will only affect time/grouping")
            # Do NOT choose entity here. Comparison should influence time_range/grouping, not entity.
            # We still skip compound mappings later when is_comparison is True.
        
        # Check for compound queries (multiple sensors) - but NOT for comparison queries
        if not is_comparison:  # Skip compound mappings for comparison queries
            compound_mappings = {
            "fa": {
                "Ø®Ø§Ú© Ùˆ Ø¢Ø¨": ["soil_moisture", "water_usage"],
                "Ø¯Ù…Ø§ Ùˆ Ø±Ø·ÙˆØ¨Øª": ["temperature", "humidity"],
                "Ø¢ÙØ§Øª Ùˆ Ø¨ÛŒÙ…Ø§Ø±ÛŒ": ["pest_count", "disease_risk"],
                "Ø®Ø§Ú© Ùˆ Ú©ÙˆØ¯": ["soil_moisture", "fertilizer_usage"],
                "Ù†ÙˆØ± Ùˆ Ø¯Ù…Ø§": ["light", "temperature"],
                # COMPREHENSIVE IRRIGATION MAPPINGS
                "Ø§Ø¨ÛŒØ§Ø±ÛŒ": ["soil_moisture", "water_usage", "humidity", "temperature"],
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ": ["soil_moisture", "water_usage", "humidity", "temperature"],
                "Ø¢Ø¨ Ø¯Ø§Ø¯Ù†": ["soil_moisture", "water_usage", "humidity"],
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ú©Ù†Ù…": ["soil_moisture", "water_usage", "humidity", "temperature"],
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ú©Ù†Ù… ÛŒØ§ Ù†Ù‡": ["soil_moisture", "water_usage", "humidity", "temperature"],
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ø§Ù…Ø±ÙˆØ²": ["soil_moisture", "water_usage", "humidity", "temperature"],
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ù„Ø§Ø²Ù…": ["soil_moisture", "water_usage", "humidity"],
                "Ø¢Ø¨ Ø®Ø§Ú©": ["soil_moisture", "water_usage"],
                "Ø±Ø·ÙˆØ¨Øª Ø®Ø§Ú©": ["soil_moisture", "water_usage"],
                "Ù…ØµØ±Ù Ø¢Ø¨": ["water_usage", "water_efficiency"],
                "Ø¢Ø¨ Ù…ØµØ±ÙÛŒ": ["water_usage", "water_efficiency"],
                "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¢Ø¨": ["water_usage", "water_efficiency"],
                "Ø¨Ø§Ø²Ø¯Ù‡ÛŒ Ø¢Ø¨": ["water_efficiency", "water_usage"],
                "Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¢Ø¨": ["water_efficiency", "water_usage"],
                "Ù…ØµØ±Ù Ø¨Ù‡ÛŒÙ†Ù‡ Ø¢Ø¨": ["water_efficiency", "water_usage"],
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±": ["soil_moisture", "water_usage", "humidity"],
                "Ø³ÛŒØ³ØªÙ… Ø¢Ø¨ÛŒØ§Ø±ÛŒ": ["soil_moisture", "water_usage", "humidity"],
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ù‚Ø·Ø±Ù‡â€ŒØ§ÛŒ": ["soil_moisture", "water_usage", "humidity"],
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ø¨Ø§Ø±Ø§Ù†ÛŒ": ["soil_moisture", "water_usage", "humidity"],
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯": ["soil_moisture", "water_usage", "humidity", "temperature"],
                # COMPREHENSIVE PEST MANAGEMENT MAPPINGS
                "Ø³Ù… Ù¾Ø§Ø´ÛŒ": ["pest_count", "pest_detection", "disease_risk", "temperature", "humidity"],
                "Ø¢ÙØª Ú©Ø´": ["pest_count", "pest_detection", "disease_risk", "temperature", "humidity"],
                "Ø¢ÙØ§Øª": ["pest_count", "pest_detection", "disease_risk"],
                "Ú©Ù†ØªØ±Ù„ Ø¢ÙØ§Øª": ["pest_count", "pest_detection", "disease_risk"],
                "Ù…Ø¨Ø§Ø±Ø²Ù‡ Ø¨Ø§ Ø¢ÙØ§Øª": ["pest_count", "pest_detection", "disease_risk"],
                "Ù¾ÛŒØ´Ú¯ÛŒØ±ÛŒ Ø¢ÙØ§Øª": ["pest_count", "pest_detection", "disease_risk"],
                "Ø³Ù…ÙˆÙ…": ["pest_count", "pest_detection", "disease_risk"],
                "Ø¢ÙØªâ€ŒÚ©Ø´": ["pest_count", "pest_detection", "disease_risk"],
                "Ø­Ø´Ø±Ù‡â€ŒÚ©Ø´": ["pest_count", "pest_detection", "disease_risk"],
                "Ù‚Ø§Ø±Ú†â€ŒÚ©Ø´": ["pest_count", "pest_detection", "disease_risk"],
                "Ø¹Ù„Ùâ€ŒÚ©Ø´": ["pest_count", "pest_detection", "disease_risk"],
                "Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ú¯ÛŒØ§Ù‡ÛŒ": ["disease_risk", "pest_count"],
                "Ù‚Ø§Ø±Ú†": ["disease_risk", "pest_count"],
                "ÙˆÛŒØ±ÙˆØ³": ["disease_risk", "pest_count"],
                "Ø¨Ø§Ú©ØªØ±ÛŒ": ["disease_risk", "pest_count"],
                # COMPREHENSIVE ENVIRONMENTAL MAPPINGS
                "Ù…Ø­ÛŒØ·": ["temperature", "humidity", "co2_level", "light"],
                "Ú¯Ù„Ø®Ø§Ù†Ù‡": ["temperature", "humidity", "co2_level", "light"],
                "Ø´Ø±Ø§ÛŒØ· Ù…Ø­ÛŒØ·ÛŒ": ["temperature", "humidity", "co2_level", "light"],
                "Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§": ["temperature", "humidity", "pressure", "wind_speed", "rainfall"],
                "Ø§Ù‚Ù„ÛŒÙ…": ["temperature", "humidity", "pressure", "wind_speed", "rainfall"],
                "Ù‡ÙˆØ§": ["temperature", "humidity", "pressure", "wind_speed"],
                "Ø¬Ùˆ": ["temperature", "humidity", "pressure", "wind_speed"],
                "ØªÙ‡ÙˆÛŒÙ‡": ["temperature", "humidity", "co2_level", "wind_speed"],
                "ØªÙ‡ÙˆÛŒÙ‡ Ù…Ø·Ø¨ÙˆØ¹": ["temperature", "humidity", "co2_level", "wind_speed"],
                "Ø³ÛŒØ³ØªÙ… ØªÙ‡ÙˆÛŒÙ‡": ["temperature", "humidity", "co2_level", "wind_speed"],
                "ÙÙ†": ["temperature", "humidity", "co2_level", "wind_speed"],
                "Ú©ÙˆÙ„Ø±": ["temperature", "humidity", "co2_level"],
                "Ú¯Ø±Ù…Ø§ÛŒØ´": ["temperature", "humidity"],
                "Ø³Ø±Ù…Ø§ÛŒØ´": ["temperature", "humidity"],
                "Ú©Ù†ØªØ±Ù„ Ø¯Ù…Ø§": ["temperature", "humidity"],
                "Ú©Ù†ØªØ±Ù„ Ø±Ø·ÙˆØ¨Øª": ["humidity", "temperature"],
                "Ú©Ù†ØªØ±Ù„ Ù†ÙˆØ±": ["light", "temperature"],
                "Ú©Ù†ØªØ±Ù„ co2": ["co2_level", "temperature", "humidity"],
            },
            "en": {
                "soil and water": ["soil_moisture", "water_usage"],
                "temperature and humidity": ["temperature", "humidity"],
                "pests and disease": ["pest_count", "disease_risk"],
                "soil and fertilizer": ["soil_moisture", "fertilizer_usage"],
                "light and temperature": ["light", "temperature"],
                # COMPREHENSIVE IRRIGATION MAPPINGS
                "irrigation": ["soil_moisture", "water_usage", "humidity", "temperature"],
                "watering": ["soil_moisture", "water_usage", "humidity"],
                "should i water": ["soil_moisture", "water_usage", "humidity", "temperature"],
                "water today": ["soil_moisture", "water_usage", "humidity", "temperature"],
                "irrigate today": ["soil_moisture", "water_usage", "humidity", "temperature"],
                "soil moisture": ["soil_moisture", "water_usage"],
                "water usage": ["water_usage", "water_efficiency"],
                "water consumption": ["water_usage", "water_efficiency"],
                "water used": ["water_usage", "water_efficiency"],
                "water efficiency": ["water_efficiency", "water_usage"],
                "water optimization": ["water_efficiency", "water_usage"],
                "efficiency": ["water_efficiency", "water_usage"],
                "automatic irrigation": ["soil_moisture", "water_usage", "humidity"],
                "irrigation system": ["soil_moisture", "water_usage", "humidity"],
                "drip irrigation": ["soil_moisture", "water_usage", "humidity"],
                "sprinkler irrigation": ["soil_moisture", "water_usage", "humidity"],
                "smart irrigation": ["soil_moisture", "water_usage", "humidity", "temperature"],
                # COMPREHENSIVE PEST MANAGEMENT MAPPINGS
                "pesticide": ["pest_count", "pest_detection", "disease_risk", "temperature", "humidity"],
                "spray pesticide": ["pest_count", "pest_detection", "disease_risk", "temperature", "humidity"],
                "pest control": ["pest_count", "pest_detection", "disease_risk"],
                "pest management": ["pest_count", "pest_detection", "disease_risk"],
                "pest prevention": ["pest_count", "pest_detection", "disease_risk"],
                "insecticide": ["pest_count", "pest_detection", "disease_risk"],
                "fungicide": ["pest_count", "pest_detection", "disease_risk"],
                "herbicide": ["pest_count", "pest_detection", "disease_risk"],
                "plant disease": ["disease_risk", "pest_count"],
                "fungus": ["disease_risk", "pest_count"],
                "virus": ["disease_risk", "pest_count"],
                "bacteria": ["disease_risk", "pest_count"],
                # COMPREHENSIVE ENVIRONMENTAL MAPPINGS
                "environment": ["temperature", "humidity", "co2_level", "light"],
                "greenhouse": ["temperature", "humidity", "co2_level", "light"],
                "environmental conditions": ["temperature", "humidity", "co2_level", "light"],
                "weather": ["temperature", "humidity", "pressure", "wind_speed", "rainfall"],
                "climate": ["temperature", "humidity", "pressure", "wind_speed", "rainfall"],
                "air": ["temperature", "humidity", "pressure", "wind_speed"],
                "atmosphere": ["temperature", "humidity", "pressure", "wind_speed"],
                "ventilation": ["temperature", "humidity", "co2_level", "wind_speed"],
                "air conditioning": ["temperature", "humidity", "co2_level", "wind_speed"],
                "ventilation system": ["temperature", "humidity", "co2_level", "wind_speed"],
                "fan": ["temperature", "humidity", "co2_level", "wind_speed"],
                "cooler": ["temperature", "humidity", "co2_level"],
                "heating": ["temperature", "humidity"],
                "cooling": ["temperature", "humidity"],
                "temperature control": ["temperature", "humidity"],
                "humidity control": ["humidity", "temperature"],
                "light control": ["light", "temperature"],
                "co2 control": ["co2_level", "temperature", "humidity"]
            }
        }
        
            # Check for compound queries first
            for compound_query, sensor_types in compound_mappings.get(language, {}).items():
                if compound_query in query_lower:
                    logger.info(f" Mapped compound query '{compound_query}' to sensors: {sensor_types}")
                    return {
                        "sensor_type": sensor_types,  # Return as list, not compound string
                        "matched_term": compound_query,
                        "mapping_type": "compound",
                        "confidence": 0.9,
                        "candidates": sensor_types
                    }
        
        # Map query to sensor type using ontology
        sensor_mappings = ontology.get("sensor_mappings", {})
        
        # First pass: Check for exact matches (more specific terms first)
        exact_matches = []
        for sensor_type, mapping in sensor_mappings.items():
            # Check Persian terms
            if language == "fa":
                persian_terms = mapping.get("persian_terms", [])
                for term in persian_terms:
                    if term in query_lower:
                        exact_matches.append((len(term), sensor_type, term))  # Sort by length (longer = more specific)
            
            # Check English terms
            english_terms = mapping.get("english_terms", [])
            for term in english_terms:
                if term in query_lower:
                    exact_matches.append((len(term), sensor_type, term))  # Sort by length (longer = more specific)
        
        # Second pass: Check for partial and phrase matches (if no exact match found)
        if not exact_matches:
            partial_matches = []
            # High-priority pest growth phrases
            pest_growth_phrases_fa = ["Ø±Ø´Ø¯ Ø¢ÙØ§Øª", "Ø§ÙØ²Ø§ÛŒØ´ Ø¢ÙØ§Øª", "Ø±Ø´Ø¯ Ø¢ÙØª", "Ø§ÙØ²Ø§ÛŒØ´ Ø¢ÙØª"]
            pest_growth_phrases_en = ["pest growth", "increase in pests", "pest trend"]
            if any(p in query_lower for p in pest_growth_phrases_fa + pest_growth_phrases_en):
                logger.info(" Phrase match: pest growth -> pest_count")
                return {
                    "sensor_type": "pest_count",
                    "matched_term": "pest_growth_phrase",
                    "mapping_type": "phrase",
                    "confidence": 0.92,
                    "candidates": ["pest_count"]
                }

            # Language-agnostic heuristic: pest tokens + growth/trend tokens in any order
            pest_tokens_fa = ["Ø¢ÙØ§Øª", "Ø¢ÙØª", "Ø­Ø´Ø±Ù‡", "Ø­Ø´Ø±Ø§Øª"]
            pest_tokens_en = ["pest", "pests", "insect", "insects"]
            growth_tokens_fa = ["Ø±Ø´Ø¯", "Ø§ÙØ²Ø§ÛŒØ´", "Ø±ÙˆÙ†Ø¯", "ØªØºÛŒÛŒØ±Ø§Øª"]
            growth_tokens_en = ["growth", "increase", "trend", "changes"]
            has_pest = any(t in query_lower for t in pest_tokens_fa + pest_tokens_en)
            has_growth = any(t in query_lower for t in growth_tokens_fa + growth_tokens_en)
            if has_pest and has_growth:
                logger.info(" Heuristic match: pest + growth terms -> pest_count")
                return {
                    "sensor_type": "pest_count",
                    "matched_term": "pest_growth_heuristic",
                    "mapping_type": "heuristic",
                    "confidence": 0.9,
                    "candidates": ["pest_count"]
                }
            for sensor_type, mapping in sensor_mappings.items():
                # Check Persian terms for partial matches
                if language == "fa":
                    persian_terms = mapping.get("persian_terms", [])
                    for term in persian_terms:
                        # Check if any word from the term appears in the query
                        term_words = term.split()
                        for word in term_words:
                            if len(word) > 2 and word in query_lower:  # Only words longer than 2 characters
                                partial_matches.append((len(word), sensor_type, word))
                
                # Check English terms for partial matches
                english_terms = mapping.get("english_terms", [])
                for term in english_terms:
                    term_words = term.split()
                    for word in term_words:
                        if len(word) > 2 and word in query_lower:
                            partial_matches.append((len(word), sensor_type, word))
            
            # Sort by length (descending) to prioritize longer matches
            partial_matches.sort(reverse=True)
            if partial_matches:
                _, sensor_type, word = partial_matches[0]
                logger.info(f" Mapped partial term '{word}' to sensor '{sensor_type}'")
                print(f" Mapped partial term '{word}' to sensor '{sensor_type}'")
                return {
                    "sensor_type": sensor_type,
                    "matched_term": word,
                    "mapping_type": "partial",
                    "confidence": 0.8,
                    "candidates": [sensor_type]
                }
        
        # Sort by length (descending) to prioritize more specific terms
        exact_matches.sort(reverse=True)
        
        if exact_matches:
            # Take the longest (most specific) match
            _, sensor_type, term = exact_matches[0]
            logger.info(f" Mapped specific term '{term}' to sensor '{sensor_type}'")
            print(f" Mapped specific term '{term}' to sensor '{sensor_type}'")
            return {
                "sensor_type": sensor_type,
                "matched_term": term,
                "mapping_type": "exact",
                "confidence": 0.95,
                "candidates": [sensor_type]
            }

        # Feature-based tie-break (only if still no mapping)
        if feature in ["pest-detection", "pest", "pest_detection"]:
            logger.info(" Feature bias: pest-detection -> pest_count")
            return {
                "sensor_type": "pest_count",
                "matched_term": "feature_bias",
                "mapping_type": "feature_bias",
                "confidence": 0.75,
                "candidates": ["pest_count"]
            }
        
        # Enhanced fallback with fuzzy matching
        fallback_mappings = {
            "fa": {
                "Ú†Ø·ÙˆØ±Ù‡": "temperature",  # Default to temperature for general "how is" queries
                "ÙˆØ¶Ø¹ÛŒØª": "temperature",  # Default to temperature for general "status" queries
                "Ø§Ù…Ø±ÙˆØ²": "temperature",  # Default to temperature for "today" queries
                "Ø§Ù„Ø§Ù†": "temperature",   # Default to temperature for "now" queries
                "ÙØ¹Ù„Ø§": "temperature",   # Default to temperature for "currently" queries
                "Ø­Ø§Ù„Ø§": "temperature",   # Default to temperature for "now" queries
                "Ø§Ú©Ù†ÙˆÙ†": "temperature",  # Default to temperature for "now" queries
                "Ø¨Ø±Ú¯": "leaf_wetness",   # "leaf" maps to leaf wetness
                "Ù…ÛŒÙˆÙ‡": "fruit_count",   # "fruit" maps to fruit count
                "ØªØ³Øª": "test_temperature", # "test" maps to test temperature
                "Ø¢Ø²Ù…Ø§ÛŒØ´": "test_temperature" # "experiment" maps to test temperature
            },
            "en": {
                "how": "temperature",    # Default to temperature for "how" queries
                "status": "temperature", # Default to temperature for "status" queries
                "today": "temperature",  # Default to temperature for "today" queries
                "now": "temperature",    # Default to temperature for "now" queries
                "current": "temperature", # Default to temperature for "current" queries
                "leaf": "leaf_wetness",  # "leaf" maps to leaf wetness
                "fruit": "fruit_count",  # "fruit" maps to fruit count
                "test": "test_temperature", # "test" maps to test temperature
                "experimental": "test_temperature" # "experimental" maps to test temperature
            }
        }
        
        # Context-specific mappings (higher priority than general fallbacks)
        context_mappings = {
            "fa": {
                "Ø®Ø§Ú©": "soil_moisture",  # "soil" maps to soil moisture
                "Ø²Ù…ÛŒÙ†": "soil_moisture", # "ground" maps to soil moisture
                "Ø¢Ø¨": "water_usage",     # "water" maps to water usage
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ": "water_usage", # "irrigation" maps to water usage
                "Ø¯Ù…Ø§": "temperature",    # "temperature" maps to temperature
                "Ø±Ø·ÙˆØ¨Øª": "humidity",     # "humidity" maps to humidity
                "Ø¢ÙØ§Øª": "pest_count",    # "pests" maps to pest count
                "Ø¨ÛŒÙ…Ø§Ø±ÛŒ": "disease_risk" # "disease" maps to disease risk
            },
            "en": {
                "soil": "soil_moisture",  # "soil" maps to soil moisture
                "ground": "soil_moisture", # "ground" maps to soil moisture
                "water": "water_usage",   # "water" maps to water usage
                "irrigation": "water_usage", # "irrigation" maps to water usage
                "temperature": "temperature", # "temperature" maps to temperature
                "humidity": "humidity",   # "humidity" maps to humidity
                "pest": "pest_count",     # "pest" maps to pest count
                "disease": "disease_risk" # "disease" maps to disease risk
            }
        }
        
        # Check context-specific mappings first (higher priority)
        for term, sensor_type in context_mappings.get(language, {}).items():
            if term in query_lower:
                logger.info(f" Mapped context term '{term}' to sensor '{sensor_type}'")
                return {
                    "sensor_type": sensor_type,
                    "matched_term": term,
                    "mapping_type": "context",
                    "confidence": 0.8,
                    "candidates": [sensor_type]
                }
        
        # No direct mapping found - use LLM to find closest ontology match
        logger.info(f" No direct mapping found for query: {query}, using LLM to find closest ontology match")
        return self._llm_map_to_ontology(query, language)
    
    def _llm_map_to_ontology(self, query: str, language: str = "en") -> Dict[str, Any]:
        """Use LLM to map query to closest ontology sensor type and log new synonyms"""
        try:
            # Get all available sensor types from ontology
            sensor_mappings = self.ontology.get("sensor_mappings", {})
            available_sensors = list(sensor_mappings.keys())
            
            # Create ontology context for LLM
            ontology_context = "\n".join([
                f"- {sensor}: {mapping.get('description', 'No description')}"
                for sensor, mapping in sensor_mappings.items()
            ])
            
            prompt = f"""You are an expert in agricultural sensor data mapping. Your task is to map the user query to the closest matching sensor type from our ontology.

USER QUERY: "{query}"
LANGUAGE: {language}

AVAILABLE SENSOR TYPES IN ONTOLOGY:
{ontology_context}

INSTRUCTIONS:
1. Analyze the user query and find the closest matching sensor type from the ontology
2. If the query contains terms not in the ontology, suggest the closest semantic match
3. Return your response in this exact JSON format:
{{
    "sensor_type": "closest_sensor_type",
    "matched_term": "term_from_query",
    "mapping_type": "llm_mapping",
    "confidence": 0.0-1.0,
    "reasoning": "explanation of why this sensor type was chosen",
    "new_synonyms": ["new_term1", "new_term2"] // terms from query not in ontology
}}

EXAMPLES:
- Query: "Ø±Ø´Ø¯ Ø¢ÙØ§Øª" -> sensor_type: "pest_count" (pest growth = pest count)
- Query: "Ø±Ø·ÙˆØ¨Øª Ø®Ø§Ú©" -> sensor_type: "soil_moisture" (soil humidity = soil moisture)
- Query: "Ù…ØµØ±Ù Ø¢Ø¨" -> sensor_type: "water_usage" (water consumption = water usage)

RESPONSE:"""

            # Log LLM call
            truncated_prompt = prompt[:200] + "..." if len(prompt) > 200 else prompt
            logger.info(f"ğŸ¤– LLM Call - Ontology Mapping (Prompt): {truncated_prompt}")
            
            response = self.llm.invoke(prompt)
            response_text = getattr(response, 'content', None) or getattr(response, 'text', '')
            
            # Log LLM response
            truncated_response = response_text[:100] + "..." if len(response_text) > 100 else response_text
            logger.info(f"ğŸ¤– LLM Call - Ontology Mapping (Response): {truncated_response}")
            
            # Parse JSON response
            import json
            try:
                result = json.loads(response_text.strip())
                
                # Log new synonyms for developer to add to ontology
                if result.get("new_synonyms"):
                    logger.info(f" NEW SYNONYMS DETECTED for developer to add to ontology:")
                    for synonym in result["new_synonyms"]:
                        logger.info(f"   - '{synonym}' -> '{result['sensor_type']}'")
                    
                    # Persist synonyms to ontology for future use
                    self._persist_synonyms_to_ontology(result["new_synonyms"], result["sensor_type"])
                
                # Ensure required fields
                return {
                    "sensor_type": result.get("sensor_type", "temperature"),
                    "matched_term": result.get("matched_term", "llm_mapped"),
                    "mapping_type": "llm_mapping",
                    "confidence": result.get("confidence", 0.7),
                    "candidates": [result.get("sensor_type", "temperature")],
                    "reasoning": result.get("reasoning", "LLM mapping"),
                    "new_synonyms": result.get("new_synonyms", [])
                }
                
            except json.JSONDecodeError:
                logger.error(f" Failed to parse LLM ontology mapping response: {response_text}")
                # Fallback to pest_count for pest-related queries
                if any(term in query.lower() for term in ["Ø¢ÙØ§Øª", "pest", "Ø±Ø´Ø¯", "growth"]):
                    return {
                        "sensor_type": "pest_count",
                        "matched_term": "pest_related",
                        "mapping_type": "llm_fallback",
                        "confidence": 0.5,
                        "candidates": ["pest_count"],
                        "reasoning": "Fallback to pest_count for pest-related query",
                        "new_synonyms": []
                    }
                else:
                    return {
                        "sensor_type": "temperature",
                        "matched_term": "llm_fallback",
                        "mapping_type": "llm_fallback",
                        "confidence": 0.3,
                        "candidates": ["temperature"],
                        "reasoning": "LLM parsing failed, using temperature fallback",
                        "new_synonyms": []
                    }
                    
        except Exception as e:
            logger.error(f" Error in LLM ontology mapping: {e}")
            logger.error(f" ERROR: Ontology mapping failed for query: {query}")
            logger.error(f" ERROR: Language: {language}, Available sensors: {list(self.ontology.get('sensor_mappings', {}).keys())}")
            return {
                "sensor_type": "temperature",
                "matched_term": "error_fallback",
                "mapping_type": "error_fallback",
                "confidence": 0.1,
                "candidates": ["temperature"],
                "reasoning": f"Error in LLM mapping: {str(e)}",
                "new_synonyms": []
            }
    
    def _generate_ontology_sql(self, english_query: str, language: str = "en") -> str:
        """Generate SQL query using ontology mappings"""
        try:
            logger.info(f" Generating ontology-based SQL for: '{english_query}'")
            
            # Map query to sensor type
            mapping_result = self._map_query_to_sensor_type(english_query, language)
            sensor_type = mapping_result["sensor_type"]
            
            # Handle compound queries (sensor_type is already a list)
            if isinstance(sensor_type, list):
                return self._generate_compound_sql(sensor_type, english_query, language)
            
            # Determine query pattern
            query_patterns = self.ontology.get("query_patterns", {})
            query_lower = english_query.lower()
            
            # Check for current/latest value
            current_indicators = query_patterns.get("current_value", {}).get("persian", []) + \
                               query_patterns.get("current_value", {}).get("english", [])
            if any(indicator in query_lower for indicator in current_indicators):
                sql = f"SELECT * FROM sensor_data WHERE sensor_type = '{sensor_type}' ORDER BY timestamp DESC LIMIT 1"
                logger.info(f" Generated current value SQL: {sql}")
                return sql
            
            # Check for average value
            avg_indicators = query_patterns.get("average_value", {}).get("persian", []) + \
                           query_patterns.get("average_value", {}).get("english", [])
            if any(indicator in query_lower for indicator in avg_indicators):
                sql = f"SELECT AVG(value) as avg_value FROM sensor_data WHERE sensor_type = '{sensor_type}'"
                logger.info(f" Generated average value SQL: {sql}")
                return sql
            
            # Check for time-based queries using new time-aware system
            time_config = self._parse_time_expression(english_query, language)
            if time_config:
                sql = self._generate_time_aware_sql(sensor_type, time_config, english_query)
                logger.info(f" Generated time-aware SQL: {sql}")
                return sql
            
            # Check for trend analysis
            trend_indicators = query_patterns.get("trend_analysis", {}).get("persian", []) + \
                             query_patterns.get("trend_analysis", {}).get("english", [])
            if any(indicator in query_lower for indicator in trend_indicators):
                sql = f"SELECT timestamp, value FROM sensor_data WHERE sensor_type = '{sensor_type}' ORDER BY timestamp DESC LIMIT 10"
                logger.info(f" Generated trend analysis SQL: {sql}")
                return sql
            
            # Fallback: Default to last 24 hours with hourly aggregation
            logger.info(f" No time expression detected, using fallback: last 24 hours with hourly aggregation")
            fallback_config = {"days": 1, "granularity": "hour"}
            sql = self._generate_time_aware_sql(sensor_type, fallback_config, english_query)
            logger.info(f" Generated fallback SQL: {sql}")
            return sql
            
        except Exception as e:
            logger.error(f" Error in ontology SQL generation: {e}")
            return self._generate_simple_sql(english_query)
    
    def _parse_time_expression(self, query: str, language: str = "en") -> Dict[str, Any]:
        """ROBUST TIME PARSER: Parse ANY time expression intelligently using regex patterns"""
        try:
            import re
            query_lower = query.lower()
            
            # UNIVERSAL TIME PARSER - Handles ALL time expressions with regex
            time_patterns = [
                # Hours patterns (most specific first)
                (r'(\d+)\s*hours?\s*(?:ago|back|earlier|before)', self._parse_hours_ago),
                (r'(?:last|past|previous|recent)\s*(\d+)\s*hours?', self._parse_hours_last),
                (r'(?:in|for|over|during)\s*(?:the\s*)?(?:last|past|previous|recent)\s*(\d+)\s*hours?', self._parse_hours_last),
                (r'(?:this|current)\s*(\d+)\s*hours?', self._parse_hours_current),
                
                # Word number patterns for hours
                (r'(?:last|past|previous|recent)\s*(two|three|four|five|six|seven|eight|nine|ten|eleven|twelve)\s*hours?', self._parse_hours_last_word),
                
                # Days patterns
                (r'(\d+)\s*days?\s*(?:ago|back|earlier|before)', self._parse_days_ago),
                (r'(?:last|past|previous|recent)\s*(\d+)\s*days?', self._parse_days_last),
                (r'(?:in|for|over|during)\s*(?:the\s*)?(?:last|past|previous|recent)\s*(\d+)\s*days?', self._parse_days_last),
                (r'(?:this|current)\s*(\d+)\s*days?', self._parse_days_current),
                
                # Weeks patterns
                (r'(\d+)\s*weeks?\s*(?:ago|back|earlier|before)', self._parse_weeks_ago),
                (r'(?:last|past|previous|recent)\s*(\d+)\s*weeks?', self._parse_weeks_last),
                (r'(?:in|for|over|during)\s*(?:the\s*)?(?:last|past|previous|recent)\s*(\d+)\s*weeks?', self._parse_weeks_last),
                (r'(?:this|current)\s*(\d+)\s*weeks?', self._parse_weeks_current),
                
                # Minutes patterns
                (r'(\d+)\s*minutes?\s*(?:ago|back|earlier|before)', self._parse_minutes_ago),
                (r'(?:last|past|previous|recent)\s*(\d+)\s*minutes?', self._parse_minutes_last),
                
                # Special patterns
                (r'(?:today|this\s*day)', self._parse_today),
                (r'(?:yesterday|yesterdays)', self._parse_yesterday),
                (r'(?:this\s*week|current\s*week)', self._parse_this_week),
                (r'(?:last\s*week|previous\s*week)', self._parse_last_week),
                (r'(?:this\s*month|current\s*month)', self._parse_this_month),
                (r'(?:last\s*month|previous\s*month)', self._parse_last_month),
            ]
            
            # Try each pattern in order
            for pattern, parser_func in time_patterns:
                match = re.search(pattern, query_lower)
                if match:
                    result = parser_func(match, query_lower)
                    if result:
                        return result
            
            # Default fallback
            return {"days": 1, "hours": 24, "minutes": 1440, "granularity": "day"}
            
        except Exception as e:
            logger.error(f"Error in robust time parser: {e}")
            return {"days": 1, "hours": 24, "minutes": 1440, "granularity": "day"}
    
    def _parse_hours_ago(self, match, query_lower):
        """Parse X hours ago patterns"""
        hours = int(match.group(1))
        return {"hours": hours, "granularity": "hour", "offset": -hours}
    
    def _parse_hours_last(self, match, query_lower):
        """Parse last X hours patterns"""
        hours = int(match.group(1))
        return {"hours": hours, "granularity": "hour"}
    
    def _parse_hours_last_word(self, match, query_lower):
        """Parse last X hours patterns with word numbers"""
        word_to_number = {
            "two": 2, "three": 3, "four": 4, "five": 5, "six": 6,
            "seven": 7, "eight": 8, "nine": 9, "ten": 10, "eleven": 11, "twelve": 12
        }
        word = match.group(1)
        hours = word_to_number.get(word, 2)  # Default to 2 if not found
        return {"hours": hours, "granularity": "hour"}
    
    def _parse_hours_current(self, match, query_lower):
        """Parse this X hours patterns"""
        hours = int(match.group(1))
        return {"hours": hours, "granularity": "hour"}
    
    def _parse_days_ago(self, match, query_lower):
        """Parse X days ago patterns"""
        days = int(match.group(1))
        return {"days": days, "granularity": "day", "offset": -days}
    
    def _parse_days_last(self, match, query_lower):
        """Parse last X days patterns"""
        days = int(match.group(1))
        return {"days": days, "granularity": "day"}
    
    def _parse_days_current(self, match, query_lower):
        """Parse this X days patterns"""
        days = int(match.group(1))
        return {"days": days, "granularity": "day"}
    
    def _parse_weeks_ago(self, match, query_lower):
        """Parse X weeks ago patterns"""
        weeks = int(match.group(1))
        return {"days": weeks * 7, "granularity": "week", "offset": -(weeks * 7)}
    
    def _parse_weeks_last(self, match, query_lower):
        """Parse last X weeks patterns"""
        weeks = int(match.group(1))
        return {"days": weeks * 7, "granularity": "week"}
    
    def _parse_weeks_current(self, match, query_lower):
        """Parse this X weeks patterns"""
        weeks = int(match.group(1))
        return {"days": weeks * 7, "granularity": "week"}
    
    def _parse_minutes_ago(self, match, query_lower):
        """Parse X minutes ago patterns"""
        minutes = int(match.group(1))
        return {"minutes": minutes, "granularity": "minute", "offset": -minutes}
    
    def _parse_minutes_last(self, match, query_lower):
        """Parse last X minutes patterns"""
        minutes = int(match.group(1))
        return {"minutes": minutes, "granularity": "minute"}
    
    def _parse_today(self, match, query_lower):
        """Parse today patterns"""
        return {"days": 1, "granularity": "day"}
    
    def _parse_yesterday(self, match, query_lower):
        """Parse yesterday patterns"""
        return {"days": 1, "granularity": "day", "offset": -1}
    
    def _parse_this_week(self, match, query_lower):
        """Parse this week patterns"""
        return {"days": 7, "granularity": "week"}
    
    def _parse_last_week(self, match, query_lower):
        """Parse last week patterns"""
        return {"days": 7, "granularity": "week", "offset": -7}
    
    def _parse_this_month(self, match, query_lower):
        """Parse this month patterns"""
        return {"days": 30, "granularity": "day"}
    
    def _parse_last_month(self, match, query_lower):
        """Parse last month patterns"""
        return {"days": 30, "granularity": "day", "offset": -30}
    
    def _map_hours_to_time_range(self, hours: int) -> str:
        """ROBUST HOURS MAPPING: Map any number of hours to the correct time range"""
        if hours <= 1:
            return "last_hour"
        elif hours <= 2:
            return "last_2_hours"
        elif hours <= 4:
            return "last_4_hours"
        elif hours <= 6:
            return "last_6_hours"
        elif hours <= 8:
            return "last_8_hours"
        elif hours <= 12:
            return "last_12_hours"
        elif hours <= 24:
            return "last_24_hours"
        elif hours <= 48:
            return "last_2_days"
        elif hours <= 72:
            return "last_3_days"
        elif hours <= 168:  # 1 week
            return "last_week"
        else:
            return "last_30_days"
    
    def _map_days_to_time_range(self, days: int) -> str:
        """ROBUST DAYS MAPPING: Map any number of days to the correct time range"""
        if days <= 1:
            return "last_24_hours"
        elif days <= 2:
            return "last_2_days"
        elif days <= 3:
            return "last_3_days"
        elif days <= 7:
            return "last_week"
        elif days <= 14:
            return "last_2_weeks"
        elif days <= 30:
            return "last_month"
        else:
            return "last_30_days"
    
    def _map_minutes_to_time_range(self, minutes: int) -> str:
        """ROBUST MINUTES MAPPING: Map any number of minutes to the correct time range"""
        if minutes <= 30:
            return "last_30_minutes"
        elif minutes <= 60:
            return "last_hour"
        elif minutes <= 120:
            return "last_2_hours"
        elif minutes <= 240:
            return "last_4_hours"
        elif minutes <= 360:
            return "last_6_hours"
        elif minutes <= 480:
            return "last_8_hours"
        elif minutes <= 720:
            return "last_12_hours"
        else:
            return "last_24_hours"
            
            # Persian time patterns with granularity and enhanced numeral support
            if language == "fa":
                persian_patterns = {
                    # Hours - FIXED: Use hour granularity for hour queries
                    "ÛŒÚ© Ø³Ø§Ø¹Øª": {"hours": 1, "granularity": "hour"},
                    "Ø¯Ùˆ Ø³Ø§Ø¹Øª": {"hours": 2, "granularity": "hour"},
                    "Ø³Ù‡ Ø³Ø§Ø¹Øª": {"hours": 3, "granularity": "hour"},
                    "Ú†Ù‡Ø§Ø± Ø³Ø§Ø¹Øª": {"hours": 4, "granularity": "hour"},
                    "Ù¾Ù†Ø¬ Ø³Ø§Ø¹Øª": {"hours": 5, "granularity": "hour"},
                    "Ø´Ø´ Ø³Ø§Ø¹Øª": {"hours": 6, "granularity": "hour"},
                    "Ù‡ÙØª Ø³Ø§Ø¹Øª": {"hours": 7, "granularity": "hour"},
                    "Ù‡Ø´Øª Ø³Ø§Ø¹Øª": {"hours": 8, "granularity": "hour"},
                    "Ù†Ù‡ Ø³Ø§Ø¹Øª": {"hours": 9, "granularity": "hour"},
                    "Ø¯Ù‡ Ø³Ø§Ø¹Øª": {"hours": 10, "granularity": "hour"},
                    "Ø¯ÙˆØ§Ø²Ø¯Ù‡ Ø³Ø§Ø¹Øª": {"hours": 12, "granularity": "hour"},
                    "Ø¨ÛŒØ³Øª Ùˆ Ú†Ù‡Ø§Ø± Ø³Ø§Ø¹Øª": {"hours": 24, "granularity": "hour"},
                    # Days with numerals - FIXED: Use day granularity for day queries
                    "ÛŒÚ© Ø±ÙˆØ²": {"days": 1, "granularity": "day"},
                    "Ø¯Ùˆ Ø±ÙˆØ²": {"days": 2, "granularity": "day"},
                    "Ø³Ù‡ Ø±ÙˆØ²": {"days": 3, "granularity": "day"},
                    "Ú†Ù‡Ø§Ø± Ø±ÙˆØ²": {"days": 4, "granularity": "day"},
                    "Ù¾Ù†Ø¬ Ø±ÙˆØ²": {"days": 5, "granularity": "day"},
                    "Ø´Ø´ Ø±ÙˆØ²": {"days": 6, "granularity": "day"},
                    "Ù‡ÙØª Ø±ÙˆØ²": {"days": 7, "granularity": "day"},
                    # Weeks - FIXED: Use week granularity for week queries
                    "ÛŒÚ© Ù‡ÙØªÙ‡": {"days": 7, "granularity": "week"},
                    "Ø¯Ùˆ Ù‡ÙØªÙ‡": {"days": 14, "granularity": "week"},
                    "Ø³Ù‡ Ù‡ÙØªÙ‡": {"days": 21, "granularity": "week"},
                    # Months
                    "ÛŒÚ© Ù…Ø§Ù‡": {"days": 30, "granularity": "day"},
                    "Ø¯Ùˆ Ù…Ø§Ù‡": {"days": 60, "granularity": "day"},
                    "Ø³Ù‡ Ù…Ø§Ù‡": {"days": 90, "granularity": "day"},
                    # Minutes - COMPLETE COVERAGE
                    "ÛŒÚ© Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 1, "granularity": "minute"},
                    "Ø¯Ùˆ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 2, "granularity": "minute"},
                    "Ø³Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 3, "granularity": "minute"},
                    "Ú†Ù‡Ø§Ø± Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 4, "granularity": "minute"},
                    "Ù¾Ù†Ø¬ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 5, "granularity": "minute"},
                    "Ø´Ø´ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 6, "granularity": "minute"},
                    "Ù‡ÙØª Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 7, "granularity": "minute"},
                    "Ù‡Ø´Øª Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 8, "granularity": "minute"},
                    "Ù†Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 9, "granularity": "minute"},
                    "Ø¯Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 10, "granularity": "minute"},
                    "Ù¾Ø§Ù†Ø²Ø¯Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 15, "granularity": "minute"},
                    "Ø¨ÛŒØ³Øª Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 20, "granularity": "minute"},
                    "Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 30, "granularity": "minute"},
                    "Ú†Ù‡Ù„ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 40, "granularity": "minute"},
                    "Ù¾Ù†Ø¬Ø§Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 50, "granularity": "minute"},
                    "Ø´ØµØª Ø¯Ù‚ÛŒÙ‚Ù‡": {"minutes": 60, "granularity": "minute"},
                    # Special cases
                    "Ø§Ù…Ø±ÙˆØ²": {"days": 1, "granularity": "hour"},
                    "Ø¯ÛŒØ±ÙˆØ²": {"days": 1, "granularity": "hour", "offset": -1},
                    "Ù‡ÙØªÙ‡ Ú¯Ø°Ø´ØªÙ‡": {"days": 7, "granularity": "week", "offset": -7},
                    "Ù‡ÙØªÙ‡ Ø§Ø®ÛŒØ±": {"days": 7, "granularity": "week"},
                    "Ù…Ø§Ù‡ Ù‚Ø¨Ù„": {"days": 30, "granularity": "day", "offset": -30},
                    "Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±": {"days": 30, "granularity": "day"},
                    "Ø§Ø®ÛŒØ±Ø§Ù‹": {"days": 1, "granularity": "hour"},
                    "Ú¯Ø°Ø´ØªÙ‡": {"days": 3, "granularity": "day"},
                    "Ø§Ø®ÛŒØ±": {"days": 3, "granularity": "day"},
                    
                    # Enhanced "ago" patterns
                    "ÛŒÚ© Ø³Ø§Ø¹Øª Ù¾ÛŒØ´": {"hours": 1, "granularity": "hour", "offset": -1},
                    "Ø¯Ùˆ Ø³Ø§Ø¹Øª Ù¾ÛŒØ´": {"hours": 2, "granularity": "hour", "offset": -2},
                    "Ø³Ù‡ Ø³Ø§Ø¹Øª Ù¾ÛŒØ´": {"hours": 3, "granularity": "hour", "offset": -3},
                    "Ú†Ù‡Ø§Ø± Ø³Ø§Ø¹Øª Ù¾ÛŒØ´": {"hours": 4, "granularity": "hour", "offset": -4},
                    "Ù¾Ù†Ø¬ Ø³Ø§Ø¹Øª Ù¾ÛŒØ´": {"hours": 5, "granularity": "hour", "offset": -5},
                    "Ø´Ø´ Ø³Ø§Ø¹Øª Ù¾ÛŒØ´": {"hours": 6, "granularity": "hour", "offset": -6},
                    
                    # Enhanced "last" patterns
                    "Ù‡ÙØªÙ‡ Ú¯Ø°Ø´ØªÙ‡": {"days": 7, "granularity": "week", "offset": -7},
                    "Ù…Ø§Ù‡ Ú¯Ø°Ø´ØªÙ‡": {"days": 30, "granularity": "day", "offset": -30},
                    "Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡": {"days": 1, "granularity": "day", "offset": -1},
                    
                    # Enhanced "recent" patterns
                    "Ø§Ø®ÛŒØ±Ø§Ù‹": {"days": 1, "granularity": "hour"},
                    "Ø§Ø®ÛŒØ±": {"days": 3, "granularity": "day"},
                    "Ú¯Ø°Ø´ØªÙ‡": {"days": 3, "granularity": "day"}
                }
                
                # Check for exact pattern matches first
                for pattern, config in persian_patterns.items():
                    if pattern in query_lower:
                        logger.info(f" Parsed Persian time pattern: {pattern} -> {config}")
                        return config
                
                # Enhanced regex-based parsing for Persian numerals
                import re
                
                # Pattern for "X Ø±ÙˆØ² Ø§Ø®ÛŒØ±/Ú¯Ø°Ø´ØªÙ‡" where X is Persian numeral or digit
                # Use word boundaries to match complete Persian words
                day_pattern = r'\b(ÛŒÚ©|Ø¯Ùˆ|Ø³Ù‡|Ú†Ù‡Ø§Ø±|Ù¾Ù†Ø¬|Ø´Ø´|Ù‡ÙØª|Ù‡Ø´Øª|Ù†Ù‡|Ø¯Ù‡|Û±|Û²|Û³|Û´|Ûµ|Û¶|Û·|Û¸|Û¹|Û°|\d+)\s*Ø±ÙˆØ²\s*(Ø§Ø®ÛŒØ±|Ú¯Ø°Ø´ØªÙ‡)'
                day_match = re.search(day_pattern, query_lower)
                if day_match:
                    number_text = day_match.group(1)
                    # Convert Persian numeral to number
                    if number_text in persian_numerals:
                        days = persian_numerals[number_text]
                    else:
                        try:
                            days = int(number_text)
                        except ValueError:
                            days = 3  # Default fallback
                    
                    # FIXED: Use day granularity for day queries
                    granularity = "day"
                    logger.info(f" Parsed Persian numeral pattern: {number_text} Ø±ÙˆØ² -> {days} days, granularity: {granularity}")
                    return {"days": days, "granularity": granularity}
                
                # Pattern for "X Ù‡ÙØªÙ‡ Ø§Ø®ÛŒØ±/Ú¯Ø°Ø´ØªÙ‡"
                week_pattern = r'\b(ÛŒÚ©|Ø¯Ùˆ|Ø³Ù‡|Ú†Ù‡Ø§Ø±|Ù¾Ù†Ø¬|Ø´Ø´|Ù‡ÙØª|Ù‡Ø´Øª|Ù†Ù‡|Ø¯Ù‡|Û±|Û²|Û³|Û´|Ûµ|Û¶|Û·|Û¸|Û¹|Û°|\d+)\s*Ù‡ÙØªÙ‡\s*(Ø§Ø®ÛŒØ±|Ú¯Ø°Ø´ØªÙ‡)'
                week_match = re.search(week_pattern, query_lower)
                if week_match:
                    number_text = week_match.group(1)
                    if number_text in persian_numerals:
                        weeks = persian_numerals[number_text]
                    else:
                        try:
                            weeks = int(number_text)
                        except ValueError:
                            weeks = 1  # Default fallback
                    
                    days = weeks * 7
                    # FIXED: Use week granularity for week queries
                    logger.info(f" Parsed Persian numeral pattern: {number_text} Ù‡ÙØªÙ‡ -> {days} days, granularity: week")
                    return {"days": days, "granularity": "week"}
                
                # Pattern for "X Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±/Ú¯Ø°Ø´ØªÙ‡"
                month_pattern = r'\b(ÛŒÚ©|Ø¯Ùˆ|Ø³Ù‡|Ú†Ù‡Ø§Ø±|Ù¾Ù†Ø¬|Ø´Ø´|Ù‡ÙØª|Ù‡Ø´Øª|Ù†Ù‡|Ø¯Ù‡|Û±|Û²|Û³|Û´|Ûµ|Û¶|Û·|Û¸|Û¹|Û°|\d+)\s*Ù…Ø§Ù‡\s*(Ø§Ø®ÛŒØ±|Ú¯Ø°Ø´ØªÙ‡)'
                month_match = re.search(month_pattern, query_lower)
                if month_match:
                    number_text = month_match.group(1)
                    if number_text in persian_numerals:
                        months = persian_numerals[number_text]
                    else:
                        try:
                            months = int(number_text)
                        except ValueError:
                            months = 1  # Default fallback
                    
                    days = months * 30
                    logger.info(f" Parsed Persian numeral pattern: {number_text} Ù…Ø§Ù‡ -> {days} days")
                    return {"days": days, "granularity": "day"}
                
                # Pattern for "X Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±/Ú¯Ø°Ø´ØªÙ‡"
                hour_pattern = r'\b(ÛŒÚ©|Ø¯Ùˆ|Ø³Ù‡|Ú†Ù‡Ø§Ø±|Ù¾Ù†Ø¬|Ø´Ø´|Ù‡ÙØª|Ù‡Ø´Øª|Ù†Ù‡|Ø¯Ù‡|Û±|Û²|Û³|Û´|Ûµ|Û¶|Û·|Û¸|Û¹|Û°|\d+)\s*Ø³Ø§Ø¹Øª\s*(Ø§Ø®ÛŒØ±|Ú¯Ø°Ø´ØªÙ‡)'
                hour_match = re.search(hour_pattern, query_lower)
                if hour_match:
                    number_text = hour_match.group(1)
                    if number_text in persian_numerals:
                        hours = persian_numerals[number_text]
                    else:
                        try:
                            hours = int(number_text)
                        except ValueError:
                            hours = 1  # Default fallback
                    
                    # Use hour granularity for hour queries
                    granularity = "hour"
                    logger.info(f" Parsed Persian numeral pattern: {number_text} Ø³Ø§Ø¹Øª -> {hours} hours, granularity: {granularity}")
                    return {"hours": hours, "granularity": granularity}
                
                # Pattern for "X Ø¯Ù‚ÛŒÙ‚Ù‡ Ø§Ø®ÛŒØ±/Ú¯Ø°Ø´ØªÙ‡"
                minute_pattern = r'\b(ÛŒÚ©|Ø¯Ùˆ|Ø³Ù‡|Ú†Ù‡Ø§Ø±|Ù¾Ù†Ø¬|Ø´Ø´|Ù‡ÙØª|Ù‡Ø´Øª|Ù†Ù‡|Ø¯Ù‡|Ù¾Ø§Ù†Ø²Ø¯Ù‡|Ø¨ÛŒØ³Øª|Ø³ÛŒ|Ú†Ù‡Ù„|Ù¾Ù†Ø¬Ø§Ù‡|Ø´ØµØª|Û±|Û²|Û³|Û´|Ûµ|Û¶|Û·|Û¸|Û¹|Û°|\d+)\s*Ø¯Ù‚ÛŒÙ‚Ù‡\s*(Ø§Ø®ÛŒØ±|Ú¯Ø°Ø´ØªÙ‡)'
                minute_match = re.search(minute_pattern, query_lower)
                if minute_match:
                    number_text = minute_match.group(1)
                    if number_text in persian_numerals:
                        minutes = persian_numerals[number_text]
                    else:
                        try:
                            minutes = int(number_text)
                        except ValueError:
                            minutes = 30  # Default fallback
                    
                    # Use minute granularity for minute queries
                    granularity = "minute"
                    logger.info(f" Parsed Persian numeral pattern: {number_text} Ø¯Ù‚ÛŒÙ‚Ù‡ -> {minutes} minutes, granularity: {granularity}")
                    return {"minutes": minutes, "granularity": granularity}
            
            # English time patterns with granularity
            english_patterns = {
                # Hours - FIXED: Use hour granularity for hour queries
                "one hour": {"hours": 1, "granularity": "hour"},
                "two hours": {"hours": 2, "granularity": "hour"},
                "three hours": {"hours": 3, "granularity": "hour"},
                "four hours": {"hours": 4, "granularity": "hour"},
                "five hours": {"hours": 5, "granularity": "hour"},
                "six hours": {"hours": 6, "granularity": "hour"},
                "seven hours": {"hours": 7, "granularity": "hour"},
                "eight hours": {"hours": 8, "granularity": "hour"},
                "nine hours": {"hours": 9, "granularity": "hour"},
                "ten hours": {"hours": 10, "granularity": "hour"},
                "twelve hours": {"hours": 12, "granularity": "hour"},
                "twenty four hours": {"hours": 24, "granularity": "hour"},
                # Days - FIXED: Use day granularity for day queries
                "one day": {"days": 1, "granularity": "day"},
                "two days": {"days": 2, "granularity": "day"},
                "three days": {"days": 3, "granularity": "day"},
                "four days": {"days": 4, "granularity": "day"},
                "five days": {"days": 5, "granularity": "day"},
                "six days": {"days": 6, "granularity": "day"},
                "seven days": {"days": 7, "granularity": "day"},
                # Weeks
                "one week": {"days": 7, "granularity": "week"},
                "two weeks": {"days": 14, "granularity": "week"},
                "three weeks": {"days": 21, "granularity": "week"},
                # Enhanced "ago" patterns
                "one hour ago": {"hours": 1, "granularity": "hour", "offset": -1},
                "two hours ago": {"hours": 2, "granularity": "hour", "offset": -2},
                "three hours ago": {"hours": 3, "granularity": "hour", "offset": -3},
                "four hours ago": {"hours": 4, "granularity": "hour", "offset": -4},
                "five hours ago": {"hours": 5, "granularity": "hour", "offset": -5},
                "six hours ago": {"hours": 6, "granularity": "hour", "offset": -6},
                "one day ago": {"days": 1, "granularity": "day", "offset": -1},
                "two days ago": {"days": 2, "granularity": "day", "offset": -2},
                "three days ago": {"days": 3, "granularity": "day", "offset": -3},
                "one week ago": {"days": 7, "granularity": "week", "offset": -7},
                "two weeks ago": {"days": 14, "granularity": "week", "offset": -14},
                "three weeks ago": {"days": 21, "granularity": "week", "offset": -21},
                # Enhanced "last" patterns
                "last week": {"days": 7, "granularity": "week", "offset": -7},
                "last month": {"days": 30, "granularity": "day", "offset": -30},
                "last day": {"days": 1, "granularity": "day", "offset": -1},
                "last hour": {"hours": 1, "granularity": "hour", "offset": -1},
                # Enhanced "recent" patterns
                "recently": {"days": 1, "granularity": "hour"},
                "recent": {"days": 3, "granularity": "day"},
                "past": {"days": 3, "granularity": "day"},
                # Months
                "one month": {"days": 30, "granularity": "day"},
                "two months": {"days": 60, "granularity": "day"},
                "three months": {"days": 90, "granularity": "day"},
                # Minutes
                "ten minutes": {"minutes": 10, "granularity": "minute"},
                "twenty minutes": {"minutes": 20, "granularity": "minute"},
                "thirty minutes": {"minutes": 30, "granularity": "minute"},
                "forty minutes": {"minutes": 40, "granularity": "minute"},
                "fifty minutes": {"minutes": 50, "granularity": "minute"},
                # Special cases
                "today": {"days": 1, "granularity": "hour"},
                "yesterday": {"days": 1, "granularity": "hour", "offset": -1},
                "last week": {"days": 7, "granularity": "week", "offset": -7},
                "past week": {"days": 7, "granularity": "week"},
                "last month": {"days": 30, "granularity": "day", "offset": -30},
                "past month": {"days": 30, "granularity": "day"},
                "recently": {"days": 1, "granularity": "hour"},
                "past": {"days": 3, "granularity": "day"}
            }
            
            for pattern, config in english_patterns.items():
                if pattern in query_lower:
                    logger.info(f" Parsed English time pattern: {pattern} -> {config}")
                    return config
            
            # Check for generic "last" patterns with better context detection
            if "last" in query_lower or "past" in query_lower:
                # Check for week context
                if any(word in query_lower for word in ["week", "weeks"]):
                    logger.info(f" Generic 'last week' pattern detected, using 7 days")
                    return {"days": 7, "granularity": "week"}
                # Check for month context
                elif any(word in query_lower for word in ["month", "months"]):
                    logger.info(f" Generic 'last month' pattern detected, using 30 days")
                    return {"days": 30, "granularity": "day"}
                # Check for day context
                elif any(word in query_lower for word in ["day", "days"]):
                    logger.info(f" Generic 'last day' pattern detected, using 1 day")
                    return {"days": 1, "granularity": "day"}
                # Check for hour context - extract specific number of hours
                elif any(word in query_lower for word in ["hour", "hours"]):
                    # Try to extract specific number of hours
                    import re
                    hour_match = re.search(r'(\d+)\s*hours?', query_lower)
                    if hour_match:
                        hours = int(hour_match.group(1))
                        logger.info(f" Generic 'last {hours} hours' pattern detected, using {hours} hours")
                        return {"hours": hours, "granularity": "hour"}
                    else:
                         logger.info(f" Generic 'last hour' pattern detected, using 1 hour")
                    return {"hours": 1, "granularity": "hour"}
                # Default fallback
                else:
                    logger.info(f" Generic 'last' pattern detected, defaulting to 3 days")
                    return {"days": 3, "granularity": "day"}
    
    def _llm_refine_sql(self, original_query: str, generated_sql: str, execution_summary: str, ontology_snippet: str, schema: str) -> Dict[str, Any]:
        """Use LLM to refine SQL query when execution returns empty results"""
        try:
            prompt = f"""You are an expert SQL query optimizer for an agriculture sensor database.

DATABASE SCHEMA:
{schema}

ONTOLOGY CONTEXT:
{ontology_snippet}

ORIGINAL USER QUERY: {original_query}
GENERATED SQL: {generated_sql}
EXECUTION RESULT: {execution_summary}

The SQL query returned empty results. Please analyze and provide a corrected SQL query or suggest alternative sensor types.

RULES:
1. Only provide SELECT queries
2. Use sensor_data table only
3. Prefer SQL correction over sensor suggestions
4. Ensure proper time filtering
5. Use appropriate aggregation for time-based queries

Respond in JSON format:
{{
    "action": "correct_sql" | "suggest_sensors",
    "corrected_sql": "SELECT ... FROM sensor_data WHERE ...",
    "suggested_sensors": ["sensor1", "sensor2"],
    "reasoning": "Explanation of the correction",
    "confidence": 0.0-1.0
}}

If correcting SQL, provide the corrected_sql field.
If suggesting sensors, provide the suggested_sensors field.
Always provide reasoning and confidence score."""

            # Log LLM call with truncated prompt
            truncated_prompt = prompt[:200] + "..." if len(prompt) > 200 else prompt
            logger.info(f"ğŸ¤– LLM Call - SQL Refinement (Prompt): {truncated_prompt}")
            
            response = self.llm.invoke(prompt)
            response_text = response.content.strip()
            
            # Log LLM response (truncated)
            truncated_response = response_text[:100] + "..." if len(response_text) > 100 else response_text
            logger.info(f"ğŸ¤– LLM Call - SQL Refinement (Response): {truncated_response}")
            
            # Parse JSON response
            import json
            try:
                result = json.loads(response_text)
                logger.info(f"ğŸ¤– LLM SQL refinement result: {result}")
                return result
            except json.JSONDecodeError:
                logger.error(f" Failed to parse LLM refinement response as JSON: {response_text}")
                return {
                    "action": "suggest_sensors",
                    "suggested_sensors": ["temperature", "humidity"],
                    "reasoning": "Failed to parse LLM response",
                    "confidence": 0.1
                }
                
        except Exception as e:
            logger.error(f" Error in LLM SQL refinement: {str(e)}")
            return {
                "action": "suggest_sensors", 
                "suggested_sensors": ["temperature"],
                "reasoning": f"LLM refinement error: {str(e)}",
                "confidence": 0.1
            }
    
    def _generate_time_aware_sql(self, sensor_type: str, time_config: Dict[str, Any], english_query: str) -> str:
        """Generate time-aware SQL with proper aggregation and grouping"""
        try:
            granularity = time_config.get("granularity", "day")
            offset = time_config.get("offset", 0)
            
            # Build time window condition
            time_condition = self._build_time_window_condition(time_config, offset)
            
            # Build GROUP BY clause based on granularity
            group_by_clause = self._build_group_by_clause(granularity)
            
            # Build SELECT clause with aggregations
            select_clause = self._build_aggregation_select(granularity)
            
            # Generate the complete SQL
            sql = f"""
            SELECT {select_clause}
            FROM sensor_data 
            WHERE sensor_type = '{sensor_type}' 
            AND {time_condition}
            GROUP BY {group_by_clause}
            ORDER BY {group_by_clause} ASC
            """
            
            # Clean up the SQL (remove extra whitespace)
            sql = ' '.join(sql.split())
            
            logger.info(f" Generated time-aware SQL: {sql}")
            return sql
            
        except Exception as e:
            logger.error(f" Error generating time-aware SQL: {e}")
            return f"SELECT * FROM sensor_data WHERE sensor_type = '{sensor_type}' ORDER BY timestamp DESC LIMIT 10"
    
    def _build_time_window_condition(self, time_config: Dict[str, Any], offset: int = 0) -> str:
        """Build SQL time window condition"""
        try:
            if "days" in time_config:
                days = time_config["days"] + offset
                if offset < 0:
                    # For past periods (yesterday, last week)
                    return f"timestamp >= datetime('now', '-{abs(days)} days') AND timestamp < datetime('now', '-{abs(offset)} days')"
                else:
                    return f"timestamp >= datetime('now', '-{days} days')"
            
            elif "hours" in time_config:
                hours = time_config["hours"] + offset
                if offset < 0:
                    return f"timestamp >= datetime('now', '-{abs(hours)} hours') AND timestamp < datetime('now', '-{abs(offset)} hours')"
                else:
                    return f"timestamp >= datetime('now', '-{hours} hours')"
            
            elif "minutes" in time_config:
                minutes = time_config["minutes"] + offset
                if offset < 0:
                    return f"timestamp >= datetime('now', '-{abs(minutes)} minutes') AND timestamp < datetime('now', '-{abs(offset)} minutes')"
                else:
                    return f"timestamp >= datetime('now', '-{minutes} minutes')"
            
            else:
                # Default to last 24 hours
                return "timestamp >= datetime('now', '-1 days')"
                
        except Exception as e:
            logger.error(f" Error building time window: {e}")
            return "timestamp >= datetime('now', '-1 days')"
    
    def _build_group_by_clause(self, granularity: str) -> str:
        """Build GROUP BY clause based on time granularity"""
        try:
            if granularity == "day":
                return "DATE(timestamp)"
            elif granularity == "hour":
                return "strftime('%Y-%m-%d %H:00', timestamp)"
            elif granularity == "minute":
                return "strftime('%Y-%m-%d %H:%M', timestamp)"
            elif granularity == "week":
                # Group by week: use strftime to get week number
                return "strftime('%Y-%W', timestamp)"
            else:
                return "DATE(timestamp)"
                
        except Exception as e:
            logger.error(f" Error building GROUP BY clause: {e}")
            return "DATE(timestamp)"
    
    def _build_aggregation_select(self, granularity: str) -> str:
        """Build SELECT clause with proper aggregations"""
        try:
            if granularity == "day":
                return "DATE(timestamp) as time_period, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
            elif granularity == "hour":
                return "strftime('%Y-%m-%d %H:00', timestamp) as time_period, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
            elif granularity == "minute":
                return "strftime('%Y-%m-%d %H:%M', timestamp) as time_period, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
            elif granularity == "week":
                return "strftime('%Y-%W', timestamp) as time_period, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
            else:
                return "DATE(timestamp) as time_period, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
                
        except Exception as e:
            logger.error(f" Error building aggregation SELECT: {e}")
            return "DATE(timestamp) as time_period, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
    
    def _generate_compound_time_aware_sql(self, sensor_types: List[str], time_config: Dict[str, Any], english_query: str) -> str:
        """Generate time-aware SQL for compound queries with multiple sensor types"""
        try:
            granularity = time_config.get("granularity", "day")
            offset = time_config.get("offset", 0)
            
            # Create IN clause for multiple sensor types
            sensor_list = "', '".join(sensor_types)
            
            # Build time window condition
            time_condition = self._build_time_window_condition(time_config, offset)
            
            # Build GROUP BY clause based on granularity
            group_by_clause = self._build_group_by_clause(granularity)
            
            # Build SELECT clause with aggregations for compound queries
            if granularity == "day":
                select_clause = f"DATE(timestamp) as time_period, sensor_type, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
                group_by_clause = f"DATE(timestamp), sensor_type"
            elif granularity == "hour":
                select_clause = f"strftime('%Y-%m-%d %H:00', timestamp) as time_period, sensor_type, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
                group_by_clause = f"strftime('%Y-%m-%d %H:00', timestamp), sensor_type"
            elif granularity == "minute":
                select_clause = f"strftime('%Y-%m-%d %H:%M', timestamp) as time_period, sensor_type, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
                group_by_clause = f"strftime('%Y-%m-%d %H:%M', timestamp), sensor_type"
            elif granularity == "week":
                select_clause = f"strftime('%Y-%W', timestamp) as time_period, sensor_type, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
                group_by_clause = f"strftime('%Y-%W', timestamp), sensor_type"
            else:
                select_clause = f"DATE(timestamp) as time_period, sensor_type, AVG(value) as avg_value, MIN(value) as min_value, MAX(value) as max_value, COUNT(*) as data_points"
                group_by_clause = f"DATE(timestamp), sensor_type"
            
            # Generate the complete SQL
            sql = f"""
            SELECT {select_clause}
            FROM sensor_data 
            WHERE sensor_type IN ('{sensor_list}') 
            AND {time_condition}
            GROUP BY {group_by_clause}
            ORDER BY {group_by_clause} ASC
            """
            
            # Clean up the SQL (remove extra whitespace)
            sql = ' '.join(sql.split())
            
            logger.info(f" Generated compound time-aware SQL: {sql}")
            return sql
            
        except Exception as e:
            logger.error(f" Error generating compound time-aware SQL: {e}")
            sensor_list = "', '".join(sensor_types)
            return f"SELECT * FROM sensor_data WHERE sensor_type IN ('{sensor_list}') ORDER BY timestamp DESC LIMIT 10"
    
    def _generate_compound_sql(self, sensor_types: List[str], english_query: str, language: str = "en") -> str:
        """Generate SQL for compound queries with multiple sensor types"""
        try:
            logger.info(f" Generating compound SQL for sensors: {sensor_types}")
            
            # Create IN clause for multiple sensor types
            sensor_list = "', '".join(sensor_types)
            
            # Determine query pattern
            query_patterns = self.ontology.get("query_patterns", {})
            query_lower = english_query.lower()
            
            # Check for time-based queries using new time-aware system
            time_config = self._parse_time_expression(english_query, language)
            if time_config:
                sql = self._generate_compound_time_aware_sql(sensor_types, time_config, english_query)
                logger.info(f" Generated compound time-aware SQL: {sql}")
                return sql
            
            # Check for current/latest value
            current_indicators = query_patterns.get("current_value", {}).get("persian", []) + \
                               query_patterns.get("current_value", {}).get("english", [])
            if any(indicator in query_lower for indicator in current_indicators):
                sql = f"SELECT * FROM sensor_data WHERE sensor_type IN ('{sensor_list}') ORDER BY timestamp DESC LIMIT {len(sensor_types)}"
                logger.info(f" Generated compound current value SQL: {sql}")
                return sql
            
            # Check for average value
            avg_indicators = query_patterns.get("average_value", {}).get("persian", []) + \
                           query_patterns.get("average_value", {}).get("english", [])
            if any(indicator in query_lower for indicator in avg_indicators):
                sql = f"SELECT sensor_type, AVG(value) as avg_value FROM sensor_data WHERE sensor_type IN ('{sensor_list}') GROUP BY sensor_type"
                logger.info(f" Generated compound average value SQL: {sql}")
                return sql
            
            # Check for trend analysis
            trend_indicators = query_patterns.get("trend_analysis", {}).get("persian", []) + \
                             query_patterns.get("trend_analysis", {}).get("english", [])
            if any(indicator in query_lower for indicator in trend_indicators):
                sql = f"SELECT timestamp, sensor_type, value FROM sensor_data WHERE sensor_type IN ('{sensor_list}') ORDER BY timestamp DESC LIMIT 20"
                logger.info(f" Generated compound trend analysis SQL: {sql}")
                return sql
            
            # Default: get latest values for all sensors
            sql = f"SELECT * FROM sensor_data WHERE sensor_type IN ('{sensor_list}') ORDER BY timestamp DESC LIMIT {len(sensor_types)}"
            logger.info(f" Generated compound default SQL: {sql}")
            return sql
            
        except Exception as e:
            logger.error(f" Error in compound SQL generation: {e}")
            # Fallback to simple compound query
            sensor_list = "', '".join(sensor_types)
            return f"SELECT * FROM sensor_data WHERE sensor_type IN ('{sensor_list}') ORDER BY timestamp DESC LIMIT 10"

    def _generate_simple_sql(self, english_query: str) -> str:
        """Generate simple SQL based on query content"""
        query_lower = english_query.lower()
        
        # Check for dangerous queries first (more specific to avoid false positives)
        dangerous_keywords = ['drop table', 'delete from', 'update set', 'insert into', 'alter table', 'create table', 'truncate table', 'remove table', 'clear table']
        for keyword in dangerous_keywords:
            if keyword in query_lower:
                logger.warning(f"  Dangerous query detected in SQL generation: {keyword}")
                # Return the dangerous query as-is so validation can catch it
                return english_query
        
        # Generate safe SQL for normal queries
        if any(word in query_lower for word in ['temperature', 'temp']):
            return "SELECT timestamp, value FROM sensor_data WHERE sensor_type = 'temperature' ORDER BY timestamp DESC LIMIT 10"
        elif any(word in query_lower for word in ['humidity', 'moisture']):
            return "SELECT timestamp, value FROM sensor_data WHERE sensor_type = 'humidity' ORDER BY timestamp DESC LIMIT 10"
        elif any(word in query_lower for word in ['irrigation', 'water', 'soil']):
            return "SELECT timestamp, value FROM sensor_data WHERE sensor_type IN ('soil_moisture', 'water_usage') ORDER BY timestamp DESC LIMIT 10"
        elif any(word in query_lower for word in ['pest', 'disease']):
            return "SELECT timestamp, value FROM sensor_data WHERE sensor_type IN ('pest_count', 'disease_risk') ORDER BY timestamp DESC LIMIT 10"
        elif any(word in query_lower for word in ['co2', 'carbon']):
            return "SELECT timestamp, value FROM sensor_data WHERE sensor_type = 'co2_level' ORDER BY timestamp DESC LIMIT 10"
        elif any(word in query_lower for word in ['light', 'luminosity']):
            return "SELECT timestamp, value FROM sensor_data WHERE sensor_type = 'light' ORDER BY timestamp DESC LIMIT 10"
        else:
            return "SELECT timestamp, sensor_type, value FROM sensor_data ORDER BY timestamp DESC LIMIT 10"
    
    def _execute_direct_sql(self, sql_query: str) -> Dict[str, Any]:
        """Execute SQL query directly and return results with validation"""
        try:
            import sqlite3
            
            # Step 1: Validate SQL query before execution
            validation_result = self._validate_sql_query(sql_query)
            if not validation_result["valid"]:
                logger.error(f" SQL Validation Failed: {validation_result['message']}")
                return {
                    "success": False,
                    "error": validation_result["message"],
                    "data": [],
                    "validation": validation_result
                }
            
            logger.info(f" SQL Validation Passed: {validation_result['message']}")
            
            # Step 2: Connect to database
            # Use proper database path for Liara
            if os.getenv("LIARA_APP_ID"):
                db_dir = "/var/lib/data"
                os.makedirs(db_dir, exist_ok=True)
                db_path = os.path.join(db_dir, "smart_dashboard.db")
            else:
                db_path = "smart_dashboard.db"
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # Step 3: Execute the query with error handling and query logging
            try:
                # Log the query being executed
                logger.info(f" Executing SQL Query: {sql_query}")
                
                cursor.execute(sql_query)
                results = cursor.fetchall()
                
                # Step 4: Get column names
                columns = [description[0] for description in cursor.description]
                
                # Step 5: Convert to list of dictionaries
                data = []
                for row in results:
                    row_dict = {}
                    for i, value in enumerate(row):
                        row_dict[columns[i]] = value
                    data.append(row_dict)
                
                logger.info(f" SQL Execution Success: {len(data)} rows returned")
                logger.info(f" Query Columns: {columns}")
                logger.info(f" Sample Data: {data[:3] if data else 'No data'}")
                
                return {
                    "success": True,
                    "data": data,
                    "validation": validation_result,
                    "columns": columns,
                    "query_executed": sql_query
                }
                
            except sqlite3.Error as e:
                logger.error(f" SQL Execution Error: {str(e)}")
                logger.error(f" Failed Query: {sql_query}")
                return {
                    "success": False,
                    "error": f"SQL execution failed: {str(e)}",
                    "data": [],
                    "validation": {"valid": False, "message": f"Execution error: {str(e)}"},
                    "query_executed": sql_query
                }
            finally:
                conn.close()
            
        except Exception as e:
            logger.error(f" Database Connection Error: {str(e)}")
            return {
                "success": False,
                "error": f"Database connection failed: {str(e)}",
                "data": [],
                "validation": {"valid": False, "message": f"Connection error: {str(e)}"}
            }
    
    def _validate_sql_query(self, sql_query: str) -> Dict[str, Any]:
        """Validate SQL query for safety and correctness"""
        try:
            sql_lower = sql_query.lower().strip()
            
            # Check 1: Only SELECT queries allowed
            if not sql_lower.startswith('select'):
                return {
                    "valid": False,
                    "message": "Only SELECT queries are allowed"
                }
            
            # Check 2: No dangerous operations
            dangerous_keywords = ['drop table', 'delete from', 'insert into', 'update set', 'alter table', 'create table', 'truncate table']
            for keyword in dangerous_keywords:
                if keyword in sql_lower:
                    return {
                        "valid": False,
                        "message": f"Dangerous operation '{keyword}' not allowed"
                    }
            
            # Check 3: Must use sensor_data table
            if 'sensor_data' not in sql_lower:
                return {
                    "valid": False,
                    "message": "Query must use sensor_data table"
                }
            
            # Check 4: Check for valid sensor types
            valid_sensor_types = [
                'temperature', 'humidity', 'co2_level', 'light', 'pressure',
                'soil_moisture', 'water_usage', 'pest_count', 'disease_risk',
                'rainfall', 'wind_speed', 'energy_usage', 'yield_prediction',
                'soil_temperature', 'leaf_wetness', 'motion', 'fruit_count',
                'fruit_size', 'plant_height', 'nutrient_uptake', 'fertilizer_usage',
                'soil_ph', 'nitrogen_level', 'phosphorus_level', 'potassium_level',
                'pest_detection', 'water_efficiency', 'yield_efficiency',
                'tomato_price', 'lettuce_price', 'pepper_price',
                'co2', 'cost_per_kg', 'demand_level', 'leaf_count',
                'profit_margin', 'supply_level', 'test_temperature'
            ]
            
            # Check if query references valid sensor types (only if sensor_type is explicitly mentioned)
            if 'sensor_type' in sql_lower:
                print(f" SQL Query contains sensor_type: {sql_lower}")
            has_valid_sensor_type = False
            for sensor_type in valid_sensor_types:
                # Check for sensor type with quotes (both single and double)
                if f"'{sensor_type}'" in sql_lower or f'"{sensor_type}"' in sql_lower or sensor_type in sql_lower:
                    has_valid_sensor_type = True
                    print(f" Found valid sensor type: {sensor_type}")
                    break
            
            if not has_valid_sensor_type:
                print(f" Query does not reference valid sensor types: {sql_lower}")
                return {
                    "valid": False,
                    "message": "Query must reference valid sensor types"
                }
            
            # Check 5: Basic SQL syntax check
            if not ('from' in sql_lower and 'select' in sql_lower):
                return {
                    "valid": False,
                    "message": "Invalid SQL syntax"
                }
            
            return {
                "valid": True,
                "message": "Query validation passed",
                "sensor_types_found": [st for st in valid_sensor_types if st in sql_lower],
                "query_type": "SELECT",
                "table_used": "sensor_data"
            }
            
        except Exception as e:
            return {
                "valid": False,
                "message": f"Validation error: {str(e)}"
            }
    
    def _extract_sql_from_response(self, agent_response) -> str:
        """Extract SQL query from agent response"""
        try:
            if hasattr(agent_response, 'output'):
                output = agent_response.output
                # Look for SQL in the output - the agent shows the query in the logs
                if 'SELECT' in output.upper():
                    lines = output.split('\n')
                    for line in lines:
                        if 'SELECT' in line.upper() and 'FROM' in line.upper():
                            # Clean up the SQL query
                            sql = line.strip()
                            if sql.startswith('```sql'):
                                sql = sql.replace('```sql', '').replace('```', '').strip()
                            return sql
            return "SELECT * FROM sensor_data LIMIT 5"
        except:
            return "SELECT * FROM sensor_data LIMIT 5"
    
    def _extract_data_from_response(self, agent_response) -> List[Dict[str, Any]]:
        """Extract data from agent response"""
        try:
            if hasattr(agent_response, 'output'):
                output = agent_response.output
                # The agent response contains the actual data in the logs
                # Look for data patterns like: [('2025-09-20 07:28:27', 18.11), ...]
                import re
                
                # Find data tuples in the output
                data_pattern = r"\[(\([^)]+\),?\s*)+]"
                matches = re.findall(data_pattern, output)
                
                if matches:
                    # Parse the first match
                    data_str = matches[0]
                    # Convert string representation to actual data
                    try:
                        # This is a simplified parser - in production you'd want more robust parsing
                        data_str = data_str.replace("'", '"')  # Convert single quotes to double quotes
                        data_list = eval(data_str)  # Convert string to list of tuples
                        
                        # Convert tuples to dictionaries
                        result = []
                        for item in data_list:
                            if len(item) >= 2:
                                result.append({
                                    "timestamp": item[0],
                                    "value": item[1]
                                })
                        return result
                    except:
                        pass
                
                # Fallback: try to extract from the structured output
                if 'Final Answer:' in output:
                    # Extract data from the final answer section
                    lines = output.split('\n')
                    result = []
                    for line in lines:
                        if ' - ' in line and any(char.isdigit() for char in line):
                            parts = line.split(' - ')
                            if len(parts) >= 2:
                                timestamp = parts[0].strip()
                                value = parts[1].strip()
                                try:
                                    result.append({
                                        "timestamp": timestamp,
                                        "value": float(value)
                                    })
                                except:
                                    pass
                    return result
                    
            return []
        except Exception as e:
            logger.error(f"Data extraction error: {str(e)}")
            return []
    
    def _generate_mock_sql_result(self, english_query: str) -> Dict[str, Any]:
        """Generate mock SQL result for testing"""
        # Mock SQL based on query content
        if any(word in english_query.lower() for word in ['irrigation', 'water', 'soil']):
            sql = "SELECT * FROM sensor_data WHERE sensor_type IN ('soil_moisture', 'water_usage') ORDER BY timestamp DESC LIMIT 5"
            raw_data = [
                {"timestamp": "2025-09-20T04:44:08", "sensor_type": "soil_moisture", "value": 45.2},
                {"timestamp": "2025-09-20T04:44:05", "sensor_type": "water_usage", "value": 125.8}
            ]
        elif any(word in english_query.lower() for word in ['pest', 'disease']):
            sql = "SELECT * FROM sensor_data WHERE sensor_type IN ('pest_count', 'disease_risk') ORDER BY timestamp DESC LIMIT 5"
            raw_data = [
                {"timestamp": "2025-09-20T04:44:10", "sensor_type": "pest_count", "value": 2.1},
                {"timestamp": "2025-09-20T04:44:07", "sensor_type": "disease_risk", "value": 15.3}
            ]
        else:
            sql = "SELECT * FROM sensor_data ORDER BY timestamp DESC LIMIT 5"
            raw_data = [
                {"timestamp": "2025-09-20T04:44:17", "sensor_type": "temperature", "value": 18.8},
                {"timestamp": "2025-09-20T04:44:15", "sensor_type": "humidity", "value": 86.65}
            ]
        
        return {
            "sql": sql,
            "raw_data": raw_data,
            "success": True
        }
    
    def _format_structured_response(self, sql_result: Dict[str, Any], english_query: str, feature_context: str, intent: str = "data_query", original_query: str = None, detected_lang: str = "en", time_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Format result into unified structured JSON with consistent schema"""
        # Initialize base response structure
        base_response = {
            "success": True,
                    "summary": "",
                    "metrics": {},
                    "raw_data": [],
            "chart": None,
            "chart_type": None,
            "chart_metadata": None,
            "comparison": None,
            "sql": "",
                    "translated_query": english_query,
                    "feature_context": feature_context,
                    "timestamp": datetime.utcnow().isoformat(),
            "validation": {
                "query_valid": True,
                "execution_success": True,
                "data_points": 0,
                "sensor_types": [],
                "chart_requested": False,
                "mapping": {},
                "refined_by_llm": False,
                "semantic_json": {}
            }
        }
        
        try:
            raw_data = sql_result.get("raw_data", [])
            sql_query = sql_result.get("sql", "")
            validation_result = sql_result.get("validation", {})
            
            # Check if validation failed
            if not validation_result.get("valid", True):
                logger.error(f" Validation failed: {validation_result.get('message', 'Unknown validation error')}")
                return self._create_error_response(
                    base_response=base_response,
                    error_message=validation_result.get("message", "Query validation failed"),
                    detected_lang=detected_lang,
                    sql_query=sql_query,
                    validation_result=validation_result,
                    sql_result=sql_result
                )
            
            # Detect chart request using original query and detected language
            chart_query = original_query if original_query else english_query
            chart_request = self._detect_chart_request(chart_query, detected_lang)
            
            # Extract metrics from raw data FIRST
            metrics = self._extract_metrics(raw_data)
            
            # Check if this is a comparison query
            semantic_json = sql_result.get("semantic_json", {})
            is_comparison = semantic_json.get("comparison", False)
            time_ranges = semantic_json.get("time_range", [])
            
            # Process comparison data if needed
            comparison_data = None
            if is_comparison and isinstance(time_ranges, list) and len(time_ranges) > 1:
                comparison_data = self._process_comparison_data(raw_data, time_ranges)
            
            # Generate summary using LLM with metrics and comparison data and chart info
            summary = self._generate_summary(english_query, raw_data, feature_context, intent, metrics, comparison_data, chart_request, time_context)
            
            # Process chart data if requested
            chart_data = None
            chart_metadata = None
            if chart_request["wants_chart"]:
                chart_data = self._process_chart_data(raw_data, chart_request["chart_type"])
                logger.info(f" Chart requested: {chart_request['chart_type']} - {chart_request['matched_term']}")
                
                # Generate explicit chart metadata
                chart_metadata = self._generate_chart_metadata(
                    chart_type=chart_request["chart_type"],
                    data=raw_data,
                    semantic_json=semantic_json,
                    matched_term=chart_request.get("matched_term", ""),
                    entity=semantic_json.get("entity", "unknown")
                )
            
            # Update base response with actual data
            base_response.update({
                "success": True,
                "response": summary,  # CRITICAL FIX: Add "response" field for frontend compatibility
                "summary": summary,
                "data": raw_data,  # CRITICAL FIX: Add "data" field for frontend compatibility
                "metrics": metrics,
                "raw_data": raw_data,
                "chart": chart_data,
                "chart_type": chart_request["chart_type"] if chart_request["wants_chart"] else None,
                "chart_metadata": chart_metadata,
                "comparison": comparison_data,
                "sql": sql_query,
                "validation": {
                    "query_valid": True,
                    "execution_success": True,
                    "data_points": len(raw_data),
                    "sensor_types": list(set([item.get('sensor_type', 'unknown') for item in raw_data if 'sensor_type' in item])),
                    "chart_requested": chart_request["wants_chart"],
                    "mapping": sql_result.get("mapping", {}),
                    "refined_by_llm": sql_result.get("refined_by_llm", False),
                    "semantic_json": semantic_json
                }
            })
            
            return base_response
            
        except Exception as e:
            logger.error(f" Response Formatting Error: {str(e)}")
            return self._create_error_response(
                base_response=base_response,
                error_message=f"Response formatting error: {str(e)}",
                detected_lang=detected_lang,
                sql_query=sql_result.get("sql", ""),
                validation_result={"valid": False, "message": str(e)},
                sql_result=sql_result
            )
    
    def _generate_chart_metadata(self, chart_type: str, data: List[Dict[str, Any]], semantic_json: Dict[str, Any], matched_term: str = "", entity: str = "unknown") -> Dict[str, Any]:
        """Generate explicit chart metadata configuration"""
        try:
            # Extract sensor types from data
            sensor_types = list(set([item.get('sensor_type', 'unknown') for item in data if 'sensor_type' in item]))
            
            # Determine axis labels
            entity_str = entity if isinstance(entity, str) else entity[0] if isinstance(entity, list) and entity else "Value"
            x_axis_label = "Time" if "time_period" in (data[0] if data else {}) else "Data Point"
            y_axis_label = entity_str.replace('_', ' ').title()
            
            # Get unit from ontology
            unit = self.ontology.get("sensors", {}).get(entity_str, {}).get("unit", "")
            
            # Determine chart title
            time_range = semantic_json.get("time_range", "")
            if isinstance(time_range, list):
                time_range_str = " vs ".join(time_range)
            else:
                time_range_str = time_range
            
            chart_title = f"{y_axis_label} - {time_range_str}".title() if time_range_str else y_axis_label
            
            # Chart configuration
            metadata = {
                "chart_type": chart_type,
                "title": chart_title,
                "x_axis": {
                    "label": x_axis_label,
                    "type": "category" if "time_period" in (data[0] if data else {}) else "linear"
                },
                "y_axis": {
                    "label": y_axis_label,
                    "unit": unit,
                    "type": "linear"
                },
                "legend": {
                    "show": len(sensor_types) > 1,
                    "position": "top"
                },
                "colors": self._get_chart_colors(sensor_types),
                "data_points": len(data),
                "sensor_types": sensor_types,
                "matched_term": matched_term,
                "grouping": semantic_json.get("grouping", "none"),
                "time_range": time_range,
                "comparison": semantic_json.get("comparison", False)
            }
            
            # Add chart-specific configurations
            if chart_type == "line":
                metadata["line_config"] = {
                    "smooth": True,
                    "tension": 0.4,
                    "point_radius": 4,
                    "border_width": 2
                }
            elif chart_type == "bar":
                metadata["bar_config"] = {
                    "bar_thickness": 30,
                    "border_width": 1
                }
            elif chart_type == "pie":
                metadata["pie_config"] = {
                    "start_angle": 0,
                    "border_width": 2
                }
            
            return metadata
            
        except Exception as e:
            logger.error(f" Chart metadata generation error: {str(e)}")
            return {
                "chart_type": chart_type,
                "title": "Chart",
                "error": str(e)
            }
    
    def _get_chart_colors(self, sensor_types: List[str]) -> Dict[str, str]:
        """Get appropriate colors for chart based on sensor types"""
        color_palette = {
            "temperature": "#FF6384",
            "humidity": "#36A2EB",
            "soil_moisture": "#8B4513",
            "water_usage": "#4BC0C0",
            "water_efficiency": "#00CED1",
            "light": "#FFCE56",
            "co2_level": "#9966FF",
            "pest_count": "#FF9F40",
            "pest_detection": "#FF6384",
            "disease_risk": "#C9CBCF",
            "plant_height": "#4CAF50",
            "fruit_size": "#FF5722",
            "yield": "#FFC107",
            "ph": "#E91E63",
            "nutrient_level": "#795548",
            "default": "#808080"
        }
        
        colors = {}
        for sensor in sensor_types:
            colors[sensor] = color_palette.get(sensor, color_palette["default"])
        
        return colors
    
    def _create_error_response(self, base_response: Dict[str, Any], error_message: str, detected_lang: str, sql_query: str = "", validation_result: Dict[str, Any] = None, sql_result: Dict[str, Any] = None) -> Dict[str, Any]:
        """Create unified error response with user-friendly messages in English and Persian"""
        try:
            # Create user-friendly error messages
            error_messages = self._get_user_friendly_error_messages(error_message, detected_lang)
            
            # Update base response for error case
            error_response = base_response.copy()
            error_response.update({
                "success": False,
                "error": error_messages["english"],
                "error_persian": error_messages["persian"],
                "summary": error_messages["summary"],
                "sql": sql_query,
                "validation": {
                    "query_valid": False,
                    "execution_success": False,
                    "data_points": 0,
                    "sensor_types": [],
                    "chart_requested": False,
                    "mapping": sql_result.get("mapping", {}) if sql_result else {},
                    "refined_by_llm": sql_result.get("refined_by_llm", False) if sql_result else False,
                    "semantic_json": sql_result.get("semantic_json", {}) if sql_result else {},
                    "error_details": validation_result or {}
                }
            })
            
            return error_response
            
        except Exception as e:
            logger.error(f" Error response creation failed: {str(e)}")
            # Fallback to basic error response
            return {
                "success": False,
                "error": "An unexpected error occurred",
                "error_persian": "Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡â€ŒØ§ÛŒ Ø±Ø® Ø¯Ø§Ø¯",
                "summary": "Error processing query",
                "metrics": {},
                "raw_data": [],
                "chart": None,
                "chart_type": None,
                "chart_metadata": None,
                "comparison": None,
                "sql": sql_query,
                "translated_query": "",
                "feature_context": "",
                "timestamp": datetime.utcnow().isoformat(),
                "validation": {
                    "query_valid": False,
                    "execution_success": False,
                    "data_points": 0,
                    "sensor_types": [],
                    "chart_requested": False,
                    "mapping": {},
                    "refined_by_llm": False,
                    "semantic_json": {},
                    "error_details": {"message": str(e)}
                }
            }
    
    def _get_user_friendly_error_messages(self, error_message: str, detected_lang: str) -> Dict[str, str]:
        """Generate user-friendly error messages in English and Persian"""
        error_mappings = {
            "validation": {
                "english": "Your query couldn't be processed. Please try rephrasing your question.",
                "persian": "Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ù‚Ø§Ø¨Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯.",
                "summary": "Query validation failed"
            },
            "execution": {
                "english": "No data available for your query. Please check:\n1. Sensor connectivity and power\n2. Data collection status\n3. Time range selection\n4. Sensor type availability",
                "persian": "Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯:\n1. Ø§ØªØµØ§Ù„ Ùˆ Ø¨Ø±Ù‚ Ø³Ù†Ø³ÙˆØ±Ù‡Ø§\n2. ÙˆØ¶Ø¹ÛŒØª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡\n3. Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ\n4. Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† Ù†ÙˆØ¹ Ø³Ù†Ø³ÙˆØ±",
                "summary": "No data available - sensor troubleshooting"
            },
            "formatting": {
                "english": "There was an issue processing your request. Please try again.",
                "persian": "Ù…Ø´Ú©Ù„ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.",
                "summary": "Request processing failed"
            },
            "default": {
                "english": "Something went wrong. Please try again or contact support.",
                "persian": "Ù…Ø´Ú©Ù„ÛŒ Ù¾ÛŒØ´ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ ØªÙ…Ø§Ø³ Ø¨Ú¯ÛŒØ±ÛŒØ¯.",
                "summary": "An error occurred"
            }
        }
        
        # Determine error type based on message content
        error_lower = error_message.lower()
        if "validation" in error_lower or "invalid" in error_lower:
            error_type = "validation"
        elif "execution" in error_lower or "sql" in error_lower or "database" in error_lower:
            error_type = "execution"
        elif "formatting" in error_lower or "response" in error_lower:
            error_type = "formatting"
        else:
            error_type = "default"
        
        return error_mappings[error_type]
    
    def _generate_summary(self, english_query: str, raw_data: List[Dict[str, Any]], feature_context: str, intent: str = "data_query", metrics: Dict[str, Any] = None, comparison_data: Dict[str, Any] = None, chart_request: Dict[str, Any] = None, time_context: Dict[str, Any] = None) -> str:
        """Generate summary using LLM with intent information, metrics, comparison data, and chart info"""
        try:
            if hasattr(self.llm, 'openai_api_key') and self.llm.openai_api_key:
                # Format metrics for LLM context
                metrics_text = ""
                if metrics:
                    if "aggregated" in metrics:
                        # Handle aggregated metrics (time-aware queries) with sensor type grouping
                        agg_metrics = metrics["aggregated"]
                        metrics_text = f"""
## AGGREGATED METRICS (Time-Aware Query):
- Sensor Types: {', '.join(agg_metrics.get('sensor_types', []))}
- Total Sensor Types: {agg_metrics.get('total_sensor_types', 0)}
- Total Data Points: {agg_metrics.get('total_data_points', 0)}
- Total Periods: {agg_metrics.get('total_periods', 0)}

## SENSOR-SPECIFIC METRICS:"""
                        
                        # Add metrics for each sensor type
                        for key, value in metrics.items():
                            if key not in ["aggregated"] and not key.endswith("_period_"):
                                sensor_type = key
                                sensor_metrics = value
                                metrics_text += f"""
### {sensor_type.replace('_', ' ').title()}:
- Average: {sensor_metrics.get('overall_average', 0)}
- Min: {sensor_metrics.get('overall_min', 0)}
- Max: {sensor_metrics.get('overall_max', 0)}
- Latest Period: {sensor_metrics.get('latest_period', 'unknown')}
- Latest Average: {sensor_metrics.get('latest_average', 0)}
- Data Points: {sensor_metrics.get('total_data_points', 0)}"""
                    else:
                        # Handle raw metrics (legacy)
                        metrics_text = f"\n## RAW METRICS:\n{json.dumps(metrics, indent=2)}"
                
                # Initialize variables
                time_periods_text = ""
                comparison_text = ""
                trend_analysis = ""
                
                # For time-based queries, show the actual time periods and values
                if raw_data and "avg_value" in raw_data[0]:
                    # This is aggregated data (time-aware query)
                    is_comparison = any(word in english_query.lower() for word in ["Ù…Ù‚Ø§ÛŒØ³Ù‡", "compare", "vs", "Ø¨Ø§", "versus"])
                    
                    # Check if this is time breakdown data
                    is_daily_breakdown = "day" in raw_data[0]
                    is_time_breakdown = "time_period" in raw_data[0]
                    
                    # Start building time periods text with header
                    time_periods_text = "\nğŸ“Š Sensor Data by Time Range:\n"
                    
                    # Add comparison data if available
                    if comparison_data and comparison_data.get("sensor_comparisons"):
                        comparison_text = "\n## COMPARISON ANALYSIS:\n"
                        for sensor_type, comparison in comparison_data["sensor_comparisons"].items():
                            if "delta" in comparison and "percent_change" in comparison:
                                comparison_text += f"- {sensor_type}: {comparison['delta']:.2f} change ({comparison['percent_change']:.1f}%)\n"
                    
                    # Calculate trend for time breakdown
                    if (is_daily_breakdown or is_time_breakdown) and len(raw_data) > 1:
                        values = [item.get("avg_value", 0) for item in raw_data]
                        if len(values) >= 2:
                            first_val = values[0]
                            last_val = values[-1]
                            change = last_val - first_val
                            percent_change = (change / first_val * 100) if first_val != 0 else 0
                            
                            if change > 0:
                                trend_analysis = f"\n## Trend Analysis:\n- Overall trend: Increasing by {change:.2f} ({percent_change:.1f}%)\n"
                            elif change < 0:
                                trend_analysis = f"\n## Trend Analysis:\n- Overall trend: Decreasing by {abs(change):.2f} ({abs(percent_change):.1f}%)\n"
                            else:
                                trend_analysis = f"\n## Trend Analysis:\n- Overall trend: Stable (no significant change)\n"
                            
                            # Add period changes
                            period_changes = []
                            period_name = "Day" if is_daily_breakdown else "Period"
                            for i in range(1, len(values)):
                                prev_val = values[i-1]
                                curr_val = values[i]
                                period_change = curr_val - prev_val
                                period_changes.append(f"{period_name} {i}: {period_change:+.2f}")
                            
                            if period_changes:
                                trend_analysis += f"- {period_name} changes: {', '.join(period_changes)}\n"
                    
                    for i, item in enumerate(raw_data):
                        if is_daily_breakdown:
                            # Daily breakdown format with statistical data
                            day = item.get("day", f"Day {i+1}")
                            avg_value = item.get("avg_value", 0)
                            min_value = item.get("min_value", 0)
                            max_value = item.get("max_value", 0)
                            daily_range = item.get("daily_range", 0)
                            sensor_type = item.get("sensor_type", "sensor")
                            
                            time_periods_text += f"â€¢ {sensor_type.replace('_', ' ').title()}: Avg: {avg_value:.2f}, Min: {min_value:.2f}, Max: {max_value:.2f} ({day})\n"
                        elif is_time_breakdown:
                            # Time period breakdown format (hourly, weekly, monthly) with statistical data
                            time_period = item.get("time_period", f"Period {i+1}")
                            avg_value = item.get("avg_value", 0)
                            min_value = item.get("min_value", 0)
                            max_value = item.get("max_value", 0)
                            period_range = item.get("period_range", 0)
                            sensor_type = item.get("sensor_type", "sensor")
                            
                            # Format time period for display
                            display_period = time_period
                            if ":" in time_period:  # Hourly
                                try:
                                    from datetime import datetime
                                    dt = datetime.strptime(time_period, "%Y-%m-%d %H:%M")
                                    display_period = dt.strftime("%d %b %H:%M")
                                except:
                                    pass
                            elif len(time_period) == 7 and time_period.count("-") == 1:  # Weekly
                                try:
                                    year, week = time_period.split("-")
                                    display_period = f"Week {week}, {year}"
                                except:
                                    pass
                            elif len(time_period) == 7 and time_period.count("-") == 1:  # Monthly
                                try:
                                    from datetime import datetime
                                    dt = datetime.strptime(time_period, "%Y-%m")
                                    display_period = dt.strftime("%b %Y")
                                except:
                                    pass
                            
                            # Use exact time range from time_context if available
                            if time_context:
                                # Use the exact time range requested by user for ALL data points
                                time_range_label = f"Last {time_context['value']} {time_context['unit']}"
                            else:
                                time_range_label = display_period
                            time_periods_text += f"â€¢ {sensor_type.replace('_', ' ').title()}: Avg: {avg_value:.2f}, Min: {min_value:.2f}, Max: {max_value:.2f} ({time_range_label})\n"
                        else:
                            # Regular time period format with statistical data
                            time_period = item.get("time_period", f"Period {i+1}")
                            avg_value = item.get("avg_value", 0)
                            min_value = item.get("min_value", 0)
                            max_value = item.get("max_value", 0)
                            sensor_type = item.get("sensor_type", "sensor")
                            
                            if is_comparison and len(raw_data) == 2:
                                # For comparison queries, label periods as today/yesterday
                                if i == 0:
                                    period_label = "Ø¯ÛŒØ±ÙˆØ²" if "Ø¯ÛŒØ±ÙˆØ²" in english_query else "Ø±ÙˆØ² Ù‚Ø¨Ù„"
                                else:
                                    period_label = "Ø§Ù…Ø±ÙˆØ²" if "Ø§Ù…Ø±ÙˆØ²" in english_query else "Ø±ÙˆØ² ÙØ¹Ù„ÛŒ"
                                time_periods_text += f"â€¢ {sensor_type.replace('_', ' ').title()}: Avg: {avg_value:.2f}, Min: {min_value:.2f}, Max: {max_value:.2f} ({period_label})\n"
                            else:
                                # Use exact time range from time_context if available
                                if time_context:
                                    # Use the exact time range requested by user for ALL data points
                                    time_range_label = f"Last {time_context['value']} {time_context['unit']}"
                                else:
                                    time_range_label = time_period
                                time_periods_text += f"â€¢ {sensor_type.replace('_', ' ').title()}: Avg: {avg_value:.2f}, Min: {min_value:.2f}, Max: {max_value:.2f} ({time_range_label})\n"
                    
                # Build chart information text
                chart_info_text = ""
                if chart_request and chart_request.get("wants_chart"):
                    chart_info_text = f"""
## CHART AVAILABLE:
- Chart Type: {chart_request.get('chart_type', 'line')}
- Chart Purpose: Visualize the data trend over time
- Note: Mention that a visual chart is available to see the data graphically
"""
                
                # Add time_context information if available
                time_context_info = ""
                if time_context:
                    time_context_info = f"\n\nDetected Time Range: {time_context['value']} {time_context['unit']} (from {time_context['start_time']} to {time_context['end_time']})"
                    # Add specific instruction for time range display
                    time_context_info += f"\n\nCRITICAL INSTRUCTION: When displaying data, you MUST use the EXACT time range the user requested: '{time_context['value']} {time_context['unit']}'. DO NOT use generic labels like 'Last Hour', 'Last 6 Hours', 'Last 24 Hours', 'Last Week'. ONLY use the specific time range the user asked for."
                
                context = f"""You are a smart agriculture AI assistant. Provide concise, actionable responses.

RESPONSE FORMAT (KEEP IT SHORT):
1. **Brief overview** - 1-2 sentences about what the data shows
2. **Key data points** - Show the most important values  
3. **Quick analysis** - 2-3 sentences about trends/patterns
4. **Action items** - 2-3 specific recommendations

CRITICAL INSTRUCTIONS:
- Keep response under 200 words total
- Use EXACT time range from the data provided below
- NEVER use generic labels like "Last Hour", "Last 6 Hours", "Last 24 Hours", "Last Week"
- NEVER use Persian generic labels like "Ø¢Ø®Ø±ÛŒÙ† Ø³Ø§Ø¹Øª", "Ø¢Ø®Ø±ÛŒÙ† Û¶ Ø³Ø§Ø¹Øª", "Ø¢Ø®Ø±ÛŒÙ† Û²Û´ Ø³Ø§Ø¹Øª", "Ø¢Ø®Ø±ÛŒÙ† Ù‡ÙØªÙ‡"
- ONLY use the EXACT time range shown in the data points below
- Be specific and actionable
- Focus on what matters most for farming decisions
- If Persian query, respond in Persian; if English, respond in English

DATA TO USE (USE THE EXACT TIME LABELS FROM THIS DATA):
{time_periods_text if time_periods_text else "No time period breakdown available"}

ANALYSIS TO INCLUDE:
{trend_analysis if trend_analysis else ""}
{comparison_text if comparison_text else ""}

{chart_info_text}

RESPONSE TEMPLATE:
```
ğŸ“Š [Sensor Type] Data:
â€¢ [Copy the EXACT time labels from the data above, don't create new ones]

ğŸ” Analysis:
â€¢ [Brief trend analysis]

ğŸ’¡ Actions:
â€¢ [2-3 specific recommendations]
```

CRITICAL WARNING: DO NOT generate your own time range labels! Use ONLY the exact labels provided in the DATA TO USE section above!

Query: {english_query}
Feature Context: {feature_context}
Intent: {intent}
Data Points: {len(raw_data)}{time_context_info}

ACTUAL DATA FROM DATABASE (RAW):
{json.dumps(raw_data[:10], indent=2)}

Provide a natural, conversational response using the pre-formatted data sections above.
"""
                
                # Log LLM call with truncated context
                truncated_context = context[:200] + "..." if len(context) > 200 else context
                logger.info(f"ğŸ¤– LLM Call - Summary Generation (Prompt): {truncated_context}")
                
                response = self.llm.invoke(context)
                summary = response.content.strip()
                
                # Log LLM response (truncated)
                truncated_summary = summary[:100] + "..." if len(summary) > 100 else summary
                logger.info(f"ğŸ¤– LLM Call - Summary Generation (Response): {truncated_summary}")
                
                # Add intent information at the end
                intent_explanation = {
                    "data_query": " Data Query - Analyzed sensor data",
                    "chit_chat": " General Chat - Conversational response", 
                    "mixed": " Mixed Query - Combined data analysis and conversation"
                }
                
                intent_text = intent_explanation.get(intent, f" Intent: {intent}")
                return f"{summary}\n\n---\n{intent_text}"
            else:
                # Fallback for when LLM is not available
                return f"LLM service is currently unavailable. Please check your API configuration."
                    
        except Exception as e:
            logger.error(f"Summary generation error: {str(e)}")
            return f"Summary for query: {english_query}"
    
    def _extract_metrics(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Extract metrics from raw data (handles both aggregated and raw data formats)"""
        try:
            if not raw_data:
                return {}
            
            # Check if data is in aggregated format (time-aware queries)
            if raw_data and "avg_value" in raw_data[0]:
                logger.info(f" Processing aggregated data format with {len(raw_data)} time periods")
                return self._extract_aggregated_metrics(raw_data)
            
            # Handle raw data format (legacy)
            logger.info(f" Processing raw data format with {len(raw_data)} data points")
            sensor_groups = {}
            for item in raw_data:
                sensor_type = item.get("sensor_type", "unknown")
                value = item.get("value", 0)
                
                if sensor_type not in sensor_groups:
                    sensor_groups[sensor_type] = []
                sensor_groups[sensor_type].append(value)
            
            # Calculate metrics for raw data
            metrics = {}
            for sensor_type, values in sensor_groups.items():
                if values:
                    metrics[sensor_type] = {
                        "count": len(values),
                        "average": round(sum(values) / len(values), 2),
                        "min": round(min(values), 2),
                        "max": round(max(values), 2),
                        "latest": round(values[0], 2) if values else 0
                    }
            
            return metrics
            
        except Exception as e:
            logger.error(f"Metrics extraction error: {str(e)}")
            return {}
    
    def _extract_aggregated_metrics(self, aggregated_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Extract metrics from aggregated data (time-aware queries) with proper sensor type grouping"""
        try:
            if not aggregated_data:
                return {}
            
            # Group data by sensor type for compound queries
            sensor_groups = {}
            for item in aggregated_data:
                sensor_type = item.get("sensor_type", "unknown")
                if sensor_type not in sensor_groups:
                    sensor_groups[sensor_type] = []
                sensor_groups[sensor_type].append(item)
            
            # Calculate metrics for each sensor type separately
            metrics = {}
            total_data_points = 0
            
            for sensor_type, sensor_data in sensor_groups.items():
                if sensor_data:
                    # Calculate metrics for this sensor type
                    avg_values = [item.get("avg_value", 0) for item in sensor_data]
                    min_values = [item.get("min_value", 0) for item in sensor_data]
                    max_values = [item.get("max_value", 0) for item in sensor_data]
                    data_points = sum(item.get("data_points", 0) for item in sensor_data)
                    total_data_points += data_points
                    
                    # Calculate weighted average for this sensor type
                    weighted_sum = sum(avg * item.get("data_points", 0) for avg, item in zip(avg_values, sensor_data))
                    sensor_average = weighted_sum / data_points if data_points > 0 else 0
                    
                    metrics[sensor_type] = {
                        "time_periods": len(sensor_data),
                        "total_data_points": data_points,
                        "overall_average": round(sensor_average, 2),
                        "overall_min": round(min(min_values), 2),
                        "overall_max": round(max(max_values), 2),
                        "latest_period": sensor_data[-1].get("time_period", "unknown") if sensor_data else "unknown",
                        "latest_average": round(sensor_data[-1].get("avg_value", 0), 2) if sensor_data else 0
                    }
                    
                    # Add individual period metrics for this sensor type
                    for i, period in enumerate(sensor_data):
                        period_key = f"{sensor_type}_period_{i+1}"
                        metrics[period_key] = {
                            "sensor_type": sensor_type,
                            "time_period": period.get("time_period", "unknown"),
                            "average": round(period.get("avg_value", 0), 2),
                            "min": round(period.get("min_value", 0), 2),
                            "max": round(period.get("max_value", 0), 2),
                            "data_points": period.get("data_points", 0)
                        }
            
            # Add overall summary
            metrics["aggregated"] = {
                "sensor_types": list(sensor_groups.keys()),
                "total_sensor_types": len(sensor_groups),
                "total_data_points": total_data_points,
                "total_periods": len(aggregated_data)
            }
            
            logger.info(f" Extracted aggregated metrics: {len(sensor_groups)} sensor types, {len(aggregated_data)} periods, {total_data_points} total data points")
            return metrics
            
        except Exception as e:
            logger.error(f"Aggregated metrics extraction error: {str(e)}")
            return {}
    
    def _translate_response_to_persian(self, structured_result: Dict[str, Any]) -> Dict[str, Any]:
        """Translate English response back to Persian using LLM"""
        try:
            # Translate the summary using LLM translator
            english_summary = structured_result.get("summary", "")
            if english_summary:
                persian_summary = self.translator.translate_response_to_persian(english_summary)
                structured_result["summary"] = persian_summary
                structured_result["original_summary"] = english_summary  # Keep original for debugging
            
            # Translate the output as well
            english_output = structured_result.get("output", "")
            if english_output:
                persian_output = self.translator.translate_response_to_persian(english_output)
                structured_result["output"] = persian_output
                structured_result["original_output"] = english_output  # Keep original for debugging
            
            structured_result["language"] = "fa"
            return structured_result
            
        except Exception as e:
            logger.error(f" Response Translation Error: {str(e)}")
            return structured_result
    
    def _generate_llm_response(self, original_query: str, translated_query: str, language: str, live_data: List[Dict[str, Any]], feature_context: str) -> Dict[str, Any]:
        """Generate intelligent response using LLM with full context"""
        try:
            # Prepare comprehensive context
            context = f"""
You are an expert agricultural AI assistant. Provide concise, helpful responses.

RESPONSE STRUCTURE:
1. **Brief Summary** - 2 sentences maximum about the current situation
2. **Clean Data Section** - Show the actual data in a readable format

GUIDELINES:
- Keep it short and to the point (2 sentences max)
- Give quick insights about what the data shows
- If user speaks Persian, reply in Persian; if English, reply in English
- Use EXACT time range from the data provided below
- NEVER use generic labels like "Last Hour", "Last 6 Hours", "Last 24 Hours", "Last Week"
- NEVER use Persian generic labels like "Ø¢Ø®Ø±ÛŒÙ† Ø³Ø§Ø¹Øª", "Ø¢Ø®Ø±ÛŒÙ† Û¶ Ø³Ø§Ø¹Øª", "Ø¢Ø®Ø±ÛŒÙ† Û²Û´ Ø³Ø§Ø¹Øª", "Ø¢Ø®Ø±ÛŒÙ† Ù‡ÙØªÙ‡"

FOR THE DATA SECTION, use this format:
```
ğŸ“Š Sensor Data by Time Range:
â€¢ Sensor Name: Avg: X.X, Min: X.X, Max: X.X (EXACT TIME RANGE FROM DATA)
â€¢ Sensor Name: Avg: X.X, Min: X.X, Max: X.X (EXACT TIME RANGE FROM DATA)
â€¢ Sensor Name: Avg: X.X, Min: X.X, Max: X.X (EXACT TIME RANGE FROM DATA)
â€¢ Sensor Name: Avg: X.X, Min: X.X, Max: X.X (EXACT TIME RANGE FROM DATA)
```

CRITICAL INSTRUCTION: Copy the EXACT time labels from the data provided below. DO NOT create your own time range labels!

FOR THE ANALYSIS SECTION, use this format:
```
ğŸ” Analysis:
â€¢ Trend: [Describe the trend - increasing, decreasing, stable]
â€¢ Pattern: [Identify any patterns in the data]
â€¢ Significance: [What this means for the farm]
â€¢ Alert Level: [Low/Medium/High based on the data]
```

FOR THE RECOMMENDATIONS SECTION, use this format:
```
ğŸ’¡ Recommendations:
â€¢ Immediate Actions: [What to do right now]
â€¢ Monitoring: [What to watch for]
â€¢ Long-term: [Strategic advice for the future]
â€¢ Resources: [Any tools or methods to use]
```

IMPORTANT: Use the actual data values provided below. Be specific and helpful.

USER QUERY: {original_query}
LIVE DATA: {json.dumps(live_data[:3], indent=1, ensure_ascii=False)}
FEATURE: {feature_context}

Provide an intelligent, helpful response with a clean data section using the actual values above.
"""

            # Log the context being sent to LLM
            logger.info(f"ğŸ¤– LLM Context Length: {len(context)} characters")
            logger.info(f" Live Data Points: {len(live_data)}")
            logger.info(f" Language: {language}")
            logger.info(f" Feature Context: {feature_context}")

            # Get LLM response
            if hasattr(self.llm, 'openai_api_key') and self.llm.openai_api_key:
                logger.info(" Using Real LLM (OpenAI)")
                
                # Log LLM call with truncated context
                truncated_context = context[:200] + "..." if len(context) > 200 else context
                logger.info(f"ğŸ¤– LLM Call - Live Data Response (Prompt): {truncated_context}")
                
                response = self.llm.invoke(context)
                
                # Log LLM response (truncated)
                response_content = getattr(response, 'content', None) or getattr(response, 'text', '')
                truncated_response = response_content[:100] + "..." if len(response_content) > 100 else response_content
                logger.info(f"ğŸ¤– LLM Call - Live Data Response (Response): {truncated_response}")
                response_text = response.content.strip()
                logger.info(f" LLM Response Length: {len(response_text)} characters")
                logger.info(f" FINAL RESPONSE GENERATED: {response_text[:200]}..." if len(response_text) > 200 else f" FINAL RESPONSE: {response_text}")
            else:
                logger.info(" LLM not available - returning factual DB results only")
                # Return factual DB results when LLM is unavailable
                if live_data:
                    response_text = f"# Data Summary\n\n## Available Data Points: {len(live_data)}\n\n## Raw Sensor Data:\n"
                    for i, data_point in enumerate(live_data[:5]):  # Show first 5 data points
                        sensor_type = data_point.get('sensor_type', 'unknown')
                        value = data_point.get('value', data_point.get('avg_value', 'N/A'))
                        timestamp = data_point.get('timestamp', 'N/A')
                        response_text += f"- {sensor_type}: {value} (at {timestamp})\n"
                    if len(live_data) > 5:
                        response_text += f"... and {len(live_data) - 5} more data points\n"
                    response_text += "\n*Note: LLM analysis unavailable. Showing raw data only.*"
                else:
                    response_text = "No data available. LLM service is currently unavailable for analysis."
                logger.info(f" Fallback Response Length: {len(response_text)} characters")
                logger.info(f" FINAL FALLBACK RESPONSE: {response_text[:200]}..." if len(response_text) > 200 else f" FINAL FALLBACK RESPONSE: {response_text}")
            
            return {
                "success": True,
                "query": original_query,
                "response": response_text,
                "language": language,
                "feature_context": feature_context,
                "timestamp": datetime.utcnow().isoformat(),
                "data_points": len(live_data),
                "llm_type": "real" if hasattr(self.llm, 'openai_api_key') and self.llm.openai_api_key else "mock",
                "context_length": len(context)
            }
            
        except Exception as e:
            logger.error(f" Error generating LLM response: {str(e)}")
            return {
                "success": False,
                "error": f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®: {str(e)}",
                "query": original_query
            }
    
    
    def get_sample_queries(self) -> Dict[str, List[str]]:
        """Get sample queries for each entity"""
        return {
            "irrigation": [
                "ÙˆØ¶Ø¹ÛŒØª Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ø§Ù…Ø±ÙˆØ² Ú†Ø·ÙˆØ±Ù‡ØŸ",
                "Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ø§Ù…Ø±ÙˆØ² Ú©Ù…Ù‡ ÛŒØ§ Ù†Ù‡ØŸ",
                "Ù…ØµØ±Ù Ø¢Ø¨ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ",
                "Ø±Ø·ÙˆØ¨Øª Ø®Ø§Ú© Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªØŸ"
            ],
            "environment": [
                "Ø¯Ù…Ø§ÛŒ Ú¯Ù„Ø®Ø§Ù†Ù‡ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ",
                "Ø±Ø·ÙˆØ¨Øª Ù‡ÙˆØ§ Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªØŸ",
                "ÙˆØ¶Ø¹ÛŒØª Ù…Ø­ÛŒØ· Ú†Ø·ÙˆØ±Ù‡ØŸ",
                "ØªÙ‡ÙˆÛŒÙ‡ Ú¯Ù„Ø®Ø§Ù†Ù‡ Ú†Ø·ÙˆØ±Ù‡ØŸ"
            ],
            "pest": [
                "ÙˆØ¶Ø¹ÛŒØª Ø¢ÙØ§Øª Ú†Ø·ÙˆØ±Ù‡ØŸ",
                "Ø¢ÙØ§Øª Ø§Ù…Ø±ÙˆØ² Ú†Ù‡ Ù‡Ø³ØªÙ†Ø¯ØŸ",
                "Ø®Ø·Ø± Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ",
                "ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¢ÙØªâ€ŒÚ©Ø´ÛŒ Ú†ÛŒØ³ØªØŸ"
            ],
            "general": [
                "ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ Ù…Ø²Ø±Ø¹Ù‡ Ú†Ø·ÙˆØ±Ù‡ØŸ",
                "Ø¢Ø®Ø±ÛŒÙ† Ù‚Ø±Ø§Ø¦Øªâ€ŒÙ‡Ø§ Ú†Ù‡ Ù‡Ø³ØªÙ†Ø¯ØŸ",
                "Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ Ø³Ù†Ø³ÙˆØ±Ù‡Ø§ Ø±Ø§ Ù†Ø´Ø§Ù† Ø¯Ù‡ÛŒØ¯"
            ]
        }
    
    def get_ontology(self) -> Dict[str, Any]:
        """Get the complete ontology"""
        return self.ontology
    
    def _process_alert_query(self, query: str, session_id: str, detected_lang: str) -> Dict[str, Any]:
        """Process alert management queries"""
        try:
            from app.services.alert_manager import AlertManager
            
            alert_manager = AlertManager()
            query_lower = query.lower()
            
            # Handle different alert commands (English and Persian)
            create_commands = [
                # Basic alert commands
                "create", "alert me", "set alert", "add alert", "make alert", "new alert",
                # Send/notify commands
                "send an alert", "send alert", "send me an alert", "send me alert",
                "notify me", "notify", "notify when", "notify if",
                "warn me", "warn", "warn when", "warn if",
                "alert when", "alert if", "alert on",
                # Message/communication commands
                "message me", "message when", "message if",
                "tell me", "tell me when", "tell me if",
                "inform me", "inform when", "inform if",
                "let me know", "let me know when", "let me know if",
                # Action-based commands
                "remind me", "remind when", "remind if",
                "ping me", "ping when", "ping if",
                "buzz me", "buzz when", "buzz if",
                # Threshold/condition commands
                "when", "if", "whenever", "once", "as soon as",
                # Direct action commands
                "alert", "alerts", "alerting", "alerted",
                "notification", "notifications", "notify", "notifying",
                "warning", "warnings", "warn", "warning me"
            ]
            
            if any(cmd in query_lower for cmd in create_commands):
                # Create new alert
                result = alert_manager.create_alert_from_natural_language(query, session_id)
                
                if result["success"]:
                    if detected_lang == 'fa':
                        # Persian response
                        sensor_name = self._get_persian_sensor_name(result['sensor_type'])
                        condition_name = self._get_persian_condition_name(result['condition'])
                        
                        response_text = f"âœ… Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯!\n\n"
                        response_text += f"**Ø¬Ø²Ø¦ÛŒØ§Øª Ù‡Ø´Ø¯Ø§Ø±:**\n"
                        response_text += f"- **Ø³Ù†Ø³ÙˆØ±**: {sensor_name}\n"
                        response_text += f"- **Ø´Ø±Ø·**: {condition_name} {result['threshold']}\n"
                        response_text += f"- **ÙˆØ¶Ø¹ÛŒØª**: ÙØ¹Ø§Ù„\n\n"
                        response_text += f"**Ù…Ø¹Ù†ÛŒ Ø§ÛŒÙ† Ù‡Ø´Ø¯Ø§Ø±:**\n"
                        response_text += f"Ù‡Ø± Ø²Ù…Ø§Ù† Ú©Ù‡ {sensor_name} {condition_name} {result['threshold']} Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ù‡ Ø´Ù…Ø§ Ø§Ø·Ù„Ø§Ø¹ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n\n"
                        response_text += f"**Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ÛŒ:**\n"
                        response_text += f"- 'Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù…Ù† Ø±Ø§ Ù†Ø´Ø§Ù† Ø¨Ø¯Ù‡' Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¯Ù† Ù‡Ù…Ù‡ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§\n"
                        response_text += f"- 'Ù‡Ø´Ø¯Ø§Ø± {sensor_name} Ø±Ø§ Ø­Ø°Ù Ú©Ù†' Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ø§ÛŒÙ† Ù‡Ø´Ø¯Ø§Ø±\n"
                        response_text += f"- 'Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø±Ø·ÙˆØ¨Øª < 40% Ø§Ø³Øª Ø¨Ù‡ Ù…Ù† Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø¯Ù‡' Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù‡Ø´Ø¯Ø§Ø± Ø¬Ø¯ÛŒØ¯"
                    else:
                        # English response
                        response_text = f"âœ… Alert created successfully!\n\n"
                        response_text += f"**Alert Details:**\n"
                        response_text += f"- **Sensor**: {result['sensor_type'].replace('_', ' ').title()}\n"
                        response_text += f"- **Condition**: {result['condition']} {result['threshold']}\n"
                        response_text += f"- **Status**: Active\n\n"
                        response_text += f"**What this means:**\n"
                        response_text += f"You'll be notified whenever {result['sensor_type']} {result['condition']} {result['threshold']}\n\n"
                        response_text += f"**Next steps:**\n"
                        response_text += f"- Say 'Show my alerts' to see all your alerts\n"
                        response_text += f"- Say 'Delete {result['sensor_type']} alert' to remove this alert\n"
                        response_text += f"- Say 'Alert me when humidity < 40%' to create another alert"
                else:
                    if detected_lang == 'fa':
                        response_text = f"âŒ Ø§ÛŒØ¬Ø§Ø¯ Ù‡Ø´Ø¯Ø§Ø± Ù†Ø§Ù…ÙˆÙÙ‚: {result.get('error', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ')}\n\n"
                        response_text += f"**Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­:**\n"
                        response_text += f"- 'Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø¯Ù…Ø§ > 25Â°C Ø§Ø³Øª Ø¨Ù‡ Ù…Ù† Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø¯Ù‡'\n"
                        response_text += f"- 'Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø±Ø·ÙˆØ¨Øª < 40% Ø§Ø³Øª Ø¨Ù‡ Ù…Ù† Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø¯Ù‡'\n"
                        response_text += f"- 'Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø±Ø·ÙˆØ¨Øª Ø®Ø§Ú© > 60% Ø§Ø³Øª Ø¨Ù‡ Ù…Ù† Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø¯Ù‡'"
                    else:
                        response_text = f"âŒ Failed to create alert: {result.get('error', 'Unknown error')}\n\n"
                        response_text += f"**Try these examples:**\n"
                        response_text += f"- 'Alert me when temperature > 25Â°C'\n"
                        response_text += f"- 'Alert me when humidity < 40%'\n"
                        response_text += f"- 'Alert me when soil moisture > 60%'"
                
                return {
                    "success": True,
                    "type": "alert_management",
                    "response": response_text,
                    "alert_data": result if result["success"] else None
                }
            
            elif any(cmd in query_lower for cmd in ["show", "list", "my alerts", "alerts"]):
                # Show existing alerts
                alerts = alert_manager.get_user_alerts(session_id)
                
                if not alerts:
                    response_text = "You don't have any alerts set up yet.\n\n"
                    response_text += "Create your first alert:\n"
                    response_text += "â€¢ 'Alert me when temperature > 25Â°C'\n"
                    response_text += "â€¢ 'Alert me when humidity < 40%'\n"
                    response_text += "â€¢ 'Alert me when soil moisture > 60%'"
                else:
                    response_text = f"Your Alerts ({len(alerts)} total):\n\n"
                    
                    for i, alert in enumerate(alerts, 1):
                        status = "ğŸŸ¢ Active" if alert["is_active"] else "ğŸ”´ Inactive"
                        response_text += f"{i}. {alert['alert_name']} {status}\n"
                        response_text += f"   â€¢ {alert['sensor_type'].replace('_', ' ').title()} {alert['condition_type']} {alert['threshold_value']}\n"
                        response_text += f"   â€¢ Created: {alert['created_at'][:10]}\n\n"
                    
                    response_text += "Commands:\n"
                    response_text += "â€¢ 'Delete [sensor] alert' to remove an alert\n"
                    response_text += "â€¢ 'Alert me when...' to create new alerts"
                
                return {
                    "success": True,
                    "type": "alert_management",
                    "response": response_text,
                    "alerts": alerts
                }
            
            elif any(cmd in query_lower for cmd in ["delete", "remove", "cancel"]):
                # Delete alert
                # Extract sensor type from query
                sensor_types = ["temperature", "humidity", "pressure", "light", "motion", "soil_moisture", "co2_level", "ph"]
                sensor_to_delete = None
                
                for sensor in sensor_types:
                    if sensor.replace("_", " ") in query_lower or sensor in query_lower:
                        sensor_to_delete = sensor
                        break
                
                if sensor_to_delete:
                    # Find and delete the alert
                    alerts = alert_manager.get_user_alerts(session_id)
                    alert_to_delete = None
                    
                    for alert in alerts:
                        if alert["sensor_type"] == sensor_to_delete:
                            alert_to_delete = alert
                            break
                    
                    if alert_to_delete:
                        success = alert_manager.delete_alert(alert_to_delete["id"], session_id)
                        if success:
                            response_text = f"âœ… Deleted {alert_to_delete['alert_name']} successfully!"
                        else:
                            response_text = f"âŒ Failed to delete {alert_to_delete['alert_name']}"
                    else:
                        response_text = f"âŒ No {sensor_to_delete.replace('_', ' ')} alert found to delete"
                else:
                    response_text = "âŒ Please specify which alert to delete (e.g., 'Delete temperature alert')"
                
                return {
                    "success": True,
                    "type": "alert_management",
                    "response": response_text
                }
            
            else:
                # Unknown alert command
                response_text = "ğŸ¤” I didn't understand that alert command.\n\n"
                response_text += "**Available commands:**\n"
                response_text += "- 'Alert me when temperature > 25Â°C' - Create new alert\n"
                response_text += "- 'Show my alerts' - List all alerts\n"
                response_text += "- 'Delete temperature alert' - Remove an alert\n\n"
                response_text += "**Examples:**\n"
                response_text += "- 'Alert me when humidity < 40%'\n"
                response_text += "- 'Alert me when soil moisture > 60%'\n"
                response_text += "- 'Alert me when pressure > 1000'"
                
                return {
                    "success": True,
                    "type": "alert_management",
                    "response": response_text
                }
                
        except Exception as e:
            logger.error(f"âŒ Error processing alert query: {e}")
            return {
                "success": False,
                "type": "alert_management",
                "response": f"âŒ Error processing alert command: {str(e)}"
            }

    def _get_persian_sensor_name(self, sensor_type: str) -> str:
        """Get Persian name for sensor type"""
        persian_names = {
            "temperature": "Ø¯Ù…Ø§",
            "humidity": "Ø±Ø·ÙˆØ¨Øª",
            "pressure": "ÙØ´Ø§Ø±",
            "light": "Ù†ÙˆØ±",
            "motion": "Ø­Ø±Ú©Øª",
            "soil_moisture": "Ø±Ø·ÙˆØ¨Øª Ø®Ø§Ú©",
            "co2_level": "Ø³Ø·Ø­ CO2",
            "ph": "Ù¾ÛŒ Ø§Ú†"
        }
        return persian_names.get(sensor_type, sensor_type)
    
    def _get_persian_condition_name(self, condition: str) -> str:
        """Get Persian name for condition type"""
        persian_names = {
            "above": "Ø¨ÛŒØ´ØªØ± Ø§Ø²",
            "below": "Ú©Ù…ØªØ± Ø§Ø²",
            "equals": "Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§",
            "greater_equal": "Ø¨ÛŒØ´ØªØ± ÛŒØ§ Ù…Ø³Ø§ÙˆÛŒ",
            "less_equal": "Ú©Ù…ØªØ± ÛŒØ§ Ù…Ø³Ø§ÙˆÛŒ"
        }
        return persian_names.get(condition, condition)
    
    def _get_persian_severity_name(self, severity: str) -> str:
        """Get Persian name for severity level"""
        severity_map = {
            "info": "Ø§Ø·Ù„Ø§Ø¹ÛŒ",
            "warning": "Ù‡Ø´Ø¯Ø§Ø±", 
            "critical": "Ø¨Ø­Ø±Ø§Ù†ÛŒ"
        }
        return severity_map.get(severity, severity)
    
    def _detect_enhanced_alert_intent(self, query: str, detected_lang: str) -> str:
        """Detect enhanced alert intent with ontology support"""
        query_lower = query.lower()
        
        # Enhanced create commands with severity and action support
        create_commands = [
            "create", "alert me", "set alert", "add alert", "make alert", "new alert",
            "send an alert", "send alert", "notify me", "notify", "warn me", "warn",
            "critical alert", "urgent alert", "emergency alert", "Ø¨Ø­Ø±Ø§Ù†ÛŒ", "ÙÙˆØ±ÛŒ",
            "auto alert", "automatic alert", "Ø®ÙˆØ¯Ú©Ø§Ø±", "Ø§ØªÙˆÙ…Ø§ØªÛŒÚ©"
        ]
        
        if any(cmd in query_lower for cmd in create_commands):
            return "create"
        elif any(cmd in query_lower for cmd in ["show", "list", "my alerts", "alerts"]):
            return "list"
        elif any(cmd in query_lower for cmd in ["delete", "remove", "cancel"]):
            return "delete"
        else:
            return "unknown"

    def get_health_status(self) -> Dict[str, Any]:
        """Get health status of the service"""
        try:
            # Test basic functionality
            test_result = self.process_query("test query")
            
            return {
                "status": "healthy",
                "timestamp": datetime.utcnow().isoformat(),
                "llm_available": hasattr(self.llm, 'openai_api_key'),
                "ontology_entities": len(self.ontology["entities"]),
                "test_query_success": test_result["success"]
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "timestamp": datetime.utcnow().isoformat(),
                "error": str(e)
            }
